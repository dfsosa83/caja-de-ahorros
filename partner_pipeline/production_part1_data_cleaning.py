# =============================================================================
# PRODUCTION PIPELINE PART 1 - DATA CLEANING & FEATURE ENGINEERING
# =============================================================================
#
# OBJECTIVE: Create clean dataset with exactly 11 features for XGBoost model
#
# FINAL MODEL FEATURES (11 features):
# 1. 'edad'                                    - Customer age
# 2. 'fechaingresoempleo_days'                 - Employment tenure in days
# 3. 'balance_to_payment_ratio'                - Financial health indicator
# 4. 'fecha_inicio_days'                       - Account start date in days
# 5. 'saldo'                                   - Account balance
# 6. 'nombreempleadorcliente_consolidated_freq' - Employer frequency encoding
# 7. 'location_x_occupation'                   - Location-occupation interaction
# 8. 'monto_letra'                             - Monthly payment amount
# 9. 'fecha_vencimiento_days'                  - Loan end date in days
# 10. 'balance_coverage_ratio'                 - Balance coverage metric
# 11. 'payment_per_age'                        - Payment normalized by age
#
# INPUT: Raw customer data CSV (production format)
# OUTPUT: Clean dataset ready for model inference
# =============================================================================

import pandas as pd
import numpy as np
from datetime import datetime
import warnings
import pickle
import os
warnings.filterwarnings('ignore')

# Set display options
pd.set_option('display.max_columns', None)

def load_production_data(file_path):
    """
    Load raw production data with proper encoding and error handling
    """
    print("ğŸš€ PRODUCTION PART 1 - DATA LOADING")
    print("="*60)
    
    print("ğŸ“‚ Loading raw production data...")
    try:
        df = pd.read_csv(file_path, encoding='latin-1', sep=',', on_bad_lines='skip', engine='python')
        print(f"âœ… Dataset loaded successfully: {df.shape}")
        print(f"ğŸ“Š Columns found: {len(df.columns)}")
        return df
    except Exception as e:
        print(f"âŒ Error loading data: {e}")
        return None

def standardize_column_names(df):
    """
    Standardize column names to match our model requirements
    Based on exploratory data analysis patterns
    """
    print("\nğŸ”§ STANDARDIZING COLUMN NAMES")
    print("="*50)
    
    # Column mapping based on typical production data format
    column_mapping = {
        'Cliente': 'cliente',
        'Identificador_Unico': 'identificador_unico',
        'Edad': 'edad',
        'Sexo': 'sexo',
        'Ciudad': 'ciudad',
        'Pais': 'pais',
        'Ocupacion': 'ocupacion',
        'Estado_Civil': 'estado_civil',
        'FechaIngresoEmpleo': 'fechaingresoempleo',
        'NombreEmpleadorCliente': 'nombreempleadorcliente',
        'CargoEmpleoCliente': 'cargoempleocliente',
        'monto_letra': 'monto_letra',
        'saldo': 'saldo',
        'fecha_inicio': 'fecha_inicio',
        'fecha_vencimiento': 'fecha_vencimiento'
    }
    
    # Apply mapping
    df = df.rename(columns=column_mapping)
    
    # Clean remaining column names (remove special characters, BOM, etc.)
    df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('[^a-zA-Z0-9_]', '', regex=True)
    
    # Remove BOM characters if present
    df.columns = df.columns.str.replace('\ufeff', '').str.replace('Ã¯Â»Â¿', '')
    
    print(f"âœ… Column names standardized")
    print(f"ğŸ“‹ Key columns available: {[col for col in df.columns if col in ['cliente', 'identificador_unico', 'edad', 'ocupacion', 'nombreempleadorcliente']]}")
    
    return df

def convert_date_columns(df):
    """
    Convert date columns to datetime format
    Focus on the 3 key date columns needed for our model
    """
    print("\nğŸ“… CONVERTING DATE COLUMNS")
    print("="*50)
    
    # Target date columns for our model
    date_columns = ['fechaingresoempleo', 'fecha_inicio', 'fecha_vencimiento']
    
    for col in date_columns:
        if col in df.columns:
            print(f"   Converting {col}...")
            try:
                # Try DD/MM/YYYY format first (most common in production)
                df[col] = pd.to_datetime(df[col], format='%d/%m/%Y', errors='coerce')
                success_rate = df[col].notna().sum() / len(df)
                print(f"   âœ… {col} converted (success rate: {success_rate:.1%})")
            except:
                print(f"   âš ï¸ {col} conversion failed - will use default values")
        else:
            print(f"   âš ï¸ {col} not found in dataset")
    
    return df

def load_frequency_mappings():
    """
    Load pre-computed frequency mappings from training data
    These are essential for categorical feature encoding
    """
    print("\nğŸ”¢ LOADING FREQUENCY MAPPINGS")
    print("="*50)
    
    try:
        # Path to our production frequency mappings
        freq_maps_path = r'models\production\production_frequency_mappings_catboost.pkl'
        
        if os.path.exists(freq_maps_path):
            with open(freq_maps_path, 'rb') as f:
                freq_maps = pickle.load(f)
            print("âœ… Production frequency mappings loaded successfully")
            return freq_maps
        else:
            print("âš ï¸ Production frequency mappings not found - using fallback")
            return get_fallback_frequency_maps()
    except Exception as e:
        print(f"âš ï¸ Error loading frequency mappings: {e}")
        return get_fallback_frequency_maps()

def get_fallback_frequency_maps():
    """
    Fallback frequency mappings based on training data analysis
    These should match the patterns from our exploratory data analysis
    """
    print("ğŸš¨ Using fallback frequency mappings")
    
    return {
        'nombreempleadorcliente': {
            'GOBIERNO DE COSTA RICA': 200,
            'BANCO NACIONAL': 150,
            'ICE': 120,
            'CCSS': 100,
            'MUNICIPALIDAD': 80,
            'TECH COMPANY SA': 60,
            'COMERCIAL LTDA': 40,
            'SERVICIOS SA': 30,
            'INDEPENDIENTE': 15,
            'Others': 1
        },
        'ocupacion': {
            'INGENIERO': 150,
            'CONTADOR': 120,
            'ADMINISTRADOR': 100,
            'VENDEDOR': 90,
            'SECRETARIA': 80,
            'OPERARIO': 70,
            'SUPERVISOR': 50,
            'TECNICO': 45,
            'Others': 1
        }
    }

def create_categorical_frequency_features(df):
    """
    Create frequency encoding for categorical variables
    This is the most important feature engineering step
    """
    print("\nğŸ¯ CREATING CATEGORICAL FREQUENCY FEATURES")
    print("="*50)
    
    # Load frequency mappings from training data
    freq_maps = load_frequency_mappings()
    
    # 1. nombreempleadorcliente_consolidated_freq (TOP predictor)
    print("   Creating nombreempleadorcliente_consolidated_freq...")
    if 'nombreempleadorcliente' in df.columns:
        # Clean and standardize employer names
        df['nombreempleadorcliente'] = df['nombreempleadorcliente'].astype(str).str.strip().str.upper()
        
        # Apply frequency mapping
        if 'nombreempleadorcliente' in freq_maps:
            freq_map = freq_maps['nombreempleadorcliente']
            default_freq = min(freq_map.values()) if freq_map else 1
            df['nombreempleadorcliente_consolidated_freq'] = df['nombreempleadorcliente'].map(freq_map).fillna(default_freq)
        else:
            df['nombreempleadorcliente_consolidated_freq'] = 1
        
        print(f"   âœ… nombreempleadorcliente_consolidated_freq created")
    else:
        print(f"   âš ï¸ nombreempleadorcliente not found - setting to default")
        df['nombreempleadorcliente_consolidated_freq'] = 1
    
    # 2. location_x_occupation (interaction feature)
    print("   Creating location_x_occupation...")
    if 'ciudad' in df.columns and 'ocupacion' in df.columns:
        # Create location-occupation interaction
        df['ciudad'] = df['ciudad'].astype(str).str.strip().str.upper()
        df['ocupacion'] = df['ocupacion'].astype(str).str.strip().str.upper()
        
        # Simple interaction: combine and encode frequency
        df['location_occupation_combo'] = df['ciudad'] + '_' + df['ocupacion']
        combo_freq = df['location_occupation_combo'].value_counts().to_dict()
        df['location_x_occupation'] = df['location_occupation_combo'].map(combo_freq).fillna(1)
        
        # Clean up temporary column
        df.drop('location_occupation_combo', axis=1, inplace=True)
        print(f"   âœ… location_x_occupation created")
    else:
        print(f"   âš ï¸ ciudad or ocupacion not found - setting to default")
        df['location_x_occupation'] = 1
    
    return df

def create_temporal_features(df):
    """
    Create temporal features from date columns
    Convert dates to days since reference point
    """
    print("\nâ° CREATING TEMPORAL FEATURES")
    print("="*50)
    
    # Reference date for calculations (current date)
    reference_date = datetime.now()
    
    # 1. fechaingresoempleo_days (employment tenure)
    print("   Creating fechaingresoempleo_days...")
    if 'fechaingresoempleo' in df.columns and pd.api.types.is_datetime64_any_dtype(df['fechaingresoempleo']):
        df['fechaingresoempleo_days'] = (reference_date - df['fechaingresoempleo']).dt.days
        df['fechaingresoempleo_days'] = df['fechaingresoempleo_days'].fillna(df['fechaingresoempleo_days'].median())
        print(f"   âœ… fechaingresoempleo_days created")
    else:
        print(f"   âš ï¸ fechaingresoempleo not available - using default")
        df['fechaingresoempleo_days'] = 1000  # Default ~3 years
    
    # 2. fecha_inicio_days (account start)
    print("   Creating fecha_inicio_days...")
    if 'fecha_inicio' in df.columns and pd.api.types.is_datetime64_any_dtype(df['fecha_inicio']):
        df['fecha_inicio_days'] = (reference_date - df['fecha_inicio']).dt.days
        df['fecha_inicio_days'] = df['fecha_inicio_days'].fillna(df['fecha_inicio_days'].median())
        print(f"   âœ… fecha_inicio_days created")
    else:
        print(f"   âš ï¸ fecha_inicio not available - using default")
        df['fecha_inicio_days'] = 500  # Default ~1.5 years
    
    # 3. fecha_vencimiento_days (loan end date)
    print("   Creating fecha_vencimiento_days...")
    if 'fecha_vencimiento' in df.columns and pd.api.types.is_datetime64_any_dtype(df['fecha_vencimiento']):
        df['fecha_vencimiento_days'] = (df['fecha_vencimiento'] - reference_date).dt.days
        df['fecha_vencimiento_days'] = df['fecha_vencimiento_days'].fillna(df['fecha_vencimiento_days'].median())
        print(f"   âœ… fecha_vencimiento_days created")
    else:
        print(f"   âš ï¸ fecha_vencimiento not available - using default")
        df['fecha_vencimiento_days'] = 365  # Default 1 year remaining
    
    return df

def create_financial_ratio_features(df):
    """
    Create financial ratio features
    These are key predictors of income capacity
    """
    print("\nğŸ’° CREATING FINANCIAL RATIO FEATURES")
    print("="*50)
    
    # 1. balance_to_payment_ratio (financial health indicator)
    print("   Creating balance_to_payment_ratio...")
    if 'saldo' in df.columns and 'monto_letra' in df.columns:
        # Avoid division by zero
        df['balance_to_payment_ratio'] = np.where(
            df['monto_letra'] > 0,
            df['saldo'] / df['monto_letra'],
            0
        )
        # Cap extreme values for stability
        df['balance_to_payment_ratio'] = np.clip(df['balance_to_payment_ratio'], 0, 100)
        print(f"   âœ… balance_to_payment_ratio created")
    else:
        print(f"   âš ï¸ saldo or monto_letra missing - setting to default")
        df['balance_to_payment_ratio'] = 1.0
    
    # 2. balance_coverage_ratio (balance adequacy)
    print("   Creating balance_coverage_ratio...")
    if 'saldo' in df.columns and 'monto_letra' in df.columns:
        # How many months can balance cover payments
        df['balance_coverage_ratio'] = np.where(
            df['monto_letra'] > 0,
            df['saldo'] / (df['monto_letra'] * 12),  # Annual coverage
            0
        )
        df['balance_coverage_ratio'] = np.clip(df['balance_coverage_ratio'], 0, 10)
        print(f"   âœ… balance_coverage_ratio created")
    else:
        print(f"   âš ï¸ Required columns missing - setting to default")
        df['balance_coverage_ratio'] = 0.5
    
    # 3. payment_per_age (payment burden by age)
    print("   Creating payment_per_age...")
    if 'monto_letra' in df.columns and 'edad' in df.columns:
        df['payment_per_age'] = np.where(
            df['edad'] > 0,
            df['monto_letra'] / df['edad'],
            0
        )
        df['payment_per_age'] = np.clip(df['payment_per_age'], 0, 1000)
        print(f"   âœ… payment_per_age created")
    else:
        print(f"   âš ï¸ monto_letra or edad missing - setting to default")
        df['payment_per_age'] = 10.0
    
    return df

def validate_and_prepare_final_features(df):
    """
    Validate that all 11 required features are present and properly formatted
    Handle missing values and ensure data quality
    """
    print("\nâœ… VALIDATING FINAL FEATURES")
    print("="*50)

    # The exact 11 features our XGBoost model expects
    required_features = [
        'edad',                                    # 1. Customer age
        'fechaingresoempleo_days',                 # 2. Employment tenure in days
        'balance_to_payment_ratio',                # 3. Financial health indicator
        'fecha_inicio_days',                       # 4. Account start date in days
        'saldo',                                   # 5. Account balance
        'nombreempleadorcliente_consolidated_freq', # 6. Employer frequency encoding
        'location_x_occupation',                   # 7. Location-occupation interaction
        'monto_letra',                             # 8. Monthly payment amount
        'fecha_vencimiento_days',                  # 9. Loan end date in days
        'balance_coverage_ratio',                  # 10. Balance coverage metric
        'payment_per_age'                          # 11. Payment normalized by age
    ]

    print(f"ğŸ“‹ Checking {len(required_features)} required features...")

    # Check for missing features
    missing_features = []
    for feature in required_features:
        if feature not in df.columns:
            missing_features.append(feature)
            print(f"   âŒ Missing: {feature}")
        else:
            missing_count = df[feature].isnull().sum()
            missing_pct = (missing_count / len(df)) * 100
            print(f"   âœ… {feature}: {missing_count} missing ({missing_pct:.1f}%)")

    if missing_features:
        print(f"ğŸš¨ ERROR: Missing required features: {missing_features}")
        return None, False

    # Handle missing values in existing features
    print("\nğŸ”§ Handling missing values...")
    for feature in required_features:
        if df[feature].isnull().sum() > 0:
            if df[feature].dtype in ['int64', 'float64']:
                # Fill numeric features with median
                median_val = df[feature].median()
                df[feature] = df[feature].fillna(median_val)
                print(f"   ğŸ“Š {feature}: filled with median ({median_val:.2f})")
            else:
                # Fill categorical features with mode or default
                mode_val = df[feature].mode().iloc[0] if len(df[feature].mode()) > 0 else 0
                df[feature] = df[feature].fillna(mode_val)
                print(f"   ğŸ“Š {feature}: filled with mode ({mode_val})")

    # Ensure proper data types for model
    print("\nğŸ”§ Optimizing data types...")
    for feature in required_features:
        if df[feature].dtype == 'object':
            try:
                df[feature] = pd.to_numeric(df[feature], errors='coerce')
                df[feature] = df[feature].fillna(0)
                print(f"   âœ… {feature}: converted to numeric")
            except:
                print(f"   âš ï¸ {feature}: could not convert to numeric")

        # Optimize numeric types for memory efficiency
        if df[feature].dtype in ['int64']:
            df[feature] = df[feature].astype('int32')
        elif df[feature].dtype in ['float64']:
            df[feature] = df[feature].astype('float32')

    print("âœ… All features validated and optimized")
    return df, True

def production_part1_main(input_file_path, output_file_path=None):
    """
    Main function for Production Part 1: Data Cleaning & Feature Engineering

    Input: Raw production data CSV
    Output: Clean dataset with exactly 11 features ready for model inference
    """
    print("ğŸš€ PRODUCTION PART 1 - DATA CLEANING & FEATURE ENGINEERING")
    print("="*80)
    print("ğŸ¯ OBJECTIVE: Create 11 features for XGBoost income prediction model")
    print("ğŸ“‹ INPUT: Raw customer data")
    print("ğŸ“‹ OUTPUT: Clean dataset ready for model inference")
    print("="*80)

    # Step 1: Load raw data
    df = load_production_data(input_file_path)
    if df is None:
        print("âŒ Failed to load data")
        return None

    # Step 2: Standardize column names
    df = standardize_column_names(df)

    # Step 3: Convert date columns
    df = convert_date_columns(df)

    # Step 4: Create categorical frequency features
    df = create_categorical_frequency_features(df)

    # Step 5: Create temporal features
    df = create_temporal_features(df)

    # Step 6: Create financial ratio features
    df = create_financial_ratio_features(df)

    # Step 7: Validate and prepare final features
    df_final, is_valid = validate_and_prepare_final_features(df)

    if not is_valid:
        print("âŒ Feature validation failed")
        return None

    # Step 8: Create final dataset with ID columns + model features
    final_features = [
        'edad', 'fechaingresoempleo_days', 'balance_to_payment_ratio',
        'fecha_inicio_days', 'saldo', 'nombreempleadorcliente_consolidated_freq',
        'location_x_occupation', 'monto_letra', 'fecha_vencimiento_days',
        'balance_coverage_ratio', 'payment_per_age'
    ]

    # Keep ID columns for traceability
    id_columns = []
    for col in ['cliente', 'identificador_unico']:
        if col in df_final.columns:
            id_columns.append(col)

    if not id_columns:
        print("âš ï¸ No ID columns found - creating row_id")
        df_final['row_id'] = df_final.index
        id_columns = ['row_id']

    # Create final dataset
    final_columns = id_columns + final_features
    df_clean_final = df_final[final_columns].copy()

    # Save to file if output path provided
    if output_file_path:
        print(f"\nğŸ’¾ Saving clean dataset to: {output_file_path}")
        df_clean_final.to_csv(output_file_path, index=False, encoding='utf-8')
        print(f"âœ… File saved successfully!")

    # Final summary
    print(f"\nğŸ‰ PRODUCTION PART 1 COMPLETED!")
    print(f"ğŸ“Š Clean dataset shape: {df_clean_final.shape}")
    print(f"ğŸ†” ID columns: {id_columns}")
    print(f"ğŸ¯ Model features: {len(final_features)}")
    print(f"ğŸ“‹ Ready for model inference!")

    # Show feature summary
    print(f"\nğŸ“Š FEATURE SUMMARY:")
    for feature in final_features:
        dtype = df_clean_final[feature].dtype
        min_val = df_clean_final[feature].min()
        max_val = df_clean_final[feature].max()
        mean_val = df_clean_final[feature].mean()
        print(f"   {feature}: {dtype} [{min_val:.2f}, {max_val:.2f}] mean={mean_val:.2f}")

    return df_clean_final

# =============================================================================
# USAGE EXAMPLE
# =============================================================================
if __name__ == "__main__":
    # Input and output file paths
    input_file = r'data\production\final_info_clientes.csv'
    output_file = r'data\production\df_clientes_clean_final.csv'

    print("ğŸ¯ PRODUCTION PART 1 - INCOME PREDICTION DATA CLEANING")
    print("="*80)

    # Run Part 1 pipeline
    df_clean = production_part1_main(input_file, output_file)

    if df_clean is not None:
        print(f"\nğŸ“‹ SAMPLE OF CLEAN DATA:")
        print(df_clean.head())

        print(f"\nğŸ“Š FEATURE STATISTICS:")
        print(df_clean.describe())

        print(f"\nğŸ¯ SUCCESS! Dataset ready for model inference")
        print(f"ğŸ“‚ Next step: Load this clean dataset for income predictions")
    else:
        print(f"\nâŒ FAILED! Check error messages above")

# =============================================================================
# FEATURE ENGINEERING NOTES
# =============================================================================
"""
ğŸ¯ FEATURE ENGINEERING STRATEGY:

1. CATEGORICAL FEATURES (Frequency Encoding):
   - nombreempleadorcliente_consolidated_freq: TOP predictor (importance: 44,468)
   - location_x_occupation: Interaction between location and occupation

2. TEMPORAL FEATURES (Days since reference):
   - fechaingresoempleo_days: Employment tenure (experience proxy)
   - fecha_inicio_days: Account age (relationship length)
   - fecha_vencimiento_days: Time to loan maturity

3. FINANCIAL RATIO FEATURES:
   - balance_to_payment_ratio: Financial health indicator (importance: 33,232)
   - balance_coverage_ratio: Payment sustainability
   - payment_per_age: Age-adjusted payment burden

4. DIRECT FEATURES:
   - edad: Customer age (experience/earning potential)
   - saldo: Account balance (financial capacity)
   - monto_letra: Monthly payment (loan size indicator, importance: 28,950)

ğŸ”§ PRODUCTION CONSIDERATIONS:
- All features handle missing values gracefully
- Frequency encodings use training data mappings from production_frequency_mappings_catboost.pkl
- Extreme values are capped for model stability
- Data types optimized for memory efficiency (int32, float32)
- Full traceability with customer IDs preserved

âœ… VALIDATION:
- Exactly 11 features as required by XGBoost model
- All features are numeric (no categorical variables)
- Missing values handled appropriately
- Data quality checks implemented
- Feature importance rankings from final model training

ğŸ“Š EXPECTED FEATURE IMPORTANCE (from final model):
1. nombreempleadorcliente_consolidated_freq: 44,468.44 (TOP predictor)
2. balance_to_payment_ratio: 33,232.24
3. monto_letra: 28,949.64
4. fechaingresoempleo_days: 18,588.11
5. edad: 17,306.49
6. balance_coverage_ratio: 16,292.41
7. location_x_occupation: 14,862.56
8. payment_per_age: 14,638.26
9. saldo: 13,254.72
10. fecha_inicio_days: 6,144.49
11. fecha_vencimiento_days: 5,956.54

ğŸ¯ NEXT STEPS:
- Use this clean dataset for model inference
- Apply production_model_catboost_all_data.pkl for predictions
- Add confidence intervals using CI offsets: [-510.93, +755.02]
"""
