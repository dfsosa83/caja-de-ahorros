# %%
# SECTION 1: IMPORTS AND SETUP
# =============================================================================
# NOTE: Before running this notebook, install CatBoost:
# pip install catboost
# or
# conda install -c conda-forge catboost

import pandas as pd
import numpy as np
import warnings
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import (KFold, GroupKFold, cross_val_score,
                                   RandomizedSearchCV)
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.inspection import permutation_importance
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
from collections import Counter

warnings.filterwarnings('ignore')

print("================================================================================")
print("NESTED CROSS-VALIDATION INCOME PREDICTION MODEL WITH CATBOOST - STARTED")
print("================================================================================")
print("📋 Pipeline based on: model_process_with_no_tranformations.txt")
print("🎯 Primary Metrics: RMSE (Root Mean Square Error) and MAE (Mean Absolute Error)")
print("🔄 Nested CV: Outer (5-fold) for evaluation, Inner (3-fold) for hyperparameter tuning")
print("🚀 Models: Linear Regression (Baseline), XGBoost, LightGBM, Random Forest, CatBoost")

# %%
# SECTION 2: PREPARE FINAL DATASETS FOR NESTED CV MODELING
# =============================================================================
print("\n🎯 PREPARING FINAL DATASETS FOR NESTED CV")
print("-" * 60)

# EXPLICIT FEATURE SELECTION APPROACH (from original pipeline)
id_columns = ['cliente', 'identificador_unico']
target_column = 'ingresos_reportados'

# Use the selected features from the original pipeline
# NOTE: Adjust 'selected_features_final' to your actual feature list variable
feature_columns = selected_features_final

# Verify all selected features exist in the dataset
available_features = []
missing_features = []

for feature in feature_columns:
    if feature in train_df_enhanced.columns:
        available_features.append(feature)
    else:
        missing_features.append(feature)

if missing_features:
    print(f"⚠️  Missing features (will be skipped): {missing_features}")

feature_columns = available_features

print(f"   📊 Selected feature columns: {len(feature_columns)}")
print(f"   🎯 Target column: {target_column}")

# Create feature matrices and targets (from original pipeline structure)
X_train = train_df_enhanced[feature_columns].copy()
y_train = train_df_enhanced[target_column].copy()

X_valid = valid_df_enhanced[feature_columns].copy()
y_valid = valid_df_enhanced[target_column].copy()

X_test = test_df_enhanced[feature_columns].copy()
y_test = test_df_enhanced[target_column].copy()

print(f"\n📈 DATASET SHAPES:")
print(f"   X_train: {X_train.shape}")
print(f"   X_valid: {X_valid.shape}")
print(f"   X_test: {X_test.shape}")

# Combine train and validation for nested CV (test set remains untouched)
X_train_full = pd.concat([X_train, X_valid], ignore_index=True)
y_train_full = pd.concat([y_train, y_valid], ignore_index=True)

print(f"   X_train_full (for nested CV): {X_train_full.shape}")
print(f"   X_test (held out): {X_test.shape}")

# Show selected features grouped by type
print(f"\n📋 SELECTED FEATURES ({len(feature_columns)} features):")
print("-" * 60)

# Group features by type for better readability (from original pipeline)
basic_features = []
age_features = []
freq_features = []
interaction_features = []
other_features = []

for feature in feature_columns:
    if feature.startswith('age_group_'):
        age_features.append(feature)
    elif feature.endswith('_freq'):
        freq_features.append(feature)
    elif '_x_' in feature or 'retired_x_' in feature or 'employer_x_' in feature or 'gender_x_' in feature:
        interaction_features.append(feature)
    elif feature in ['edad', 'letras_mensuales', 'monto_letra', 'saldo', 'is_retired']:
        basic_features.append(feature)
    else:
        other_features.append(feature)

print(f"🔢 BASIC FEATURES ({len(basic_features)}): {basic_features}")
print(f"👥 AGE GROUP FEATURES ({len(age_features)}): {age_features}")
print(f"📊 FREQUENCY FEATURES ({len(freq_features)}): {freq_features}")
print(f"⚡ INTERACTION FEATURES ({len(interaction_features)}): {interaction_features}")
print(f"🔧 OTHER FEATURES ({len(other_features)}): {other_features}")

# Verify data quality
print(f"\n✅ DATA QUALITY CHECKS:")
print(f"   Missing values in X_train_full: {X_train_full.isnull().sum().sum()}")
print(f"   Missing values in y_train_full: {y_train_full.isnull().sum()}")
print(f"   All features numeric: {all(X_train_full.dtypes.apply(lambda x: x in ['int64', 'float64']))}")

# Save feature list to file for reference
feature_list_df = pd.DataFrame({
    'feature_name': feature_columns,
    'feature_type': ['basic' if f in basic_features else
                    'age_group' if f in age_features else
                    'frequency' if f in freq_features else
                    'interaction' if f in interaction_features else
                    'other' for f in feature_columns]
})

feature_list_df.to_csv(data_path + '/nested_cv_catboost_feature_list.csv', index=False)
print(f"\n💾 Feature list saved to: nested_cv_catboost_feature_list.csv")

# %%
# SECTION 2.5: EXTRACT AND SAVE FREQUENCY MAPPINGS FOR PRODUCTION
# =============================================================================
print("\n💾 EXTRACTING FREQUENCY MAPPINGS FOR PRODUCTION DEPLOYMENT")
print("-" * 60)

# Extract frequency mappings from the training data for production use
# These mappings ensure consistent encoding when predicting single customers

frequency_mappings = {}

# Find frequency-encoded features in your selected features
freq_features_in_model = [f for f in feature_columns if f.endswith('_freq')]

print(f"   📊 Found {len(freq_features_in_model)} frequency-encoded features:")
for freq_feature in freq_features_in_model:
    print(f"      • {freq_feature}")

# Extract mappings from the enhanced training dataframes
# We'll use the full training data (train + validation) to get complete mappings
print(f"\n🔍 Extracting frequency mappings from training data...")

for freq_feature in freq_features_in_model:
    if freq_feature in train_df_enhanced.columns:
        # Get the original categorical column name
        original_col = freq_feature.replace('_freq', '')
        
        if original_col in train_df_enhanced.columns:
            # Extract the frequency mapping from training data
            freq_mapping = train_df_enhanced[original_col].value_counts().to_dict()
            frequency_mappings[freq_feature] = freq_mapping
            
            print(f"   ✅ {freq_feature}:")
            print(f"      Original column: {original_col}")
            print(f"      Categories: {len(freq_mapping)}")
            print(f"      Top 3: {list(freq_mapping.keys())[:3]}")
        else:
            print(f"   ⚠️ Original column '{original_col}' not found for {freq_feature}")
    else:
        print(f"   ⚠️ Frequency feature '{freq_feature}' not found in training data")

# Save frequency mappings for production use
import pickle
import json

# Save as pickle for Python production systems
frequency_mappings_path = data_path + '/production_frequency_mappings_catboost.pkl'
with open(frequency_mappings_path, 'wb') as f:
    pickle.dump(frequency_mappings, f)

# Save as JSON for cross-platform compatibility
frequency_mappings_json_path = data_path + '/production_frequency_mappings_catboost.json'
with open(frequency_mappings_json_path, 'w') as f:
    json.dump(frequency_mappings, f, indent=2)

print(f"\n💾 FREQUENCY MAPPINGS SAVED:")
print(f"   📦 Pickle format: production_frequency_mappings_catboost.pkl")
print(f"   📄 JSON format: production_frequency_mappings_catboost.json")
print(f"   🎯 Total mappings: {len(frequency_mappings)}")

# Create a summary of the mappings for documentation
mappings_summary = {}
for freq_feature, mapping in frequency_mappings.items():
    mappings_summary[freq_feature] = {
        'total_categories': len(mapping),
        'top_5_categories': dict(list(mapping.items())[:5]),
        'min_frequency': min(mapping.values()),
        'max_frequency': max(mapping.values()),
        'others_frequency': mapping.get('Others', 'Not found')
    }

# Save summary for documentation
summary_path = data_path + '/frequency_mappings_summary_catboost.json'
with open(summary_path, 'w') as f:
    json.dump(mappings_summary, f, indent=2)

print(f"   📋 Summary saved: frequency_mappings_summary_catboost.json")

print(f"\n🎯 PRODUCTION USAGE:")
print(f"   1. Load mappings: frequency_mappings = pickle.load(open('production_frequency_mappings_catboost.pkl', 'rb'))")
print(f"   2. Apply to new customer: customer['ocupacion_consolidated_freq'] = frequency_mappings['ocupacion_consolidated_freq'][customer['ocupacion_consolidated']]")
print(f"   3. Handle new values: Map unknown categories to 'Others' frequency")

print(f"\n✅ FREQUENCY MAPPINGS EXTRACTION COMPLETE!")

EXPLANATION WHY SAVE FREQUENCY MAPPINGS

🎯 PREPARING FINAL DATASETS FOR NESTED CV
------------------------------------------------------------
   📊 Selected feature columns: 11
   🎯 Target column: ingresos_reportados

📈 DATASET SHAPES:
   X_train: (27177, 11)
   X_valid: (2632, 11)
   X_test: (1316, 11)
   X_train_full (for nested CV): (29809, 11)
   X_test (held out): (1316, 11)

📋 SELECTED FEATURES (11 features):
------------------------------------------------------------
🔢 BASIC FEATURES (3): ['edad', 'saldo', 'monto_letra']
👥 AGE GROUP FEATURES (0): []
📊 FREQUENCY FEATURES (1): ['nombreempleadorcliente_consolidated_freq']
⚡ INTERACTION FEATURES (1): ['location_x_occupation']
🔧 OTHER FEATURES (6): ['fechaingresoempleo_days', 'balance_to_payment_ratio', 'fecha_inicio_days', 'fecha_vencimiento_days', 'balance_coverage_ratio', 'payment_per_age']

✅ DATA QUALITY CHECKS:
   Missing values in X_train_full: 0
   Missing values in y_train_full: 0
   All features numeric: False

💾 Feature list saved to: nested_cv_catboost_feature_list.csv

💾 EXTRACTING FREQUENCY MAPPINGS FOR PRODUCTION DEPLOYMENT
------------------------------------------------------------
   📊 Found 1 frequency-encoded features:
      • nombreempleadorcliente_consolidated_freq

🔍 Extracting frequency mappings from training data...
   ⚠️ Original column 'nombreempleadorcliente_consolidated' not found for nombreempleadorcliente_consolidated_freq

💾 FREQUENCY MAPPINGS SAVED:
   📦 Pickle format: production_frequency_mappings_catboost.pkl
   📄 JSON format: production_frequency_mappings_catboost.json
   🎯 Total mappings: 0
   📋 Summary saved: frequency_mappings_summary_catboost.json

wHY SCALING AND SAVE TOO EXPLANATION
# %%
# SECTION 3: FEATURE SCALING
# =============================================================================
print("\n⚖️ FEATURE SCALING")
print("-" * 50)

# Apply robust scaling to features (from original pipeline)
print("   ⚖️ Applying RobustScaler...")
scaler = RobustScaler()

# Fit scaler on full training data (train + validation combined)
X_train_full_scaled = pd.DataFrame(
    scaler.fit_transform(X_train_full),
    columns=X_train_full.columns,
    index=X_train_full.index
)

# Transform test set using the same scaler
X_test_scaled = pd.DataFrame(
    scaler.transform(X_test),
    columns=X_test.columns,
    index=X_test.index
)

print("   ✅ Feature scaling complete")
print(f"   📊 Scaled training data: {X_train_full_scaled.shape}")
print(f"   📊 Scaled test data: {X_test_scaled.shape}")

# %%
# SECTION 3: FEATURE SCALING
# =============================================================================
print("\n⚖️ FEATURE SCALING")
print("-" * 50)

# Apply robust scaling to features (from original pipeline)
print("   ⚖️ Applying RobustScaler...")
scaler = RobustScaler()

# Fit scaler on full training data (train + validation combined)
X_train_full_scaled = pd.DataFrame(
    scaler.fit_transform(X_train_full),
    columns=X_train_full.columns,
    index=X_train_full.index
)

# Transform test set using the same scaler
X_test_scaled = pd.DataFrame(
    scaler.transform(X_test),
    columns=X_test.columns,
    index=X_test.index
)

print("   ✅ Feature scaling complete")
print(f"   📊 Scaled training data: {X_train_full_scaled.shape}")
print(f"   📊 Scaled test data: {X_test_scaled.shape}")

🔄 NESTED CROSS-VALIDATION SETUP
------------------------------------------------------------
   🔄 Outer CV: 5 folds (for unbiased model evaluation)
   🔄 Inner CV: 3 folds (for hyperparameter tuning)
   🔍 Random Search: 25 iterations per inner fold
   📊 Total model trainings: 375 per model
      (5 outer × 3 inner × 25 iterations)

# %%
# SECTION 5: MODEL DEFINITIONS AND HYPERPARAMETER GRIDS WITH CATBOOST
# =============================================================================
print("\n🤖 MODEL DEFINITIONS AND HYPERPARAMETER GRIDS WITH CATBOOST")
print("-" * 60)

# Base models (from simple to complex: Linear Regression → Tree-based → Gradient Boosting)
base_models = {
    'Linear Regression': LinearRegression(n_jobs=-1),
    'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1),
    'XGBoost': xgb.XGBRegressor(random_state=42, n_jobs=-1),
    'LightGBM': lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),
    'CatBoost': cb.CatBoostRegressor(random_state=42, verbose=False, thread_count=-1)
}

# Comprehensive hyperparameter grids for nested CV
# These grids are designed for income prediction optimization
param_grids = {
    'Linear Regression': {
        # Linear Regression has no hyperparameters to tune
        # We include an empty dict to maintain consistency with the nested CV framework
    },
    'Random Forest': {
        'n_estimators': [200, 300, 400],
        'max_depth': [10, 15, 20, None],
        'min_samples_split': [5, 10, 15],
        'min_samples_leaf': [2, 5, 10],
        'max_features': ['sqrt', 'log2', 0.8],
        'max_samples': [0.7, 0.8, 0.9]
    },
    'XGBoost': {
        'n_estimators': [800, 1100],
        'max_depth': [6, 8, 10],
        'learning_rate': [0.005, 0.007, 0.009],
        'subsample': [0.8, 0.85, 0.9],
        'colsample_bytree': [0.8, 0.85, 0.9],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [0.5, 1.0, 2.0],
        'min_child_weight': [1, 3, 5]
    },
    'LightGBM': {
        'n_estimators': [800, 1100],
        'max_depth': [6, 8, 10],
        'learning_rate': [0.005, 0.007, 0.01],
        'subsample': [0.7, 0.8, 0.85],
        'colsample_bytree': [0.7, 0.8, 0.85],
        'num_leaves': [30, 50, 80],
        'min_child_samples': [30, 40, 50],
        'reg_alpha': [0.5, 1.0, 1.5],
        'reg_lambda': [2.0, 3.0, 4.0]
        },
    'CatBoost': {
        'iterations': [800, 1100],
        'depth': [6, 8, 10],
        'learning_rate': [0.005, 0.007, 0.01],
        'subsample': [0.8, 0.85, 0.9],
        'colsample_bylevel': [0.8, 0.85, 0.9],
        'l2_leaf_reg': [1, 3, 5],
        'border_count': [32, 64, 128],
        'bagging_temperature': [0, 0.5, 1.0]
    }
}

print(f"   🤖 Models defined: {list(base_models.keys())}")
print(f"   🔧 Hyperparameter search space per model:")
for model_name, grid in param_grids.items():
    total_combinations = np.prod([len(values) for values in grid.values()])
    print(f"      {model_name}: {total_combinations:,} total combinations")

print(f"\n💡 Primary Optimization Metric: RMSE (Root Mean Square Error)")
print(f"   📊 RMSE penalizes large prediction errors more heavily")
print(f"   💰 Better suited for income prediction than R² alone")
print(f"   📈 Lower RMSE = Better model performance")
print(f"\n🚀 CatBoost Advantages:")
print(f"   🎯 Excellent with categorical features (even after encoding)")
print(f"   🔧 Built-in regularization and overfitting protection")
print(f"   ⚡ Often competitive with XGBoost/LightGBM")
print(f"   📊 Robust hyperparameter defaults")

# %%
# SECTION 6: NESTED CROSS-VALIDATION IMPLEMENTATION
# =============================================================================
print("\n🎯 NESTED CROSS-VALIDATION IMPLEMENTATION")
print("=" * 70)

def nested_cross_validation(model, param_grid, X, y, outer_cv, inner_cv, model_name):
    """
    Perform nested cross-validation for unbiased model evaluation with robust metrics focus

    This implementation prioritizes RMSE and MAE as primary metrics for income prediction,
    while still tracking R² for comparison purposes.

    Args:
        model: Base model to evaluate
        param_grid: Hyperparameter grid for tuning
        X, y: Training data and target
        outer_cv, inner_cv: Cross-validation objects
        model_name: Name for reporting

    Returns:
        dict: Comprehensive nested CV results with robust metrics
    """
    print(f"\n🔄 Starting Nested CV for {model_name}")
    print(f"   📊 Outer folds: {outer_cv.n_splits}, Inner folds: {inner_cv.n_splits}")
    print(f"   🎯 Primary metrics: RMSE (lower is better), MAE (lower is better)")

    # Storage for results - prioritizing robust metrics
    outer_scores_rmse = []
    outer_scores_mae = []
    outer_scores_r2 = []  # Still track R² for comparison
    best_params_per_fold = []
    fold_details = []

    # Outer CV loop - each iteration gives unbiased performance estimate
    for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X, y), 1):
        print(f"\n   🔄 Outer Fold {fold_idx}/{outer_cv.n_splits}")

        # Split data for this outer fold
        X_train_outer = X.iloc[train_idx]
        X_val_outer = X.iloc[val_idx]
        y_train_outer = y.iloc[train_idx]
        y_val_outer = y.iloc[val_idx]

        print(f"      📊 Fold {fold_idx} sizes: Train={len(train_idx)}, Val={len(val_idx)}")

        # Inner CV: Hyperparameter tuning using RMSE as optimization metric
        print(f"      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...")

        # Handle Linear Regression (no hyperparameters to tune)
        if model_name == 'Linear Regression':
            # Linear Regression has no hyperparameters, so just fit the model
            best_model = model
            best_model.fit(X_train_outer, y_train_outer)
            best_params = {}  # No parameters to store
            best_params_per_fold.append(best_params)

            # Calculate RMSE on inner CV for consistency
            inner_cv_scores = cross_val_score(model, X_train_outer, y_train_outer,
                                            cv=inner_cv, scoring='neg_mean_squared_error', n_jobs=-1)
            best_inner_rmse = np.sqrt(-inner_cv_scores.mean())
            print(f"      ✅ Linear Regression CV RMSE: ${best_inner_rmse:.2f} (no hyperparameters to tune)")
        else:
            # For other models, perform hyperparameter search
            random_search = RandomizedSearchCV(
                estimator=model,
                param_distributions=param_grid,
                n_iter=RANDOM_SEARCH_ITERATIONS,  # Use global variable
                cv=inner_cv,
                scoring='neg_mean_squared_error',  # Optimizing RMSE (MSE negated)
                n_jobs=-1,
                random_state=42,
                verbose=0
            )

            # Fit hyperparameter search on outer training data
            random_search.fit(X_train_outer, y_train_outer)

            # Get best model from inner CV
            best_model = random_search.best_estimator_
            best_params = random_search.best_params_
            best_params_per_fold.append(best_params)

            # Convert negative MSE back to RMSE for reporting
            best_inner_rmse = np.sqrt(-random_search.best_score_)
            print(f"      ✅ Best inner CV RMSE: ${best_inner_rmse:.2f}")

        # Evaluate best model on outer validation fold
        y_pred_outer = best_model.predict(X_val_outer)

        # Calculate comprehensive metrics for this outer fold
        fold_rmse = np.sqrt(mean_squared_error(y_val_outer, y_pred_outer))
        fold_mae = mean_absolute_error(y_val_outer, y_pred_outer)
        fold_r2 = r2_score(y_val_outer, y_pred_outer)

        # Store scores
        outer_scores_rmse.append(fold_rmse)
        outer_scores_mae.append(fold_mae)
        outer_scores_r2.append(fold_r2)

        print(f"      📊 Outer fold performance:")
        print(f"         🎯 RMSE: ${fold_rmse:.2f}")
        print(f"         🎯 MAE:  ${fold_mae:.2f}")
        print(f"         📈 R²:   {fold_r2:.4f}")

        # Calculate additional income-specific metrics
        # Mean Absolute Percentage Error (MAPE) - but handle low incomes carefully
        valid_mask = y_val_outer > 100  # Avoid division by very small numbers
        if valid_mask.sum() > 0:
            mape = np.mean(np.abs((y_val_outer[valid_mask] - y_pred_outer[valid_mask]) / y_val_outer[valid_mask])) * 100
            print(f"         💰 MAPE (>$100): {mape:.1f}%")
        else:
            mape = np.nan

        # Store detailed results for this fold
        fold_details.append({
            'fold': fold_idx,
            'rmse': fold_rmse,
            'mae': fold_mae,
            'r2': fold_r2,
            'mape': mape,
            'best_params': best_params,
            'inner_cv_rmse': best_inner_rmse,
            'train_size': len(train_idx),
            'val_size': len(val_idx),
            'y_true_mean': y_val_outer.mean(),
            'y_pred_mean': y_pred_outer.mean()
        })

    # Calculate final nested CV results with robust metrics emphasis
    nested_cv_results = {
        'model_name': model_name,
        # Primary metrics (RMSE/MAE)
        'outer_cv_rmse_mean': np.mean(outer_scores_rmse),
        'outer_cv_rmse_std': np.std(outer_scores_rmse),
        'outer_cv_mae_mean': np.mean(outer_scores_mae),
        'outer_cv_mae_std': np.std(outer_scores_mae),
        # Secondary metric (R²)
        'outer_cv_r2_mean': np.mean(outer_scores_r2),
        'outer_cv_r2_std': np.std(outer_scores_r2),
        # Raw scores for analysis
        'outer_scores_rmse': outer_scores_rmse,
        'outer_scores_mae': outer_scores_mae,
        'outer_scores_r2': outer_scores_r2,
        # Hyperparameter analysis
        'best_params_per_fold': best_params_per_fold,
        'fold_details': fold_details,
        # Model selection criteria (lower is better for RMSE/MAE)
        'selection_metric': 'rmse',  # Primary metric for model selection
        'selection_score': np.mean(outer_scores_rmse)
    }

    print(f"\n   🏆 {model_name} Nested CV Summary:")
    print(f"      🎯 RMSE: ${nested_cv_results['outer_cv_rmse_mean']:.2f} ± ${nested_cv_results['outer_cv_rmse_std']:.2f}")
    print(f"      🎯 MAE:  ${nested_cv_results['outer_cv_mae_mean']:.2f} ± ${nested_cv_results['outer_cv_mae_std']:.2f}")
    print(f"      📈 R²:   {nested_cv_results['outer_cv_r2_mean']:.4f} ± {nested_cv_results['outer_cv_r2_std']:.4f}")

    return nested_cv_results

print("✅ Nested CV function defined with robust metrics focus")


# %%
# SECTION 7: RUN NESTED CROSS-VALIDATION FOR ALL MODELS (INCLUDING CATBOOST)
# =============================================================================
print("\n🚀 RUNNING NESTED CROSS-VALIDATION FOR ALL MODELS (BASELINE TO ADVANCED)")
print("=" * 70)
print("⚠️  This will take 30-90 minutes depending on your hardware...")
print(f"   Each complex model will be trained {OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS} times")
print(f"   ({OUTER_CV_FOLDS} outer × {INNER_CV_FOLDS} inner × {RANDOM_SEARCH_ITERATIONS} iterations)")
print("🎯 Optimizing for RMSE (Root Mean Square Error) - best metric for income prediction")
print("📊 Model progression: Linear Regression (baseline) → Tree-based → Gradient Boosting")
print("🚀 Including CatBoost for comprehensive model comparison!")

# Storage for all nested CV results
nested_cv_results = {}
total_start_time = datetime.now()

# Run nested CV for each model
for model_idx, (model_name, base_model) in enumerate(base_models.items(), 1):
    print(f"\n{'='*80}")
    print(f"NESTED CV {model_idx}/{len(base_models)}: {model_name.upper()}")
    print(f"{'='*80}")

    model_start_time = datetime.now()

    # Run nested cross-validation
    results = nested_cross_validation(
        model=base_model,
        param_grid=param_grids[model_name],
        X=X_train_full_scaled,  # Use full training data (train + validation)
        y=y_train_full,
        outer_cv=outer_cv,
        inner_cv=inner_cv,
        model_name=model_name
    )

    nested_cv_results[model_name] = results

    model_end_time = datetime.now()
    model_duration = (model_end_time - model_start_time).total_seconds() / 60

    print(f"\n   ⏱️ {model_name} completed in {model_duration:.1f} minutes")

    # Show progress
    remaining_models = len(base_models) - model_idx
    if remaining_models > 0:
        estimated_remaining = model_duration * remaining_models
        print(f"   📊 Progress: {model_idx}/{len(base_models)} models complete")
        print(f"   ⏰ Estimated remaining time: {estimated_remaining:.1f} minutes")

total_end_time = datetime.now()
total_duration = (total_end_time - total_start_time).total_seconds() / 60

print(f"\n🎉 ALL NESTED CV COMPLETED (BASELINE TO ADVANCED MODELS)!")
print(f"⏱️ Total execution time: {total_duration:.1f} minutes")


-FIND A WAY TO REPRESENT THIS:
🚀 RUNNING NESTED CROSS-VALIDATION FOR ALL MODELS (BASELINE TO ADVANCED)
======================================================================
⚠️  This will take 30-90 minutes depending on your hardware...
   Each complex model will be trained 375 times
   (5 outer × 3 inner × 25 iterations)
🎯 Optimizing for RMSE (Root Mean Square Error) - best metric for income prediction
📊 Model progression: Linear Regression (baseline) → Tree-based → Gradient Boosting
🚀 Including CatBoost for comprehensive model comparison!

================================================================================
NESTED CV 1/5: LINEAR REGRESSION
================================================================================

🔄 Starting Nested CV for Linear Regression
   📊 Outer folds: 5, Inner folds: 3
   🎯 Primary metrics: RMSE (lower is better), MAE (lower is better)

   🔄 Outer Fold 1/5
      📊 Fold 1 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Linear Regression CV RMSE: $647.86 (no hyperparameters to tune)
      📊 Outer fold performance:
         🎯 RMSE: $647.60
         🎯 MAE:  $520.60
         📈 R²:   0.1222
         💰 MAPE (>$100): 49.9%

   🔄 Outer Fold 2/5
      📊 Fold 2 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Linear Regression CV RMSE: $649.23 (no hyperparameters to tune)
      📊 Outer fold performance:
         🎯 RMSE: $641.87
         🎯 MAE:  $516.10
         📈 R²:   0.1124
         💰 MAPE (>$100): 50.7%

   🔄 Outer Fold 3/5
      📊 Fold 3 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Linear Regression CV RMSE: $648.34 (no hyperparameters to tune)
      📊 Outer fold performance:
         🎯 RMSE: $643.67
         🎯 MAE:  $513.38
         📈 R²:   0.1127
         💰 MAPE (>$100): 49.6%

   🔄 Outer Fold 4/5
      📊 Fold 4 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Linear Regression CV RMSE: $645.62 (no hyperparameters to tune)
      📊 Outer fold performance:
         🎯 RMSE: $657.39
         🎯 MAE:  $527.04
         📈 R²:   0.1044
         💰 MAPE (>$100): 51.2%

   🔄 Outer Fold 5/5
      📊 Fold 5 sizes: Train=23848, Val=5961
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Linear Regression CV RMSE: $647.80 (no hyperparameters to tune)
      📊 Outer fold performance:
         🎯 RMSE: $646.00
         🎯 MAE:  $516.38
         📈 R²:   0.1189
         💰 MAPE (>$100): 50.5%

   🏆 Linear Regression Nested CV Summary:
      🎯 RMSE: $647.31 ± $5.41
      🎯 MAE:  $518.70 ± $4.77
      📈 R²:   0.1141 ± 0.0061

   ⏱️ Linear Regression completed in 0.2 minutes
   📊 Progress: 1/5 models complete
   ⏰ Estimated remaining time: 0.9 minutes

================================================================================
NESTED CV 2/5: RANDOM FOREST
================================================================================

🔄 Starting Nested CV for Random Forest
   📊 Outer folds: 5, Inner folds: 3
   🎯 Primary metrics: RMSE (lower is better), MAE (lower is better)

   🔄 Outer Fold 1/5
      📊 Fold 1 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $548.87
      📊 Outer fold performance:
         🎯 RMSE: $531.15
         🎯 MAE:  $386.73
         📈 R²:   0.4095
         💰 MAPE (>$100): 36.0%

   🔄 Outer Fold 2/5
      📊 Fold 2 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $547.66
      📊 Outer fold performance:
         🎯 RMSE: $538.22
         🎯 MAE:  $392.28
         📈 R²:   0.3759
         💰 MAPE (>$100): 37.9%

   🔄 Outer Fold 3/5
      📊 Fold 3 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $550.55
      📊 Outer fold performance:
         🎯 RMSE: $529.54
         🎯 MAE:  $383.97
         📈 R²:   0.3994
         💰 MAPE (>$100): 36.0%

   🔄 Outer Fold 4/5
      📊 Fold 4 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $545.39
      📊 Outer fold performance:
         🎯 RMSE: $546.78
         🎯 MAE:  $397.17
         📈 R²:   0.3804
         💰 MAPE (>$100): 37.3%

   🔄 Outer Fold 5/5
      📊 Fold 5 sizes: Train=23848, Val=5961
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $548.15
      📊 Outer fold performance:
         🎯 RMSE: $532.90
         🎯 MAE:  $384.93
         📈 R²:   0.4004
         💰 MAPE (>$100): 36.8%

   🏆 Random Forest Nested CV Summary:
      🎯 RMSE: $535.72 ± $6.26
      🎯 MAE:  $389.02 ± $4.99
      📈 R²:   0.3931 ± 0.0128

   ⏱️ Random Forest completed in 18.7 minutes
   📊 Progress: 2/5 models complete
   ⏰ Estimated remaining time: 56.2 minutes

================================================================================
NESTED CV 3/5: XGBOOST
================================================================================

🔄 Starting Nested CV for XGBoost
   📊 Outer folds: 5, Inner folds: 3
   🎯 Primary metrics: RMSE (lower is better), MAE (lower is better)

   🔄 Outer Fold 1/5
      📊 Fold 1 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $540.55
      📊 Outer fold performance:
         🎯 RMSE: $524.64
         🎯 MAE:  $377.31
         📈 R²:   0.4239
         💰 MAPE (>$100): 35.0%

   🔄 Outer Fold 2/5
      📊 Fold 2 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $539.77
      📊 Outer fold performance:
         🎯 RMSE: $528.23
         🎯 MAE:  $382.61
         📈 R²:   0.3989
         💰 MAPE (>$100): 36.8%

   🔄 Outer Fold 3/5
      📊 Fold 3 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $542.93
      📊 Outer fold performance:
         🎯 RMSE: $523.09
         🎯 MAE:  $375.26
         📈 R²:   0.4140
         💰 MAPE (>$100): 35.0%

   🔄 Outer Fold 4/5
      📊 Fold 4 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $535.86
      📊 Outer fold performance:
         🎯 RMSE: $539.42
         🎯 MAE:  $387.19
         📈 R²:   0.3970
         💰 MAPE (>$100): 36.3%

   🔄 Outer Fold 5/5
      📊 Fold 5 sizes: Train=23848, Val=5961
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $540.16
      📊 Outer fold performance:
         🎯 RMSE: $525.93
         🎯 MAE:  $377.01
         📈 R²:   0.4160
         💰 MAPE (>$100): 35.8%

   🏆 XGBoost Nested CV Summary:
      🎯 RMSE: $528.26 ± $5.83
      🎯 MAE:  $379.88 ± $4.41
      📈 R²:   0.4099 ± 0.0104

   ⏱️ XGBoost completed in 18.8 minutes
   📊 Progress: 3/5 models complete
   ⏰ Estimated remaining time: 37.5 minutes

================================================================================
NESTED CV 4/5: LIGHTGBM
================================================================================

🔄 Starting Nested CV for LightGBM
   📊 Outer folds: 5, Inner folds: 3
   🎯 Primary metrics: RMSE (lower is better), MAE (lower is better)

   🔄 Outer Fold 1/5
      📊 Fold 1 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $550.58
      📊 Outer fold performance:
         🎯 RMSE: $541.45
         🎯 MAE:  $396.69
         📈 R²:   0.3864
         💰 MAPE (>$100): 36.7%

   🔄 Outer Fold 2/5
      📊 Fold 2 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $550.10
      📊 Outer fold performance:
         🎯 RMSE: $545.73
         🎯 MAE:  $401.88
         📈 R²:   0.3584
         💰 MAPE (>$100): 38.5%

   🔄 Outer Fold 3/5
      📊 Fold 3 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $552.72
      📊 Outer fold performance:
         🎯 RMSE: $539.40
         🎯 MAE:  $392.68
         📈 R²:   0.3769
         💰 MAPE (>$100): 36.6%

   🔄 Outer Fold 4/5
      📊 Fold 4 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $547.29
      📊 Outer fold performance:
         🎯 RMSE: $553.35
         🎯 MAE:  $402.96
         📈 R²:   0.3655
         💰 MAPE (>$100): 37.6%

   🔄 Outer Fold 5/5
      📊 Fold 5 sizes: Train=23848, Val=5961
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $551.08
      📊 Outer fold performance:
         🎯 RMSE: $541.12
         🎯 MAE:  $393.73
         📈 R²:   0.3817
         💰 MAPE (>$100): 37.3%

   🏆 LightGBM Nested CV Summary:
      🎯 RMSE: $544.21 ± $5.02
      🎯 MAE:  $397.59 ± $4.17
      📈 R²:   0.3738 ± 0.0104

   ⏱️ LightGBM completed in 16.4 minutes
   📊 Progress: 4/5 models complete
   ⏰ Estimated remaining time: 16.4 minutes

================================================================================
NESTED CV 5/5: CATBOOST
================================================================================

🔄 Starting Nested CV for CatBoost
   📊 Outer folds: 5, Inner folds: 3
   🎯 Primary metrics: RMSE (lower is better), MAE (lower is better)

   🔄 Outer Fold 1/5
      📊 Fold 1 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $554.49
      📊 Outer fold performance:
         🎯 RMSE: $547.07
         🎯 MAE:  $406.54
         📈 R²:   0.3736
         💰 MAPE (>$100): 37.8%

   🔄 Outer Fold 2/5
      📊 Fold 2 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $553.29
      📊 Outer fold performance:
         🎯 RMSE: $548.08
         🎯 MAE:  $408.25
         📈 R²:   0.3528
         💰 MAPE (>$100): 39.2%

   🔄 Outer Fold 3/5
      📊 Fold 3 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $555.10
      📊 Outer fold performance:
         🎯 RMSE: $544.00
         🎯 MAE:  $400.96
         📈 R²:   0.3662
         💰 MAPE (>$100): 37.7%

   🔄 Outer Fold 4/5
      📊 Fold 4 sizes: Train=23847, Val=5962
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $549.96
      📊 Outer fold performance:
         🎯 RMSE: $557.60
         🎯 MAE:  $411.15
         📈 R²:   0.3557
         💰 MAPE (>$100): 38.5%

   🔄 Outer Fold 5/5
      📊 Fold 5 sizes: Train=23848, Val=5961
      🔧 Inner CV: Hyperparameter tuning (optimizing RMSE)...
      ✅ Best inner CV RMSE: $553.65
      📊 Outer fold performance:
         🎯 RMSE: $546.92
         🎯 MAE:  $402.93
         📈 R²:   0.3684
         💰 MAPE (>$100): 38.2%

   🏆 CatBoost Nested CV Summary:
      🎯 RMSE: $548.73 ± $4.64
      🎯 MAE:  $405.96 ± $3.65
      📈 R²:   0.3633 ± 0.0078

   ⏱️ CatBoost completed in 49.2 minutes

🎉 ALL NESTED CV COMPLETED (BASELINE TO ADVANCED MODELS)!
⏱️ Total execution time: 103.3 minutes

# %%
# SECTION 8: NESTED CV RESULTS ANALYSIS AND MODEL COMPARISON (BASELINE TO ADVANCED)
# =============================================================================
print("\n📊 NESTED CV RESULTS ANALYSIS AND MODEL COMPARISON (BASELINE TO ADVANCED)")
print("=" * 70)

# Create comprehensive comparison DataFrame with robust metrics focus
comparison_data = []
for model_name, results in nested_cv_results.items():
    comparison_data.append({
        'Model': model_name,
        # Primary metrics (lower is better)
        'Nested_CV_RMSE_Mean': results['outer_cv_rmse_mean'],
        'Nested_CV_RMSE_Std': results['outer_cv_rmse_std'],
        'Nested_CV_MAE_Mean': results['outer_cv_mae_mean'],
        'Nested_CV_MAE_Std': results['outer_cv_mae_std'],
        # Secondary metric
        'Nested_CV_R2_Mean': results['outer_cv_r2_mean'],
        'Nested_CV_R2_Std': results['outer_cv_r2_std'],
        # Selection score (RMSE for ranking)
        'Selection_Score': results['selection_score']
    })

nested_comparison_df = pd.DataFrame(comparison_data)
# Sort by RMSE (lower is better) - primary metric for income prediction
nested_comparison_df = nested_comparison_df.sort_values('Nested_CV_RMSE_Mean', ascending=True)

print("\n🏆 NESTED CV PERFORMANCE COMPARISON (Sorted by RMSE - Lower is Better):")
print("=" * 90)
print(nested_comparison_df.round(4).to_string(index=False))

# Identify best model based on RMSE (robust metric for income prediction)
best_model_nested = nested_comparison_df.iloc[0]['Model']
best_rmse_nested = nested_comparison_df.iloc[0]['Nested_CV_RMSE_Mean']
best_rmse_std_nested = nested_comparison_df.iloc[0]['Nested_CV_RMSE_Std']
best_mae_nested = nested_comparison_df.iloc[0]['Nested_CV_MAE_Mean']
best_mae_std_nested = nested_comparison_df.iloc[0]['Nested_CV_MAE_Std']
best_r2_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Mean']
best_r2_std_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Std']

print(f"\n🥇 BEST MODEL (Based on RMSE): {best_model_nested}")
print(f"   🎯 Unbiased RMSE: ${best_rmse_nested:.2f} ± ${best_rmse_std_nested:.2f}")
print(f"   🎯 Unbiased MAE:  ${best_mae_nested:.2f} ± ${best_mae_std_nested:.2f}")
print(f"   📈 Unbiased R²:   {best_r2_nested:.4f} ± {best_r2_std_nested:.4f}")

# Calculate confidence intervals for RMSE (primary metric)
rmse_ci_lower = best_rmse_nested - 1.96 * best_rmse_std_nested
rmse_ci_upper = best_rmse_nested + 1.96 * best_rmse_std_nested
print(f"   📊 95% Confidence Interval (RMSE): [${rmse_ci_lower:.2f}, ${rmse_ci_upper:.2f}]")

# Calculate confidence intervals for MAE and R² as well
mae_ci_lower = best_mae_nested - 1.96 * best_mae_std_nested
mae_ci_upper = best_mae_nested + 1.96 * best_mae_std_nested
print(f"   📊 95% Confidence Interval (MAE): [${mae_ci_lower:.2f}, ${mae_ci_upper:.2f}]")

r2_ci_lower = best_r2_nested - 1.96 * best_r2_std_nested
r2_ci_upper = best_r2_nested + 1.96 * best_r2_std_nested
print(f"   📊 95% Confidence Interval (R²): [{r2_ci_lower:.4f}, {r2_ci_upper:.4f}]")

# Performance assessment based on RMSE
print(f"\n📈 PERFORMANCE ASSESSMENT:")
if best_rmse_nested <= 600:
    performance_level = "EXCELLENT"
    emoji = "🎉"
elif best_rmse_nested <= 800:
    performance_level = "GOOD"
    emoji = "✅"
elif best_rmse_nested <= 1000:
    performance_level = "ACCEPTABLE"
    emoji = "👍"
else:
    performance_level = "NEEDS IMPROVEMENT"
    emoji = "⚠️"

print(f"   {emoji} Performance Level: {performance_level}")
print(f"   💰 RMSE = ${best_rmse_nested:.2f} (Average prediction error)")
print(f"   💰 MAE = ${best_mae_nested:.2f} (Median prediction error)")

# Model ranking summary
print(f"\n📋 MODEL RANKING (by RMSE):")
for idx, row in nested_comparison_df.iterrows():
    rank = nested_comparison_df.index.get_loc(idx) + 1
    model = row['Model']
    rmse = row['Nested_CV_RMSE_Mean']
    rmse_std = row['Nested_CV_RMSE_Std']
    print(f"   {rank}. {model:<15}: RMSE = ${rmse:.2f} ± ${rmse_std:.2f}")

print(f"\n💡 INTERPRETATION:")
print(f"   🎯 RMSE measures average prediction error in dollars")
print(f"   🎯 MAE measures typical prediction error (less sensitive to outliers)")
print(f"   📈 R² measures proportion of variance explained (0-1 scale)")
print(f"   ✅ Lower RMSE/MAE = Better model for income prediction")
print(f"   📊 Linear Regression provides interpretable baseline performance")
print(f"   🚀 CatBoost often excels with categorical features and provides robust results")

📊 NESTED CV RESULTS ANALYSIS AND MODEL COMPARISON (BASELINE TO ADVANCED)
======================================================================

🏆 NESTED CV PERFORMANCE COMPARISON (Sorted by RMSE - Lower is Better):
==========================================================================================
            Model  Nested_CV_RMSE_Mean  Nested_CV_RMSE_Std  Nested_CV_MAE_Mean  Nested_CV_MAE_Std  Nested_CV_R2_Mean  Nested_CV_R2_Std  Selection_Score
          XGBoost             528.2633              5.8262            379.8761             4.4069             0.4099            0.0104         528.2633
    Random Forest             535.7196              6.2556            389.0154             4.9912             0.3931            0.0128         535.7196
         LightGBM             544.2096              5.0221            397.5885             4.1737             0.3738            0.0104         544.2096
         CatBoost             548.7342              4.6356            405.9648             3.6512             0.3633            0.0078         548.7342
Linear Regression             647.3062              5.4079            518.7030             4.7674             0.1141            0.0061         647.3062

🥇 BEST MODEL (Based on RMSE): XGBoost
   🎯 Unbiased RMSE: $528.26 ± $5.83
   🎯 Unbiased MAE:  $379.88 ± $4.41
   📈 Unbiased R²:   0.4099 ± 0.0104
   📊 95% Confidence Interval (RMSE): [$516.84, $539.68]
   📊 95% Confidence Interval (MAE): [$371.24, $388.51]
   📊 95% Confidence Interval (R²): [0.3896, 0.4303]

📈 PERFORMANCE ASSESSMENT:
   🎉 Performance Level: EXCELLENT
   💰 RMSE = $528.26 (Average prediction error)
   💰 MAE = $379.88 (Median prediction error)

📋 MODEL RANKING (by RMSE):
   1. XGBoost        : RMSE = $528.26 ± $5.83
   2. Random Forest  : RMSE = $535.72 ± $6.26
   3. LightGBM       : RMSE = $544.21 ± $5.02
   4. CatBoost       : RMSE = $548.73 ± $4.64
   5. Linear Regression: RMSE = $647.31 ± $5.41

💡 INTERPRETATION:
   🎯 RMSE measures average prediction error in dollars
   🎯 MAE measures typical prediction error (less sensitive to outliers)
   📈 R² measures proportion of variance explained (0-1 scale)
   ✅ Lower RMSE/MAE = Better model for income prediction
   📊 Linear Regression provides interpretable baseline performance
   🚀 CatBoost often excels with categorical features and provides robust results

# %%
# SECTION 8.5: BASELINE COMPARISON ANALYSIS
# =============================================================================
print("\n📈 BASELINE COMPARISON ANALYSIS")
print("-" * 60)

if 'Linear Regression' in nested_cv_results:
    lr_results = nested_cv_results['Linear Regression']
    lr_rmse = lr_results['outer_cv_rmse_mean']
    lr_rmse_std = lr_results['outer_cv_rmse_std']
    
    # CORRECTED: Use the same ranking method as Section 8
    lr_row = nested_comparison_df[nested_comparison_df['Model'] == 'Linear Regression']
    lr_idx = lr_row.index[0]
    lr_rank = nested_comparison_df.index.get_loc(lr_idx) + 1
    
    print(f"🎯 LINEAR REGRESSION BASELINE PERFORMANCE:")
    print(f"   📊 RMSE: ${lr_rmse:.2f} ± ${lr_rmse_std:.2f}")
    print(f"   🏆 Ranking: #{lr_rank} out of {len(base_models)} models")
    
    # Calculate improvement over baseline for each model
    print(f"\n📊 IMPROVEMENT OVER LINEAR REGRESSION BASELINE:")
    for idx, row in nested_comparison_df.iterrows():
        model = row['Model']
        rmse = row['Nested_CV_RMSE_Mean']
        
        if model != 'Linear Regression':
            improvement = ((lr_rmse - rmse) / lr_rmse) * 100
            improvement_dollars = lr_rmse - rmse
            
            if improvement > 0:
                print(f"   ✅ {model:<15}: {improvement:+5.1f}% improvement (${improvement_dollars:+6.0f})")
            else:
                print(f"   ❌ {model:<15}: {improvement:+5.1f}% worse (${improvement_dollars:+6.0f})")
    
    # Business interpretation - CORRECTED LOGIC
    print(f"\n💡 BASELINE INSIGHTS:")
    if lr_rank == len(base_models):  # Linear Regression is LAST (worst) - rank 5
        print(f"   ✅ EXCELLENT! Complex models significantly outperform baseline")
        print(f"   💼 Investment in advanced algorithms is clearly justified")
        print(f"   🚀 Linear Regression serves as proof that complexity adds substantial value")
    elif lr_rank >= len(base_models) - 1:  # Linear Regression is among worst 2
        print(f"   ✅ Complex models substantially outperform baseline")
        print(f"   💼 Advanced algorithms provide meaningful improvements")
    elif lr_rank <= 2:  # Linear Regression is among best 2
        print(f"   🚨 LINEAR REGRESSION PERFORMS SURPRISINGLY WELL! Complex models may be overfitting.")
        print(f"   💼 Consider using Linear Regression for production (simpler, interpretable)")
    else:  # Linear Regression is in middle
        print(f"   👍 Linear Regression performs competitively")
        print(f"   💼 Complex models provide modest but meaningful improvements")
    
    # Additional insights based on improvement magnitude
    best_improvement = max([((lr_rmse - row['Nested_CV_RMSE_Mean']) / lr_rmse) * 100 
                           for _, row in nested_comparison_df.iterrows() 
                           if row['Model'] != 'Linear Regression'])
    
    print(f"\n🎯 COMPLEXITY VALUE ASSESSMENT:")
    if best_improvement >= 15:
        print(f"   🎉 OUTSTANDING: Best model improves {best_improvement:.1f}% over baseline")
        print(f"   💰 Strong business case for complex models")
    elif best_improvement >= 10:
        print(f"   ✅ GOOD: Best model improves {best_improvement:.1f}% over baseline")
        print(f"   💼 Solid justification for model complexity")
    elif best_improvement >= 5:
        print(f"   👍 MODERATE: Best model improves {best_improvement:.1f}% over baseline")
        print(f"   ⚖️ Consider cost-benefit of complexity")
    else:
        print(f"   ⚠️ MINIMAL: Best model improves only {best_improvement:.1f}% over baseline")
        print(f"   🤔 Question whether complexity is worth it")
    
    print(f"\n🔍 LINEAR REGRESSION ADVANTAGES (as baseline):")
    print(f"   ✅ Highly interpretable (feature coefficients)")
    print(f"   ✅ Fast training and prediction")
    print(f"   ✅ No hyperparameter tuning needed")
    print(f"   ✅ Robust to overfitting")
    print(f"   ✅ Easy to deploy and maintain")
    print(f"   📊 Serves as performance floor - complex models must beat this!")

else:
    print("   ⚠️ Linear Regression results not found.")

📈 BASELINE COMPARISON ANALYSIS
------------------------------------------------------------
🎯 LINEAR REGRESSION BASELINE PERFORMANCE:
   📊 RMSE: $647.31 ± $5.41
   🏆 Ranking: #5 out of 5 models

📊 IMPROVEMENT OVER LINEAR REGRESSION BASELINE:
   ✅ XGBoost        : +18.4% improvement ($  +119)
   ✅ Random Forest  : +17.2% improvement ($  +112)
   ✅ LightGBM       : +15.9% improvement ($  +103)
   ✅ CatBoost       : +15.2% improvement ($   +99)

💡 BASELINE INSIGHTS:
   ✅ EXCELLENT! Complex models significantly outperform baseline
   💼 Investment in advanced algorithms is clearly justified
   🚀 Linear Regression serves as proof that complexity adds substantial value

🎯 COMPLEXITY VALUE ASSESSMENT:
   🎉 OUTSTANDING: Best model improves 18.4% over baseline
   💰 Strong business case for complex models

🔍 LINEAR REGRESSION ADVANTAGES (as baseline):
   ✅ Highly interpretable (feature coefficients)
   ✅ Fast training and prediction
   ✅ No hyperparameter tuning needed
   ✅ Robust to overfitting
   ✅ Easy to deploy and maintain
   📊 Serves as performance floor - complex models must beat this!

# %%
# SECTION 9: CATBOOST SPECIFIC ANALYSIS
# =============================================================================
print("\n🚀 CATBOOST SPECIFIC ANALYSIS")
print("-" * 60)

if 'CatBoost' in nested_cv_results:
    catboost_results = nested_cv_results['CatBoost']
    catboost_rmse = catboost_results['outer_cv_rmse_mean']
    catboost_rmse_std = catboost_results['outer_cv_rmse_std']
    catboost_rank = nested_comparison_df[nested_comparison_df['Model'] == 'CatBoost'].index[0] + 1

    print(f"🎯 CATBOOST PERFORMANCE:")
    print(f"   📊 RMSE: ${catboost_rmse:.2f} ± ${catboost_rmse_std:.2f}")
    print(f"   🏆 Ranking: #{catboost_rank} out of {len(base_models)} models")

    # Compare with other gradient boosting models
    if 'XGBoost' in nested_cv_results and 'LightGBM' in nested_cv_results:
        xgb_rmse = nested_cv_results['XGBoost']['outer_cv_rmse_mean']
        lgb_rmse = nested_cv_results['LightGBM']['outer_cv_rmse_mean']

        print(f"\n📊 GRADIENT BOOSTING COMPARISON:")
        print(f"   CatBoost: ${catboost_rmse:.2f}")
        print(f"   XGBoost:  ${xgb_rmse:.2f}")
        print(f"   LightGBM: ${lgb_rmse:.2f}")

        if catboost_rmse <= min(xgb_rmse, lgb_rmse):
            print(f"   🥇 CatBoost WINS among gradient boosting models!")
        elif catboost_rmse <= max(xgb_rmse, lgb_rmse):
            print(f"   🥈 CatBoost performs competitively with other gradient boosting models")
        else:
            print(f"   📊 CatBoost provides alternative perspective to XGBoost/LightGBM")

    print(f"\n🚀 CATBOOST ADVANTAGES FOR THIS DATASET:")
    print(f"   🎯 Excellent handling of categorical features (even after frequency encoding)")
    print(f"   🔧 Built-in overfitting protection and regularization")
    print(f"   ⚡ Often requires less hyperparameter tuning than XGBoost/LightGBM")
    print(f"   📊 Robust performance across different data distributions")
    print(f"   💰 Well-suited for financial/income prediction tasks")

    # Hyperparameter stability analysis for CatBoost
    catboost_params = catboost_results['best_params_per_fold']
    print(f"\n🔧 CATBOOST HYPERPARAMETER STABILITY:")

    # Check most common parameters
    param_stability = {}
    for param_name in ['iterations', 'depth', 'learning_rate']:
        if param_name in catboost_params[0]:
            values = [params.get(param_name) for params in catboost_params]
            unique_values = len(set(values))
            most_common = max(set(values), key=values.count)
            param_stability[param_name] = {
                'unique_values': unique_values,
                'most_common': most_common,
                'stability': 'High' if unique_values <= 2 else 'Moderate' if unique_values <= 3 else 'Low'
            }

    for param, info in param_stability.items():
        print(f"   {param}: {info['stability']} stability (most common: {info['most_common']})")

else:
    print("   ⚠️ CatBoost results not found. Make sure CatBoost was included in the model comparison.")

print(f"\n✅ CATBOOST ANALYSIS COMPLETE!")

# %%
# FINAL SUMMARY: COMPREHENSIVE MODEL COMPARISON WITH CATBOOST
# =============================================================================
print("\n🎉 FINAL SUMMARY: COMPREHENSIVE MODEL COMPARISON WITH CATBOOST")
print("=" * 80)

print(f"📊 MODELS EVALUATED: {len(base_models)} (Baseline to Advanced)")
for i, model in enumerate(base_models.keys(), 1):
    if model == 'Linear Regression':
        print(f"   {i}. {model} (📊 BASELINE)")
    elif model in ['XGBoost', 'LightGBM', 'CatBoost']:
        print(f"   {i}. {model} (🚀 ADVANCED)")
    else:
        print(f"   {i}. {model}")

print(f"\n🏆 FINAL RANKINGS (by RMSE):")
for idx, row in nested_comparison_df.iterrows():
    rank = nested_comparison_df.index.get_loc(idx) + 1
    model = row['Model']
    rmse = row['Nested_CV_RMSE_Mean']
    rmse_std = row['Nested_CV_RMSE_Std']

    if rank == 1:
        emoji = "🥇"
    elif rank == 2:
        emoji = "🥈"
    elif rank == 3:
        emoji = "🥉"
    else:
        emoji = f"{rank}."

    print(f"   {emoji} {model}: ${rmse:.2f} ± ${rmse_std:.2f}")

print(f"\n🎯 WINNER: {best_model_nested}")
print(f"   💰 Expected RMSE: ${best_rmse_nested:.2f}")
print(f"   📊 This represents the average prediction error in dollars")

print(f"\n📊 BASELINE TO ADVANCED PROGRESSION:")
print(f"   ✅ Linear Regression: Interpretable baseline performance")
print(f"   ✅ Random Forest: Tree-based ensemble approach")
print(f"   ✅ XGBoost/LightGBM/CatBoost: Advanced gradient boosting")
print(f"   ✅ Comprehensive comparison across algorithmic complexity")
print(f"   ✅ Demonstrates value of complex models vs simple baseline")

print(f"\n💾 NEXT STEPS:")
print(f"   1. Use the best model ({best_model_nested}) for final training")
print(f"   2. Apply saved frequency mappings for production predictions")
print(f"   3. Monitor model performance with expected RMSE: ${best_rmse_nested:.2f}")
print(f"   4. Consider ensemble approaches if multiple models perform similarly")

print(f"\n🎉 NESTED CV WITH CATBOOST PIPELINE COMPLETE!")

🚀 CATBOOST SPECIFIC ANALYSIS
------------------------------------------------------------
🎯 CATBOOST PERFORMANCE:
   📊 RMSE: $548.73 ± $4.64
   🏆 Ranking: #5 out of 5 models

📊 GRADIENT BOOSTING COMPARISON:
   CatBoost: $548.73
   XGBoost:  $528.26
   LightGBM: $544.21
   📊 CatBoost provides alternative perspective to XGBoost/LightGBM

🚀 CATBOOST ADVANTAGES FOR THIS DATASET:
   🎯 Excellent handling of categorical features (even after frequency encoding)
   🔧 Built-in overfitting protection and regularization
   ⚡ Often requires less hyperparameter tuning than XGBoost/LightGBM
   📊 Robust performance across different data distributions
   💰 Well-suited for financial/income prediction tasks

🔧 CATBOOST HYPERPARAMETER STABILITY:
   iterations: High stability (most common: 1100)
   depth: High stability (most common: 10)
   learning_rate: High stability (most common: 0.01)

✅ CATBOOST ANALYSIS COMPLETE!

🎉 FINAL SUMMARY: COMPREHENSIVE MODEL COMPARISON WITH CATBOOST
================================================================================
📊 MODELS EVALUATED: 5 (Baseline to Advanced)
   1. Linear Regression (📊 BASELINE)
   2. Random Forest
   3. XGBoost (🚀 ADVANCED)
   4. LightGBM (🚀 ADVANCED)
   5. CatBoost (🚀 ADVANCED)

🏆 FINAL RANKINGS (by RMSE):
   🥇 XGBoost: $528.26 ± $5.83
   🥈 Random Forest: $535.72 ± $6.26
   🥉 LightGBM: $544.21 ± $5.02
   4. CatBoost: $548.73 ± $4.64
   5. Linear Regression: $647.31 ± $5.41

🎯 WINNER: XGBoost
   💰 Expected RMSE: $528.26
   📊 This represents the average prediction error in dollars

📊 BASELINE TO ADVANCED PROGRESSION:
   ✅ Linear Regression: Interpretable baseline performance
   ✅ Random Forest: Tree-based ensemble approach
   ✅ XGBoost/LightGBM/CatBoost: Advanced gradient boosting
   ✅ Comprehensive comparison across algorithmic complexity
   ✅ Demonstrates value of complex models vs simple baseline

💾 NEXT STEPS:
   1. Use the best model (XGBoost) for final training
   2. Apply saved frequency mappings for production predictions
   3. Monitor model performance with expected RMSE: $528.26
   4. Consider ensemble approaches if multiple models perform similarly

# %%
# SECTION 10: HYPERPARAMETER ANALYSIS AND STABILITY ASSESSMENT
# =============================================================================
print("\n🔧 HYPERPARAMETER ANALYSIS AND STABILITY ASSESSMENT")
print("-" * 70)

# Analyze hyperparameter consistency across folds for model robustness
for model_name, results in nested_cv_results.items():
    print(f"\n📋 {model_name} - Hyperparameter Stability Analysis:")
    print("-" * 50)

    best_params_list = results['best_params_per_fold']

    # Get all unique parameter names
    all_param_names = set()
    for params in best_params_list:
        all_param_names.update(params.keys())

    # Analyze each parameter for consistency
    stable_params = 0
    total_params = len(all_param_names)

    # Handle models with no hyperparameters (like Linear Regression)
    if total_params == 0:
        print("   📊 No hyperparameters to analyze (model has no tunable parameters)")
        print("   ✅ PERFECT STABILITY: No hyperparameters = No hyperparameter sensitivity")
        continue

    for param_name in sorted(all_param_names):
        param_values = [params.get(param_name, 'N/A') for params in best_params_list]
        unique_values = list(set(param_values))

        if len(unique_values) == 1:
            consistency = "✅ STABLE"
            stable_params += 1
        elif len(unique_values) == 2:
            consistency = "⚠️ MODERATE"
        else:
            consistency = "❌ UNSTABLE"

        # Show parameter values across folds
        value_counts = Counter(param_values)
        most_common = value_counts.most_common(1)[0]

        print(f"   {param_name:<25}: {consistency}")
        print(f"      Values: {param_values}")
        print(f"      Most frequent: {most_common[0]} ({most_common[1]}/{len(param_values)} folds)")

    # Calculate stability percentage
    stability_pct = (stable_params / total_params) * 100
    print(f"\n   📊 Hyperparameter Stability: {stable_params}/{total_params} stable ({stability_pct:.1f}%)")

    if stability_pct >= 80:
        print("   ✅ HIGH STABILITY: Model hyperparameters are robust across data splits")
    elif stability_pct >= 60:
        print("   👍 MODERATE STABILITY: Some hyperparameter variation across splits")
    else:
        print("   ⚠️ LOW STABILITY: High hyperparameter sensitivity to data splits")

# %%
# SECTION 11: NESTED CV RESULTS ANALYSIS
# =============================================================================
print("\n📊 NESTED CV RESULTS ANALYSIS")
print("=" * 60)

# Create comparison DataFrame
comparison_data = []
for model_name, results in nested_cv_results.items():
    comparison_data.append({
        'Model': model_name,
        'Nested_CV_R2_Mean': results['outer_cv_r2_mean'],
        'Nested_CV_R2_Std': results['outer_cv_r2_std'],
        'Nested_CV_RMSE_Mean': results['outer_cv_rmse_mean'],
        'Nested_CV_RMSE_Std': results['outer_cv_rmse_std'],
        'Nested_CV_MAE_Mean': results['outer_cv_mae_mean'],
        'Nested_CV_MAE_Std': results['outer_cv_mae_std']
    })

nested_comparison_df = pd.DataFrame(comparison_data)
nested_comparison_df = nested_comparison_df.sort_values('Nested_CV_R2_Mean', ascending=False)

print("\n🏆 NESTED CV PERFORMANCE COMPARISON:")
print("=" * 80)
print(nested_comparison_df.round(4).to_string(index=False))

# Identify best model from nested CV
best_model_nested = nested_comparison_df.iloc[0]['Model']
best_r2_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Mean']
best_r2_std_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Std']

print(f"\n🥇 BEST MODEL (Nested CV): {best_model_nested}")
print(f"   📊 Unbiased R²: {best_r2_nested:.4f} ± {best_r2_std_nested:.4f}")
print(f"   📊 95% Confidence Interval: [{best_r2_nested - 1.96*best_r2_std_nested:.4f}, {best_r2_nested + 1.96*best_r2_std_nested:.4f}]")


📊 NESTED CV RESULTS ANALYSIS
============================================================

🏆 NESTED CV PERFORMANCE COMPARISON:
================================================================================
            Model  Nested_CV_R2_Mean  Nested_CV_R2_Std  Nested_CV_RMSE_Mean  Nested_CV_RMSE_Std  Nested_CV_MAE_Mean  Nested_CV_MAE_Std
          XGBoost             0.4099            0.0104             528.2633              5.8262            379.8761             4.4069
    Random Forest             0.3931            0.0128             535.7196              6.2556            389.0154             4.9912
         LightGBM             0.3738            0.0104             544.2096              5.0221            397.5885             4.1737
         CatBoost             0.3633            0.0078             548.7342              4.6356            405.9648             3.6512
Linear Regression             0.1141            0.0061             647.3062              5.4079            518.7030             4.7674

🥇 BEST MODEL (Nested CV): XGBoost
   📊 Unbiased R²: 0.4099 ± 0.0104
   📊 95% Confidence Interval: [0.3896, 0.4303]

# %%
# SECTION 12: FINAL MODEL TRAINING WITH AGGREGATED BEST HYPERPARAMETERS
# =============================================================================
print("\n🎯 FINAL MODEL TRAINING WITH AGGREGATED BEST HYPERPARAMETERS")
print("=" * 70)

def get_most_frequent_params(best_params_list):
    """
    Get the most frequently selected hyperparameters across CV folds
    This provides robust hyperparameter selection for the final model
    """
    # Get all parameter names
    all_param_names = set()
    for params in best_params_list:
        all_param_names.update(params.keys())

    # Find most frequent value for each parameter
    final_params = {}
    for param_name in all_param_names:
        param_values = [params.get(param_name) for params in best_params_list if param_name in params]
        if param_values:
            # Get most common value
            most_common = Counter(param_values).most_common(1)[0][0]
            final_params[param_name] = most_common

    return final_params

# Get best hyperparameters for the winning model
best_model_results = nested_cv_results[best_model_nested]
best_params_final = get_most_frequent_params(best_model_results['best_params_per_fold'])

print(f"🏆 Training final {best_model_nested} model with aggregated best parameters:")
print("📋 Final hyperparameters (most frequent across CV folds):")
for param, value in sorted(best_params_final.items()):
    print(f"   {param:<25}: {value}")

# Create and train final model on full training data
print(f"\n🚀 Training final {best_model_nested} model...")
final_model = base_models[best_model_nested].set_params(**best_params_final)
final_model.fit(X_train_full_scaled, y_train_full)

print(f"✅ Final {best_model_nested} model trained on full training set")
print(f"   📊 Training data: {X_train_full_scaled.shape[0]:,} samples")
print(f"   🔧 Features: {len(feature_columns)} variables")
print(f"   🎯 Expected RMSE: ${best_rmse_nested:.2f} ± ${best_rmse_std_nested:.2f}")

🎯 FINAL MODEL TRAINING WITH AGGREGATED BEST HYPERPARAMETERS
======================================================================
🏆 Training final XGBoost model with aggregated best parameters:
📋 Final hyperparameters (most frequent across CV folds):
   colsample_bytree         : 0.8
   learning_rate            : 0.007
   max_depth                : 10
   min_child_weight         : 1
   n_estimators             : 1100
   reg_alpha                : 0.5
   reg_lambda               : 1.0
   subsample                : 0.9

🚀 Training final XGBoost model...
✅ Final XGBoost model trained on full training set
   📊 Training data: 29,809 samples
   🔧 Features: 11 variables
   🎯 Expected RMSE: $528.26 ± $5.83

# %%
# SECTION 13: FINAL MODEL EVALUATION ON TEST SET
# =============================================================================
print("\n🎯 FINAL MODEL EVALUATION ON TEST SET")
print("-" * 60)

# Make predictions on test set (held-out data never seen during nested CV)
y_pred_test = final_model.predict(X_test_scaled)

# Calculate comprehensive test metrics
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
test_mae = mean_absolute_error(y_test, y_pred_test)
test_r2 = r2_score(y_test, y_pred_test)

# Calculate additional income-specific metrics
# MAPE for incomes > $100 (avoid division by very small numbers)
valid_mask = y_test > 100
if valid_mask.sum() > 0:
    test_mape = np.mean(np.abs((y_test[valid_mask] - y_pred_test[valid_mask]) / y_test[valid_mask])) * 100
else:
    test_mape = np.nan

print(f"🏆 FINAL TEST PERFORMANCE ({best_model_nested}):")
print("=" * 60)
print(f"   🎯 Test RMSE: ${test_rmse:.2f}")
print(f"   🎯 Test MAE:  ${test_mae:.2f}")
print(f"   📈 Test R²:   {test_r2:.4f}")
if not np.isnan(test_mape):
    print(f"   💰 Test MAPE (>$100): {test_mape:.1f}%")

# Compare with nested CV estimates (validation of nested CV effectiveness)
print(f"\n📊 NESTED CV vs TEST SET COMPARISON:")
print("-" * 50)
print(f"   Metric    | Nested CV Estimate | Test Set | Difference")
print(f"   ----------|-------------------|----------|----------")
print(f"   RMSE      | ${best_rmse_nested:.2f} ± ${best_rmse_std_nested:.2f}     | ${test_rmse:.2f}     | ${abs(test_rmse - best_rmse_nested):.2f}")
print(f"   MAE       | ${best_mae_nested:.2f} ± ${best_mae_std_nested:.2f}     | ${test_mae:.2f}     | ${abs(test_mae - best_mae_nested):.2f}")
print(f"   R²        | {best_r2_nested:.4f} ± {best_r2_std_nested:.4f} | {test_r2:.4f}   | {abs(test_r2 - best_r2_nested):.4f}")

# Assess nested CV prediction accuracy
rmse_within_ci = abs(test_rmse - best_rmse_nested) <= 2 * best_rmse_std_nested
mae_within_ci = abs(test_mae - best_mae_nested) <= 2 * best_mae_std_nested
r2_within_ci = abs(test_r2 - best_r2_nested) <= 2 * best_r2_std_nested

print(f"\n✅ NESTED CV VALIDATION:")
print(f"   RMSE within 95% CI: {'✅ YES' if rmse_within_ci else '⚠️ NO'}")
print(f"   MAE within 95% CI:  {'✅ YES' if mae_within_ci else '⚠️ NO'}")
print(f"   R² within 95% CI:   {'✅ YES' if r2_within_ci else '⚠️ NO'}")

if rmse_within_ci and mae_within_ci:
    print("   🎉 EXCELLENT: Nested CV provided accurate performance estimates!")
elif rmse_within_ci or mae_within_ci:
    print("   👍 GOOD: Nested CV estimates reasonably accurate")
else:
    print("   ⚠️ WARNING: Test performance differs significantly from nested CV estimates")

# Target distribution comparison (from original pipeline)
print(f"\n📈 TARGET DISTRIBUTION COMPARISON:")
print(f"   Training Full - Mean: ${y_train_full.mean():,.2f}, Std: ${y_train_full.std():,.2f}")
print(f"   Test Set      - Mean: ${y_test.mean():,.2f}, Std: ${y_test.std():,.2f}")
print(f"   Predictions   - Mean: ${y_pred_test.mean():,.2f}, Std: ${y_pred_test.std():,.2f}")


🎯 FINAL MODEL EVALUATION ON TEST SET
------------------------------------------------------------
🏆 FINAL TEST PERFORMANCE (XGBoost):
============================================================
   🎯 Test RMSE: $589.79
   🎯 Test MAE:  $425.28
   📈 Test R²:   0.2756
   💰 Test MAPE (>$100): 39.0%

📊 NESTED CV vs TEST SET COMPARISON:
--------------------------------------------------
   Metric    | Nested CV Estimate | Test Set | Difference
   ----------|-------------------|----------|----------
   RMSE      | $528.26 ± $5.83     | $589.79     | $61.53
   MAE       | $379.88 ± $4.41     | $425.28     | $45.40
   R²        | 0.4099 ± 0.0104 | 0.2756   | 0.1344

✅ NESTED CV VALIDATION:
   RMSE within 95% CI: ⚠️ NO
   MAE within 95% CI:  ⚠️ NO
   R² within 95% CI:   ⚠️ NO
   ⚠️ WARNING: Test performance differs significantly from nested CV estimates

📈 TARGET DISTRIBUTION COMPARISON:
   Training Full - Mean: $1,311.30, Std: $687.77
   Test Set      - Mean: $1,332.00, Std: $693.21
   Predictions   - Mean: $1,315.43, Std: $394.52

# %%
# SECTION 14: FINAL MODEL TRAINING WITH BEST HYPERPARAMETERS
# =============================================================================
print("\n🎯 FINAL MODEL TRAINING WITH BEST HYPERPARAMETERS")
print("=" * 60)

def get_most_frequent_params(best_params_list):
    """
    Get the most frequently selected hyperparameters across CV folds
    """
    from collections import Counter

    # Get all parameter names
    all_param_names = set()
    for params in best_params_list:
        all_param_names.update(params.keys())

    # Find most frequent value for each parameter
    final_params = {}
    for param_name in all_param_names:
        param_values = [params.get(param_name) for params in best_params_list if param_name in params]
        if param_values:
            # Get most common value
            most_common = Counter(param_values).most_common(1)[0][0]
            final_params[param_name] = most_common

    return final_params

# Get best hyperparameters for the winning model
best_model_results = nested_cv_results[best_model_nested]
best_params_final = get_most_frequent_params(best_model_results['best_params_per_fold'])

print(f"🏆 Training final {best_model_nested} model with aggregated best parameters:")
print("📋 Final hyperparameters (most frequent across CV folds):")
for param, value in best_params_final.items():
    print(f"   {param}: {value}")

# Create and train final model
final_model = base_models[best_model_nested].set_params(**best_params_final)
final_model.fit(X_train_full_scaled, y_train_full)

print(f"\n✅ Final {best_model_nested} model trained on full training set")


# %%
# SECTION 15: PERMUTATION IMPORTANCE ANALYSIS
# =============================================================================
print("\n🔍 PERMUTATION IMPORTANCE ANALYSIS")
print("-" * 60)

# Calculate permutation importance on test set (from original pipeline)
print("🔄 Calculating permutation importance (this may take a few minutes)...")
perm_importance = permutation_importance(
    final_model,
    X_test_scaled,
    y_test,
    n_repeats=15,  # More repeats for robust estimates
    random_state=42,
    scoring='neg_mean_squared_error'  # Use RMSE-based scoring
)

# Fix the importance calculation - use the raw importance values
# The permutation importance already gives us the decrease in performance
feature_importance_mean = -perm_importance.importances_mean  # Convert from negative to positive
feature_importance_std = perm_importance.importances_std

# Create importance DataFrame
importance_df = pd.DataFrame({
    'feature': feature_columns,
    'importance_mean': feature_importance_mean,
    'importance_std': feature_importance_std
}).sort_values('importance_mean', ascending=False)

print(f"\n📊 TOP 15 MOST IMPORTANT FEATURES (by MSE increase when permuted):")
print("-" * 60)
for i, (_, row) in enumerate(importance_df.head(15).iterrows(), 1):
    print(f"   {i:2d}. {row['feature']:<30} {row['importance_mean']:>8.4f} ± {row['importance_std']:>6.4f}")

# Save importance results
importance_df.to_csv(data_path + '/nested_cv_catboost_permutation_importance.csv', index=False)
print(f"\n💾 Permutation importance saved to: nested_cv_catboost_permutation_importance.csv")

# Create permutation importance visualization (from original pipeline)
print(f"\n📈 Creating permutation importance visualization...")

plt.figure(figsize=(12, 8))

# Get top 20 features for visualization
num_features_to_plot = min(20, len(feature_importance_mean))
top_features = importance_df.head(num_features_to_plot)

# Create horizontal bar plot
y_pos = np.arange(num_features_to_plot)
bars = plt.barh(y_pos, top_features['importance_mean'],
                xerr=top_features['importance_std'],
                capsize=3, alpha=0.7, color='steelblue')

plt.yticks(y_pos, top_features['feature'])
plt.xlabel('MSE Increase When Feature Permuted')
plt.title(f'Permutation Importance - Top {num_features_to_plot} Features\n({best_model_nested} Model)')
plt.grid(True, alpha=0.3, axis='x')

# Add value labels on bars
for i, (bar, mean_val) in enumerate(zip(bars, top_features['importance_mean'])):
    plt.text(bar.get_width() + max(top_features['importance_mean']) * 0.01,
             bar.get_y() + bar.get_height()/2,
             f'{mean_val:.4f}',
             va='center', fontsize=8)

plt.tight_layout()

# Save plot
plt.savefig(data_path + '/nested_cv_catboost_permutation_importance.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"✅ Permutation importance visualization saved to: nested_cv_catboost_permutation_importance.png")

# Feature importance insights
print(f"\n💡 FEATURE IMPORTANCE INSIGHTS:")
top_5_features = importance_df.head(5)['feature'].tolist()
print(f"   🔝 Top 5 features: {', '.join(top_5_features)}")

# Analyze feature types in top 10
top_10_features = importance_df.head(10)['feature'].tolist()
top_basic = [f for f in top_10_features if f in basic_features]
top_age = [f for f in top_10_features if f in age_features]
top_interaction = [f for f in top_10_features if f in interaction_features]

print(f"   📊 In top 10 - Basic: {len(top_basic)}, Age: {len(top_age)}, Interaction: {len(top_interaction)}")
print(f"   💰 Feature importance measured as MSE increase when feature is randomly shuffled")

# Additional analysis: Check for zero importance features
zero_importance = importance_df[importance_df['importance_mean'] <= 0]
if len(zero_importance) > 0:
    print(f"\n⚠️  Features with zero or negative importance: {len(zero_importance)}")
    print("   These features may not be contributing to model performance")
else:
    print(f"\n✅ All features show positive importance")

# Show importance statistics
print(f"\n📊 IMPORTANCE STATISTICS:")
print(f"   Mean importance: {importance_df['importance_mean'].mean():.4f}")
print(f"   Std importance: {importance_df['importance_mean'].std():.4f}")
print(f"   Max importance: {importance_df['importance_mean'].max():.4f}")
print(f"   Min importance: {importance_df['importance_mean'].min():.4f}")

✅ Permutation importance visualization saved to: nested_cv_catboost_permutation_importance.png

💡 FEATURE IMPORTANCE INSIGHTS:
   🔝 Top 5 features: fecha_vencimiento_days, fecha_inicio_days, saldo, payment_per_age, location_x_occupation
   📊 In top 10 - Basic: 3, Age: 0, Interaction: 1
   💰 Feature importance measured as MSE increase when feature is randomly shuffled

⚠️  Features with zero or negative importance: 11
   These features may not be contributing to model performance

📊 IMPORTANCE STATISTICS:
   Mean importance: -21048.4303
   Std importance: 15966.4198
   Max importance: -6958.5396
   Min importance: -64406.0875

# %%
# SECTION 16: COMPREHENSIVE VISUALIZATIONS
# =============================================================================
print("\n📈 CREATING COMPREHENSIVE NESTED CV VISUALIZATIONS")
print("-" * 60)

# Create comprehensive visualization dashboard
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('Nested Cross-Validation Results Dashboard (with CatBoost)', fontsize=16, fontweight='bold')

# 1. Model comparison by RMSE (primary metric)
ax1 = axes[0, 0]
models = nested_comparison_df['Model']
rmse_means = nested_comparison_df['Nested_CV_RMSE_Mean']
rmse_stds = nested_comparison_df['Nested_CV_RMSE_Std']

bars = ax1.bar(models, rmse_means, yerr=rmse_stds, capsize=5, alpha=0.7,
               color=['gold' if m == best_model_nested else 'lightblue' for m in models])
ax1.set_ylabel('RMSE ($)')
ax1.set_title('Model Comparison by RMSE (Lower is Better)')
ax1.grid(True, alpha=0.3)
ax1.tick_params(axis='x', rotation=45)

# Add value labels
for bar, mean, std in zip(bars, rmse_means, rmse_stds):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 10,
             f'${mean:.0f}±{std:.0f}', ha='center', va='bottom', fontsize=9)

# 2. RMSE scores across CV folds for best model
ax2 = axes[0, 1]
best_rmse_scores = nested_cv_results[best_model_nested]['outer_scores_rmse']
fold_numbers = range(1, len(best_rmse_scores) + 1)

ax2.plot(fold_numbers, best_rmse_scores, 'o-', linewidth=2, markersize=8, color='red')
ax2.axhline(y=np.mean(best_rmse_scores), color='red', linestyle='--', alpha=0.7,
            label=f'Mean: ${np.mean(best_rmse_scores):.0f}')
ax2.fill_between(fold_numbers,
                 np.mean(best_rmse_scores) - np.std(best_rmse_scores),
                 np.mean(best_rmse_scores) + np.std(best_rmse_scores),
                 alpha=0.2, color='red')
ax2.set_xlabel('CV Fold')
ax2.set_ylabel('RMSE ($)')
ax2.set_title(f'{best_model_nested} - RMSE Across CV Folds')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. MAE comparison
ax3 = axes[0, 2]
mae_means = nested_comparison_df['Nested_CV_MAE_Mean']
mae_stds = nested_comparison_df['Nested_CV_MAE_Std']

bars = ax3.bar(models, mae_means, yerr=mae_stds, capsize=5, alpha=0.7,
               color=['coral' if m == best_model_nested else 'lightgreen' for m in models])
ax3.set_ylabel('MAE ($)')
ax3.set_title('Model Comparison by MAE (Lower is Better)')
ax3.grid(True, alpha=0.3)
ax3.tick_params(axis='x', rotation=45)

# 4. Nested CV vs Test Set comparison
ax4 = axes[1, 0]
metrics = ['RMSE', 'MAE', 'R²']
nested_values = [best_rmse_nested, best_mae_nested, best_r2_nested]
test_values = [test_rmse, test_mae, test_r2]

x = np.arange(len(metrics))
width = 0.35

bars1 = ax4.bar(x - width/2, nested_values, width, label='Nested CV', alpha=0.7, color='skyblue')
bars2 = ax4.bar(x + width/2, test_values, width, label='Test Set', alpha=0.7, color='orange')

ax4.set_ylabel('Value')
ax4.set_title('Nested CV vs Test Set Performance')
ax4.set_xticks(x)
ax4.set_xticklabels(metrics)
ax4.legend()
ax4.grid(True, alpha=0.3)

# 5. Prediction vs Actual scatter plot
ax5 = axes[1, 1]
ax5.scatter(y_test, y_pred_test, alpha=0.6, s=20)
min_val = min(y_test.min(), y_pred_test.min())
max_val = max(y_test.max(), y_pred_test.max())
ax5.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
ax5.set_xlabel('Actual Income ($)')
ax5.set_ylabel('Predicted Income ($)')
ax5.set_title(f'Predictions vs Actual ({best_model_nested})')
ax5.legend()
ax5.grid(True, alpha=0.3)

# 6. Residuals plot
ax6 = axes[1, 2]
residuals = y_test - y_pred_test
ax6.scatter(y_pred_test, residuals, alpha=0.6, s=20)
ax6.axhline(y=0, color='r', linestyle='--', linewidth=2)
ax6.set_xlabel('Predicted Income ($)')
ax6.set_ylabel('Residuals ($)')
ax6.set_title('Residuals Plot')
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(data_path + '/nested_cv_catboost_comprehensive_results.png', dpi=300, bbox_inches='tight')
plt.show()

print("   ✅ Comprehensive visualizations saved to: nested_cv_catboost_comprehensive_results.png")

PLOT EXPLAINATION

# %%
# SECTION 16: COMPREHENSIVE VISUALIZATIONS
# =============================================================================
print("\n📈 CREATING COMPREHENSIVE NESTED CV VISUALIZATIONS")
print("-" * 60)

# Create comprehensive visualization dashboard
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('Nested Cross-Validation Results Dashboard (with CatBoost)', fontsize=16, fontweight='bold')

# 1. Model comparison by RMSE (primary metric)
ax1 = axes[0, 0]
models = nested_comparison_df['Model']
rmse_means = nested_comparison_df['Nested_CV_RMSE_Mean']
rmse_stds = nested_comparison_df['Nested_CV_RMSE_Std']

bars = ax1.bar(models, rmse_means, yerr=rmse_stds, capsize=5, alpha=0.7,
               color=['gold' if m == best_model_nested else 'lightblue' for m in models])
ax1.set_ylabel('RMSE ($)')
ax1.set_title('Model Comparison by RMSE (Lower is Better)')
ax1.grid(True, alpha=0.3)
ax1.tick_params(axis='x', rotation=45)

# Add value labels
for bar, mean, std in zip(bars, rmse_means, rmse_stds):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 10,
             f'${mean:.0f}±{std:.0f}', ha='center', va='bottom', fontsize=9)

# 2. RMSE scores across CV folds for best model
ax2 = axes[0, 1]
best_rmse_scores = nested_cv_results[best_model_nested]['outer_scores_rmse']
fold_numbers = range(1, len(best_rmse_scores) + 1)

ax2.plot(fold_numbers, best_rmse_scores, 'o-', linewidth=2, markersize=8, color='red')
ax2.axhline(y=np.mean(best_rmse_scores), color='red', linestyle='--', alpha=0.7,
            label=f'Mean: ${np.mean(best_rmse_scores):.0f}')
ax2.fill_between(fold_numbers,
                 np.mean(best_rmse_scores) - np.std(best_rmse_scores),
                 np.mean(best_rmse_scores) + np.std(best_rmse_scores),
                 alpha=0.2, color='red')
ax2.set_xlabel('CV Fold')
ax2.set_ylabel('RMSE ($)')
ax2.set_title(f'{best_model_nested} - RMSE Across CV Folds')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. MAE comparison
ax3 = axes[0, 2]
mae_means = nested_comparison_df['Nested_CV_MAE_Mean']
mae_stds = nested_comparison_df['Nested_CV_MAE_Std']

bars = ax3.bar(models, mae_means, yerr=mae_stds, capsize=5, alpha=0.7,
               color=['coral' if m == best_model_nested else 'lightgreen' for m in models])
ax3.set_ylabel('MAE ($)')
ax3.set_title('Model Comparison by MAE (Lower is Better)')
ax3.grid(True, alpha=0.3)
ax3.tick_params(axis='x', rotation=45)

# 4. Nested CV vs Test Set comparison
ax4 = axes[1, 0]
metrics = ['RMSE', 'MAE', 'R²']
nested_values = [best_rmse_nested, best_mae_nested, best_r2_nested]
test_values = [test_rmse, test_mae, test_r2]

x = np.arange(len(metrics))
width = 0.35

bars1 = ax4.bar(x - width/2, nested_values, width, label='Nested CV', alpha=0.7, color='skyblue')
bars2 = ax4.bar(x + width/2, test_values, width, label='Test Set', alpha=0.7, color='orange')

ax4.set_ylabel('Value')
ax4.set_title('Nested CV vs Test Set Performance')
ax4.set_xticks(x)
ax4.set_xticklabels(metrics)
ax4.legend()
ax4.grid(True, alpha=0.3)

# 5. Prediction vs Actual scatter plot
ax5 = axes[1, 1]
ax5.scatter(y_test, y_pred_test, alpha=0.6, s=20)
min_val = min(y_test.min(), y_pred_test.min())
max_val = max(y_test.max(), y_pred_test.max())
ax5.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
ax5.set_xlabel('Actual Income ($)')
ax5.set_ylabel('Predicted Income ($)')
ax5.set_title(f'Predictions vs Actual ({best_model_nested})')
ax5.legend()
ax5.grid(True, alpha=0.3)

# 6. Residuals plot
ax6 = axes[1, 2]
residuals = y_test - y_pred_test
ax6.scatter(y_pred_test, residuals, alpha=0.6, s=20)
ax6.axhline(y=0, color='r', linestyle='--', linewidth=2)
ax6.set_xlabel('Predicted Income ($)')
ax6.set_ylabel('Residuals ($)')
ax6.set_title('Residuals Plot')
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(data_path + '/nested_cv_catboost_comprehensive_results.png', dpi=300, bbox_inches='tight')
plt.show()

print("   ✅ Comprehensive visualizations saved to: nested_cv_catboost_comprehensive_results.png")

🚀 PRODUCTION MODEL TRAINING (USING ALL AVAILABLE DATA)
======================================================================
Training final production model using ALL available data (train + validation + test)...
Using best hyperparameters from nested CV analysis.

📊 PRODUCTION TRAINING DATA:
   • Total samples: 31,125
   • Features: 11
   • Target mean: $1,312.18
   • Target std: $688.01

🔧 Training final production XGBoost model...
✅ Production XGBoost model trained successfully!
💾 Production model saved: production_model_catboost_all_data.pkl

🎯 PRODUCTION DEPLOYMENT READY:
   • Model: XGBoost
   • Training samples: 31,125
   • Expected RMSE: $528.26
   • Frequency mappings: Available for new predictions
   • Artifacts saved: Ready for deployment

🔢 CALCULATING CONFIDENCE INTERVALS FOR PREDICTIONS
--------------------------------------------------
   📊 Making predictions on training data for residual calculation...
   ✅ Residuals calculated: 31125 samples
   📈 Confidence Level: 90%
   📉 Lower offset (5th percentile): $-510.93
   📈 Upper offset (95th percentile): $755.02
   📊 Average CI width: $1265.95
   ✅ Confidence interval calculation complete!
💾 Production model updated with confidence intervals

🎯 PRODUCTION PREDICTION USAGE:
   1. Point prediction: production_model.predict(X)
   2. Lower bound: prediction + -510.93
   3. Upper bound: prediction + 755.02
   4. Confidence level: 90%
   5. Example: If prediction = $1000, range = [$489, $1755]

🎉 COMPLETE NESTED CV PIPELINE WITH CATBOOST FINISHED!