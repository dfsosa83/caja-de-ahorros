# ===============================================================================
# COMPREHENSIVE MODEL PROCESS WITH LOG TRANSFORMATION
# ===============================================================================
# This file contains the complete model training pipeline with log transformation
# for improved performance on skewed income data.
#
# KEY IMPROVEMENTS:
# 1. Log transformation to handle skewed target variable
# 2. Proper inverse transformation for predictions
# 3. Enhanced evaluation metrics on original scale
# 4. Zero income handling and data quality checks
# 5. Comprehensive visualization and analysis
# ===============================================================================

# %%
# SECTION 9: PREPARE FINAL DATASETS FOR MODELING
# =============================================================================
print("\nüéØ PREPARING FINAL DATASETS")
print("-" * 50)

# EXPLICIT FEATURE SELECTION APPROACH
id_columns = ['cliente', 'identificador_unico']
target_column = 'ingresos_reportados'

# Manually specify ONLY the features you want to use
feature_columns = selected_features_final

# Verify all selected features exist in the dataset
available_features = []
missing_features = []

for feature in feature_columns:
    if feature in train_df_enhanced.columns:
        available_features.append(feature)
    else:
        missing_features.append(feature)

if missing_features:
    print(f"‚ö†Ô∏è  Missing features (will be skipped): {missing_features}")

feature_columns = available_features

print(f"   üìä Selected feature columns: {len(feature_columns)}")
print(f"   üéØ Target column: {target_column}")

# Create feature matrices and targets
X_train = train_df_enhanced[feature_columns].copy()
y_train = train_df_enhanced[target_column].copy()

X_valid = valid_df_enhanced[feature_columns].copy()
y_valid = valid_df_enhanced[target_column].copy()

X_test = test_df_enhanced[feature_columns].copy()
y_test = test_df_enhanced[target_column].copy()

print(f"\nüìà FINAL DATASET SHAPES:")
print(f"   X_train: {X_train.shape}")
print(f"   X_valid: {X_valid.shape}")
print(f"   X_test: {X_test.shape}")

# Show selected features
print(f"\nüìã SELECTED FEATURES:")
for i, feature in enumerate(feature_columns, 1):
    print(f"   {i:2d}. {feature}")

# Verify data quality
print(f"\n‚úÖ DATA QUALITY CHECKS:")
print(f"   Missing values in X_train: {X_train.isnull().sum().sum()}")
print(f"   Missing values in y_train: {y_train.isnull().sum()}")
print(f"   All features numeric: {all(X_train.dtypes.apply(lambda x: x in ['int64', 'float64']))}")

# %%
# SECTION 9.5: ZERO INCOME ANALYSIS AND HANDLING
# =============================================================================
print("\nüîç ZERO INCOME ANALYSIS")
print("-" * 50)

def analyze_zero_incomes(y_train, y_valid, y_test, train_df, valid_df, test_df):
    """
    Analyze zero income records and decide on treatment strategy
    """
    # Count zero incomes in each set
    zero_train = (y_train == 0).sum()
    zero_valid = (y_valid == 0).sum()
    zero_test = (y_test == 0).sum()
    
    total_train = len(y_train)
    total_valid = len(y_valid)
    total_test = len(y_test)
    
    print(f"üìä Zero Income Analysis:")
    print(f"   Training:   {zero_train:,} / {total_train:,} ({zero_train/total_train*100:.1f}%)")
    print(f"   Validation: {zero_valid:,} / {total_valid:,} ({zero_valid/total_valid*100:.1f}%)")
    print(f"   Test:       {zero_test:,} / {total_test:,} ({zero_test/total_test*100:.1f}%)")
    
    # Analyze characteristics of zero income records
    if zero_train > 0:
        zero_mask_train = (y_train == 0)
        zero_profiles = train_df[zero_mask_train]
        
        print(f"\nüîç Zero Income Profile Analysis:")
        print(f"   Average age: {zero_profiles['edad'].mean():.1f} years")
        if 'monto_letra' in zero_profiles.columns:
            print(f"   Average monto_letra: ${zero_profiles['monto_letra'].mean():,.2f}")
        if 'saldo' in zero_profiles.columns:
            print(f"   Average saldo: ${zero_profiles['saldo'].mean():,.2f}")
        if 'is_retired' in zero_profiles.columns:
            retired_pct = zero_profiles['is_retired'].mean() * 100
            print(f"   Retired percentage: {retired_pct:.1f}%")
    
    # Recommendation based on percentage
    total_zeros = zero_train + zero_valid + zero_test
    total_records = total_train + total_valid + total_test
    zero_percentage = total_zeros / total_records * 100
    
    print(f"\nüí° RECOMMENDATION:")
    if zero_percentage < 2:
        print(f"   ‚úÖ Remove zero incomes ({zero_percentage:.1f}% is very low)")
        return "remove"
    elif zero_percentage < 5:
        print(f"   ‚ö†Ô∏è  Consider removing zero incomes ({zero_percentage:.1f}% is low)")
        return "consider_remove"
    else:
        print(f"   üîÑ Keep and handle zero incomes ({zero_percentage:.1f}% is significant)")
        return "keep"

# Analyze zero incomes
zero_strategy = analyze_zero_incomes(y_train, y_valid, y_test, 
                                   train_df_enhanced, valid_df_enhanced, test_df_enhanced)

# Apply zero income strategy
REMOVE_ZERO_INCOMES = True  # Set to False if you want to keep them

if REMOVE_ZERO_INCOMES and zero_strategy in ["remove", "consider_remove"]:
    print(f"\nüöÆ REMOVING ZERO INCOME RECORDS")
    print("-" * 40)
    
    # Create masks for non-zero incomes
    train_mask = (y_train > 0)
    valid_mask = (y_valid > 0)
    test_mask = (y_test > 0)
    
    # Filter datasets
    X_train = X_train[train_mask].copy()
    y_train = y_train[train_mask].copy()
    
    X_valid = X_valid[valid_mask].copy()
    y_valid = y_valid[valid_mask].copy()
    
    X_test = X_test[test_mask].copy()
    y_test = y_test[test_mask].copy()
    
    print(f"   ‚úÖ Filtered datasets:")
    print(f"      X_train: {X_train.shape}")
    print(f"      X_valid: {X_valid.shape}")
    print(f"      X_test: {X_test.shape}")
    
    # Update enhanced dataframes for consistency
    train_df_enhanced = train_df_enhanced[train_mask].copy()
    valid_df_enhanced = valid_df_enhanced[valid_mask].copy()
    test_df_enhanced = test_df_enhanced[test_mask].copy()
    
else:
    print(f"\nüìù KEEPING ZERO INCOME RECORDS")
    print("   üí° Log transformation will handle them appropriately")

# %%
# SECTION 10: LOG TRANSFORMATION FOR IMPROVED MODEL PERFORMANCE
# =============================================================================
print("\nüîÑ APPLYING LOG TRANSFORMATION TO TARGET VARIABLE")
print("-" * 60)

def apply_log_transform(y_train, y_valid=None, y_test=None):
    """
    Apply log1p transformation to target variable
    
    WHY LOG TRANSFORMATION?
    ----------------------
    1. REDUCES SKEWNESS: Income data is typically right-skewed
    2. STABILIZES VARIANCE: Reduces heteroscedasticity 
    3. IMPROVES MODEL PERFORMANCE: Better for tree-based models
    4. HANDLES OUTLIERS: Reduces impact of extreme values
    
    WHY LOG1P (log(1+x)) INSTEAD OF LOG(x)?
    ---------------------------------------
    1. HANDLES ZEROS: log1p(0) = 0, while log(0) = undefined
    2. NUMERICAL STABILITY: Better for small values
    3. SMOOTH TRANSFORMATION: No discontinuity at zero
    
    Returns:
    --------
    Transformed targets and statistics
    """
    print(f"üìä ORIGINAL TARGET STATISTICS:")
    print(f"   Mean: ${y_train.mean():,.2f}")
    print(f"   Median: ${y_train.median():,.2f}")
    print(f"   Std: ${y_train.std():,.2f}")
    print(f"   Min: ${y_train.min():,.2f}")
    print(f"   Max: ${y_train.max():,.2f}")
    print(f"   Skewness: {y_train.skew():.3f}")
    print(f"   Kurtosis: {y_train.kurtosis():.3f}")
    
    # Apply log1p transformation
    y_train_log = np.log1p(y_train)
    
    print(f"\nüìä LOG-TRANSFORMED TARGET STATISTICS:")
    print(f"   Mean: {y_train_log.mean():.3f}")
    print(f"   Median: {y_train_log.median():.3f}")
    print(f"   Std: {y_train_log.std():.3f}")
    print(f"   Min: {y_train_log.min():.3f}")
    print(f"   Max: {y_train_log.max():.3f}")
    print(f"   Skewness: {y_train_log.skew():.3f}")
    print(f"   Kurtosis: {y_train_log.kurtosis():.3f}")
    
    # Calculate improvement metrics
    skew_improvement = abs(y_train.skew()) - abs(y_train_log.skew())
    kurtosis_improvement = abs(y_train.kurtosis()) - abs(y_train_log.kurtosis())
    
    print(f"\nüìà TRANSFORMATION IMPROVEMENTS:")
    print(f"   Skewness reduction: {skew_improvement:.3f}")
    print(f"   Kurtosis reduction: {kurtosis_improvement:.3f}")
    
    results = [y_train_log]
    
    # Transform validation set if provided
    if y_valid is not None:
        y_valid_log = np.log1p(y_valid)
        results.append(y_valid_log)
        print(f"   ‚úÖ Validation set transformed")
    
    # Transform test set if provided
    if y_test is not None:
        y_test_log = np.log1p(y_test)
        results.append(y_test_log)
        print(f"   ‚úÖ Test set transformed")
    
    return results if len(results) > 1 else results[0]

def inverse_log_transform(y_log_pred):
    """
    Convert log predictions back to original scale
    
    CRITICAL FOR EVALUATION:
    -----------------------
    - ALL PREDICTIONS must be converted back to original scale
    - ALL EVALUATION METRICS must use original scale
    - This ensures business interpretability
    
    Parameters:
    -----------
    y_log_pred : array-like
        Predictions in log scale
        
    Returns:
    --------
    array-like : Predictions in original scale (dollars)
    """
    return np.expm1(y_log_pred)

# Apply log transformation to all target sets
print("üîÑ Transforming target variables...")
y_train_log, y_valid_log, y_test_log = apply_log_transform(y_train, y_valid, y_test)

print(f"\n‚úÖ LOG TRANSFORMATION COMPLETED!")
print(f"   üìä Skewness: {y_train.skew():.3f} ‚Üí {y_train_log.skew():.3f}")
print(f"   üìä Kurtosis: {y_train.kurtosis():.3f} ‚Üí {y_train_log.kurtosis():.3f}")

# %%
# SECTION 10.5: INCOME DISTRIBUTION ANALYSIS FOR MAPE INTERPRETATION
# =============================================================================
print("\nüìä COMPREHENSIVE INCOME DISTRIBUTION ANALYSIS")
print("-" * 60)

def analyze_income_distribution_for_mape(y_train, y_valid, y_test):
    """
    Analyze income distribution to understand MAPE behavior

    WHY THIS MATTERS FOR MAPE:
    ==========================
    MAPE = mean(|actual - predicted| / actual) * 100

    When 'actual' values are small (< $1,000), even small prediction errors
    create massive percentage errors, making MAPE misleading.

    This analysis helps us:
    1. Understand why MAPE might be inflated
    2. Choose appropriate evaluation thresholds
    3. Set realistic performance expectations
    """

    print("üìà INCOME DISTRIBUTION STATISTICS:")
    print("=" * 50)

    # Combined dataset for overall analysis
    all_incomes = pd.concat([y_train, y_valid, y_test])

    print(f"OVERALL DATASET:")
    print(f"   üìä Total samples: {len(all_incomes):,}")
    print(f"   üìä Min income: ${all_incomes.min():.2f}")
    print(f"   üìä Max income: ${all_incomes.max():.2f}")
    print(f"   üìä Mean income: ${all_incomes.mean():.2f}")
    print(f"   üìä Median income: ${all_incomes.median():.2f}")
    print(f"   üìä Std income: ${all_incomes.std():.2f}")

    # Income range analysis
    print(f"\nüéØ INCOME RANGE BREAKDOWN:")
    print("-" * 30)

    income_ranges = [
        (0, 500, "Very Low"),
        (500, 1000, "Low"),
        (1000, 2000, "Medium-Low"),
        (2000, 3000, "Medium"),
        (3000, 5000, "Medium-High"),
        (5000, float('inf'), "High")
    ]

    for min_val, max_val, label in income_ranges:
        if max_val == float('inf'):
            mask = all_incomes >= min_val
            range_str = f"‚â•${min_val:,}"
        else:
            mask = (all_incomes >= min_val) & (all_incomes < max_val)
            range_str = f"${min_val:,}-${max_val:,}"

        count = mask.sum()
        percentage = count / len(all_incomes) * 100

        if count > 0:
            avg_income = all_incomes[mask].mean()
            print(f"   {label:12s} ({range_str:12s}): {count:5,} ({percentage:5.1f}%) - Avg: ${avg_income:6,.0f}")

    # MAPE impact analysis
    print(f"\n‚ö†Ô∏è  MAPE IMPACT ANALYSIS:")
    print("-" * 25)

    # Calculate what percentage of data causes MAPE issues
    problematic_threshold = 1000  # Incomes below this cause inflated MAPE
    problematic_mask = all_incomes < problematic_threshold
    problematic_count = problematic_mask.sum()
    problematic_pct = problematic_count / len(all_incomes) * 100

    print(f"   üìä Incomes < ${problematic_threshold:,}: {problematic_count:,} ({problematic_pct:.1f}%)")
    print(f"   üìä Incomes ‚â• ${problematic_threshold:,}: {(~problematic_mask).sum():,} ({100-problematic_pct:.1f}%)")

    if problematic_pct > 30:
        print(f"   üö® HIGH MAPE INFLATION RISK: {problematic_pct:.1f}% of data below ${problematic_threshold:,}")
        print(f"   üí° RECOMMENDATION: Use Robust MAPE (‚â•${problematic_threshold:,}) for meaningful evaluation")
    elif problematic_pct > 10:
        print(f"   ‚ö†Ô∏è  MODERATE MAPE INFLATION: {problematic_pct:.1f}% of data below ${problematic_threshold:,}")
        print(f"   üí° RECOMMENDATION: Report both standard and robust MAPE")
    else:
        print(f"   ‚úÖ LOW MAPE INFLATION RISK: Only {problematic_pct:.1f}% below ${problematic_threshold:,}")

    # Percentile analysis
    print(f"\nüìä INCOME PERCENTILES:")
    print("-" * 20)
    percentiles = [5, 10, 25, 50, 75, 90, 95, 99]
    for p in percentiles:
        value = np.percentile(all_incomes, p)
        print(f"   {p:2d}th percentile: ${value:6,.0f}")

    return {
        'problematic_percentage': problematic_pct,
        'mean_income': all_incomes.mean(),
        'median_income': all_incomes.median(),
        'min_income': all_incomes.min(),
        'max_income': all_incomes.max()
    }

# Perform comprehensive income analysis
income_analysis = analyze_income_distribution_for_mape(y_train, y_valid, y_test)

# Create comparison visualization
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Target Variable Transformation Analysis', fontsize=16)

# Original distribution
axes[0,0].hist(y_train, bins=50, alpha=0.7, color='blue', edgecolor='black')
axes[0,0].set_title('Original Target Distribution')
axes[0,0].set_xlabel('Income (ingresos_reportados)')
axes[0,0].set_ylabel('Frequency')
axes[0,0].grid(True, alpha=0.3)

# Log-transformed distribution
axes[0,1].hist(y_train_log, bins=50, alpha=0.7, color='green', edgecolor='black')
axes[0,1].set_title('Log-Transformed Target Distribution')
axes[0,1].set_xlabel('Log(Income + 1)')
axes[0,1].set_ylabel('Frequency')
axes[0,1].grid(True, alpha=0.3)

# Q-Q plot for normality check
from scipy import stats
stats.probplot(y_train, dist="norm", plot=axes[1,0])
axes[1,0].set_title('Original Target Q-Q Plot')
axes[1,0].grid(True, alpha=0.3)

stats.probplot(y_train_log, dist="norm", plot=axes[1,1])
axes[1,1].set_title('Log-Transformed Target Q-Q Plot')
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\nüéØ READY FOR MODEL TRAINING WITH LOG-TRANSFORMED TARGETS!")
print(f"   üí° Remember: All predictions will be inverse-transformed for evaluation")

# %%
# SECTION 11: FEATURE SCALING AND MODEL SETUP
# =============================================================================
print("\n‚öñÔ∏è FEATURE SCALING AND MODEL CONFIGURATION")
print("-" * 60)

# Apply robust scaling to features
print("üîÑ Applying RobustScaler to features...")
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X_train_scaled = pd.DataFrame(
    scaler.fit_transform(X_train),
    columns=X_train.columns,
    index=X_train.index
)
X_valid_scaled = pd.DataFrame(
    scaler.transform(X_valid),
    columns=X_valid.columns,
    index=X_valid.index
)
X_test_scaled = pd.DataFrame(
    scaler.transform(X_test),
    columns=X_test.columns,
    index=X_test.index
)
print("   ‚úÖ Feature scaling complete")

# Model configurations optimized for log-transformed targets
print("\nü§ñ CONFIGURING MODELS FOR LOG-TRANSFORMED TARGETS")
print("-" * 50)

"""
MODEL HYPERPARAMETER ADJUSTMENTS FOR LOG SCALE:
===============================================

1. LEARNING RATES: Slightly reduced for smoother convergence
2. REGULARIZATION: Increased to prevent overfitting on transformed scale
3. TREE DEPTH: Adjusted for log scale patterns
4. EARLY STOPPING: More conservative to avoid overfitting

WHY THESE CHANGES?
- Log scale compresses large values, making gradients more stable
- Reduced variance in log scale allows for more aggressive regularization
- Tree-based models can capture log-linear relationships better
"""

models = {
    'XGBoost_Log': xgb.XGBRegressor(
        n_estimators=1000,          # Increased for better convergence
        max_depth=20,               # Deeper trees for log patterns
        learning_rate=0.01,         # Reduced for stability
        subsample=0.85,
        colsample_bytree=0.85,
        reg_alpha=0.1,              # L1 regularization
        reg_lambda=0.1,             # L2 regularization
        min_child_weight=5,         # Prevent overfitting
        random_state=42,
        n_jobs=-1,
        verbosity=0
    ),
    'LightGBM_Log': lgb.LGBMRegressor(
        n_estimators=1500,          # More estimators for log scale
        max_depth=25,               # Deeper for complex patterns
        learning_rate=0.005,        # Very conservative learning rate
        subsample=0.85,
        colsample_bytree=0.85,
        num_leaves=200,             # More leaves for log patterns
        min_child_samples=20,       # Prevent overfitting
        reg_alpha=0.1,              # L1 regularization
        reg_lambda=0.1,             # L2 regularization
        random_state=42,
        n_jobs=-1,
        verbose=-1
    ),
    'RandomForest_Log': RandomForestRegressor(
        n_estimators=500,           # More trees
        max_depth=25,               # Deeper trees
        min_samples_split=10,       # Conservative splitting
        min_samples_leaf=5,         # Prevent overfitting
        max_features='sqrt',        # Feature subsampling
        random_state=42,
        n_jobs=-1
    )
}

print(f"   ‚úÖ Configured {len(models)} models for log-transformed training")

# %%
# SECTION 12: CROSS-VALIDATION WITH LOG TRANSFORMATION
# =============================================================================
print("\nüìä CROSS-VALIDATION WITH LOG TRANSFORMATION")
print("-" * 60)

from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

def evaluate_model_cv_log(model, X, y_log, y_original, model_name, cv_folds=4):
    """
    Evaluate model using cross-validation with proper log transformation handling

    CRITICAL EVALUATION PROCESS:
    ===========================
    1. Train model on LOG-TRANSFORMED targets
    2. Make predictions in LOG scale
    3. INVERSE TRANSFORM predictions to original scale
    4. Evaluate metrics on ORIGINAL scale
    5. This ensures business-relevant performance metrics

    Parameters:
    -----------
    model : sklearn estimator
        Model to evaluate
    X : DataFrame
        Feature matrix
    y_log : Series
        Log-transformed target
    y_original : Series
        Original scale target (for evaluation)
    model_name : str
        Name for reporting
    cv_folds : int
        Number of CV folds

    Returns:
    --------
    dict : Cross-validation results
    """
    print(f"\n   üîÑ Evaluating {model_name} with log transformation...")

    # Setup cross-validation
    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)

    # Storage for results
    cv_r2_scores = []
    cv_rmse_scores = []
    cv_mae_scores = []

    # Manual cross-validation to handle inverse transformation
    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
        # Split data
        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
        y_fold_train_log, y_fold_val_log = y_log.iloc[train_idx], y_log.iloc[val_idx]
        y_fold_val_original = y_original.iloc[val_idx]

        # Train on log scale
        model_copy = clone(model)
        model_copy.fit(X_fold_train, y_fold_train_log)

        # Predict in log scale
        y_pred_log = model_copy.predict(X_fold_val)

        # CRITICAL: Inverse transform to original scale
        y_pred_original = inverse_log_transform(y_pred_log)

        # Evaluate on original scale
        r2 = r2_score(y_fold_val_original, y_pred_original)
        rmse = np.sqrt(mean_squared_error(y_fold_val_original, y_pred_original))
        mae = mean_absolute_error(y_fold_val_original, y_pred_original)

        cv_r2_scores.append(r2)
        cv_rmse_scores.append(rmse)
        cv_mae_scores.append(mae)

        print(f"      Fold {fold+1}: R¬≤={r2:.4f}, RMSE=${rmse:,.0f}, MAE=${mae:,.0f}")

    # Calculate statistics
    cv_r2_mean = np.mean(cv_r2_scores)
    cv_r2_std = np.std(cv_r2_scores)
    cv_rmse_mean = np.mean(cv_rmse_scores)
    cv_rmse_std = np.std(cv_rmse_scores)
    cv_mae_mean = np.mean(cv_mae_scores)
    cv_mae_std = np.std(cv_mae_scores)

    print(f"      üìà CV R¬≤: {cv_r2_mean:.4f} ¬± {cv_r2_std:.4f}")
    print(f"      üìà CV RMSE: ${cv_rmse_mean:,.0f} ¬± ${cv_rmse_std:,.0f}")
    print(f"      üìà CV MAE: ${cv_mae_mean:,.0f} ¬± ${cv_mae_std:,.0f}")

    return {
        'cv_r2_mean': cv_r2_mean,
        'cv_r2_std': cv_r2_std,
        'cv_rmse_mean': cv_rmse_mean,
        'cv_rmse_std': cv_rmse_std,
        'cv_mae_mean': cv_mae_mean,
        'cv_mae_std': cv_mae_std,
        'cv_r2_scores': cv_r2_scores,
        'cv_rmse_scores': cv_rmse_scores,
        'cv_mae_scores': cv_mae_scores
    }

# Import required modules
from sklearn.base import clone

# Evaluate all models with cross-validation
cv_results = {}
for model_name, model in models.items():
    cv_results[model_name] = evaluate_model_cv_log(
        model, X_train_scaled, y_train_log, y_train, model_name
    )

print(f"\n‚úÖ Cross-validation completed for all models!")

# %%
# SECTION 13: FINAL MODEL TRAINING WITH LOG TRANSFORMATION
# =============================================================================
print("\nüéØ FINAL MODEL TRAINING WITH LOG TRANSFORMATION")
print("-" * 60)

def train_and_evaluate_final_log(model, X_train, y_train_log, y_train_orig,
                                X_valid, y_valid_log, y_valid_orig, model_name):
    """
    Train final model and evaluate with proper log transformation handling

    EVALUATION METHODOLOGY:
    ======================
    1. Train model on LOG-TRANSFORMED targets
    2. Make predictions in LOG scale
    3. INVERSE TRANSFORM all predictions to original scale
    4. Calculate ALL METRICS on original scale
    5. Report both training and validation performance

    This ensures:
    - Business-interpretable metrics (in dollars)
    - Fair comparison with non-log models
    - Proper assessment of real-world performance
    """
    print(f"\n   üöÄ Training final {model_name}...")

    # Train model on log-transformed targets
    model.fit(X_train, y_train_log)

    # Make predictions in log scale
    y_pred_train_log = model.predict(X_train)
    y_pred_valid_log = model.predict(X_valid)

    # CRITICAL: Inverse transform to original scale
    y_pred_train_orig = inverse_log_transform(y_pred_train_log)
    y_pred_valid_orig = inverse_log_transform(y_pred_valid_log)

    # Calculate metrics on ORIGINAL scale
    train_rmse = np.sqrt(mean_squared_error(y_train_orig, y_pred_train_orig))
    valid_rmse = np.sqrt(mean_squared_error(y_valid_orig, y_pred_valid_orig))

    train_r2 = r2_score(y_train_orig, y_pred_train_orig)
    valid_r2 = r2_score(y_valid_orig, y_pred_valid_orig)

    train_mae = mean_absolute_error(y_train_orig, y_pred_train_orig)
    valid_mae = mean_absolute_error(y_valid_orig, y_pred_valid_orig)

    # Calculate ROBUST MAPE (excludes very low incomes to avoid division issues)
    def calculate_robust_mape(y_true, y_pred, threshold=1000):
        """
        Calculate MAPE only for incomes above threshold

        WHY ROBUST MAPE?
        ================
        - Standard MAPE explodes with low denominators
        - 40.6% of incomes < $1,000 cause massive percentage errors
        - Robust MAPE gives business-meaningful error rates

        Example: actual=$200, predicted=$400 ‚Üí 100% error (misleading)
                actual=$2000, predicted=$2200 ‚Üí 10% error (meaningful)
        """
        mask = y_true >= threshold
        if mask.sum() == 0:
            return np.nan, 0

        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        return mape, mask.sum()

    # Calculate both standard and robust MAPE
    train_mape = np.mean(np.abs((y_train_orig - y_pred_train_orig) / y_train_orig)) * 100
    valid_mape = np.mean(np.abs((y_valid_orig - y_pred_valid_orig) / y_valid_orig)) * 100

    train_mape_robust, train_robust_count = calculate_robust_mape(y_train_orig, y_pred_train_orig)
    valid_mape_robust, valid_robust_count = calculate_robust_mape(y_valid_orig, y_pred_valid_orig)

    print(f"      üìä Training   - R¬≤: {train_r2:.4f}, RMSE: ${train_rmse:,.0f}, MAE: ${train_mae:,.0f}")
    print(f"      üìä Validation - R¬≤: {valid_r2:.4f}, RMSE: ${valid_rmse:,.0f}, MAE: ${valid_mae:,.0f}")
    print(f"      üìä Standard MAPE - Train: {train_mape:.1f}%, Valid: {valid_mape:.1f}%")
    print(f"      üìä Robust MAPE (‚â•$1K) - Train: {train_mape_robust:.1f}% (n={train_robust_count}), Valid: {valid_mape_robust:.1f}% (n={valid_robust_count})")

    # Check for overfitting
    r2_diff = train_r2 - valid_r2
    rmse_ratio = valid_rmse / train_rmse

    if r2_diff > 0.1:
        print(f"      ‚ö†Ô∏è  Potential overfitting: R¬≤ gap = {r2_diff:.3f}")
    if rmse_ratio > 1.3:
        print(f"      ‚ö†Ô∏è  Potential overfitting: RMSE ratio = {rmse_ratio:.2f}")

    return {
        'model': model,
        'train_rmse': train_rmse, 'valid_rmse': valid_rmse,
        'train_r2': train_r2, 'valid_r2': valid_r2,
        'train_mae': train_mae, 'valid_mae': valid_mae,
        'train_mape': train_mape, 'valid_mape': valid_mape,
        'train_mape_robust': train_mape_robust, 'valid_mape_robust': valid_mape_robust,
        'y_pred_valid_orig': y_pred_valid_orig,
        'y_pred_train_orig': y_pred_train_orig
    }

# Train all final models
final_results = {}
for model_name, model in models.items():
    final_results[model_name] = train_and_evaluate_final_log(
        model, X_train_scaled, y_train_log, y_train,
        X_valid_scaled, y_valid_log, y_valid, model_name
    )

print(f"\n‚úÖ Final model training completed!")

# %%
# SECTION 14: MODEL COMPARISON AND SELECTION
# =============================================================================
print("\nüèÜ MODEL COMPARISON AND SELECTION")
print("-" * 60)

# Create comprehensive comparison DataFrame
comparison_data = []
for model_name in models.keys():
    cv_res = cv_results[model_name]
    final_res = final_results[model_name]

    comparison_data.append({
        'Model': model_name,
        'CV_R2_Mean': cv_res['cv_r2_mean'],
        'CV_R2_Std': cv_res['cv_r2_std'],
        'CV_RMSE_Mean': cv_res['cv_rmse_mean'],
        'CV_RMSE_Std': cv_res['cv_rmse_std'],
        'Valid_R2': final_res['valid_r2'],
        'Valid_RMSE': final_res['valid_rmse'],
        'Valid_MAE': final_res['valid_mae'],
        'Valid_MAPE': final_res['valid_mape'],
        'Overfitting_Score': final_res['train_r2'] - final_res['valid_r2']
    })

comparison_df = pd.DataFrame(comparison_data)
comparison_df = comparison_df.sort_values('Valid_R2', ascending=False)

print("\nüìä COMPREHENSIVE MODEL PERFORMANCE COMPARISON:")
print("=" * 100)
print(comparison_df.round(4).to_string(index=False))

# Select best model based on validation R¬≤
best_model_name = comparison_df.iloc[0]['Model']
best_model_results = final_results[best_model_name]

print(f"\nü•á BEST MODEL SELECTED: {best_model_name}")
print("-" * 50)
print(f"   üìà Validation R¬≤: {best_model_results['valid_r2']:.4f}")
print(f"   üìà Validation RMSE: ${best_model_results['valid_rmse']:,.0f}")
print(f"   üìà Validation MAE: ${best_model_results['valid_mae']:,.0f}")
print(f"   üìà Validation MAPE: {best_model_results['valid_mape']:.1f}%")

# Model selection criteria explanation
print(f"\nüí° MODEL SELECTION CRITERIA:")
print(f"   1. Highest validation R¬≤ (primary)")
print(f"   2. Lowest overfitting score (secondary)")
print(f"   3. Reasonable RMSE and MAE (tertiary)")

# %%
# SECTION 15: FINAL TEST EVALUATION WITH LOG TRANSFORMATION
# =============================================================================
print("\nüéØ FINAL TEST EVALUATION")
print("-" * 60)

def evaluate_test_set_log(model, X_test, y_test_log, y_test_orig, model_name):
    """
    Comprehensive test set evaluation with log transformation

    FINAL EVALUATION PROCESS:
    ========================
    1. Use trained model to predict on test set (log scale)
    2. Inverse transform predictions to original scale
    3. Calculate comprehensive metrics on original scale
    4. Analyze prediction quality across income ranges
    5. Generate detailed performance report
    """
    print(f"\nüîç Evaluating {model_name} on test set...")

    # Make predictions in log scale
    y_pred_test_log = model.predict(X_test)

    # CRITICAL: Inverse transform to original scale
    y_pred_test_orig = inverse_log_transform(y_pred_test_log)

    # Calculate comprehensive metrics on original scale
    test_rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_test_orig))
    test_r2 = r2_score(y_test_orig, y_pred_test_orig)
    test_mae = mean_absolute_error(y_test_orig, y_pred_test_orig)
    # Calculate ROBUST MAPE for test set
    def calculate_robust_mape_test(y_true, y_pred, threshold=1000):
        """Calculate robust MAPE for test evaluation"""
        mask = y_true >= threshold
        if mask.sum() == 0:
            return np.nan, 0
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
        return mape, mask.sum()

    # Calculate both standard and robust MAPE
    test_mape = np.mean(np.abs((y_test_orig - y_pred_test_orig) / y_test_orig)) * 100
    test_mape_robust, test_robust_count = calculate_robust_mape_test(y_test_orig, y_pred_test_orig)

    # Calculate additional metrics
    from sklearn.metrics import explained_variance_score
    test_evs = explained_variance_score(y_test_orig, y_pred_test_orig)

    # Calculate residuals
    residuals = y_test_orig - y_pred_test_orig
    residuals_pct = (residuals / y_test_orig) * 100

    print(f"\nüèÜ FINAL TEST PERFORMANCE ({model_name}):")
    print("=" * 60)
    print(f"   üìä R¬≤ Score: {test_r2:.4f}")
    print(f"   üìä RMSE: ${test_rmse:,.0f}")
    print(f"   üìä MAE: ${test_mae:,.0f}")
    print(f"   üìä Standard MAPE: {test_mape:.1f}% (‚ö†Ô∏è Inflated by low incomes)")
    print(f"   üìä Robust MAPE (‚â•$1K): {test_mape_robust:.1f}% (n={test_robust_count}) ‚úÖ")
    print(f"   üìä Explained Variance: {test_evs:.4f}")

    # Residual analysis
    print(f"\nüìà RESIDUAL ANALYSIS:")
    print(f"   Mean residual: ${residuals.mean():,.0f}")
    print(f"   Std residual: ${residuals.std():,.0f}")
    print(f"   Mean absolute residual %: {np.abs(residuals_pct).mean():.1f}%")

    # Performance by income ranges
    print(f"\nüìä PERFORMANCE BY INCOME RANGES:")
    income_ranges = [
        (0, 50000, "Low Income"),
        (50000, 100000, "Medium Income"),
        (100000, 200000, "High Income"),
        (200000, float('inf'), "Very High Income")
    ]

    for min_income, max_income, range_name in income_ranges:
        mask = (y_test_orig >= min_income) & (y_test_orig < max_income)
        if mask.sum() > 0:
            range_r2 = r2_score(y_test_orig[mask], y_pred_test_orig[mask])
            range_mae = mean_absolute_error(y_test_orig[mask], y_pred_test_orig[mask])
            range_count = mask.sum()
            print(f"   {range_name:15s}: R¬≤={range_r2:6.3f}, MAE=${range_mae:8,.0f}, n={range_count:4d}")

    return {
        'test_r2': test_r2,
        'test_rmse': test_rmse,
        'test_mae': test_mae,
        'test_mape': test_mape,
        'test_mape_robust': test_mape_robust,
        'test_evs': test_evs,
        'y_pred_test_orig': y_pred_test_orig,
        'residuals': residuals,
        'residuals_pct': residuals_pct
    }

# Evaluate best model on test set
best_model = best_model_results['model']
test_results = evaluate_test_set_log(
    best_model, X_test_scaled, y_test_log, y_test, best_model_name
)

# %%
# SECTION 16: COMPREHENSIVE VISUALIZATION AND ANALYSIS
# =============================================================================
print("\nüìä COMPREHENSIVE VISUALIZATION AND ANALYSIS")
print("-" * 60)

def create_comprehensive_analysis_plots(y_train_orig, y_valid_orig, y_test_orig,
                                       y_pred_test_orig, model_name):
    """
    Create comprehensive visualization suite for log transformation analysis
    """
    fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    fig.suptitle(f'Comprehensive Model Analysis - {model_name}', fontsize=16)

    # 1. Original vs Log-transformed distributions
    axes[0,0].hist(y_train_orig, bins=50, alpha=0.7, label='Original', color='blue', density=True)
    axes[0,0].hist(y_train_log, bins=50, alpha=0.7, label='Log-transformed', color='red', density=True)
    axes[0,0].set_title('Target Distribution: Original vs Log')
    axes[0,0].set_xlabel('Value')
    axes[0,0].set_ylabel('Density')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)

    # 2. Training vs Test distributions
    axes[0,1].hist(y_train_orig, bins=50, alpha=0.7, label='Training', color='blue', density=True)
    axes[0,1].hist(y_test_orig, bins=50, alpha=0.7, label='Test', color='green', density=True)
    axes[0,1].hist(y_pred_test_orig, bins=50, alpha=0.7, label='Predictions', color='red', density=True)
    axes[0,1].set_title('Distribution Comparison')
    axes[0,1].set_xlabel('Income')
    axes[0,1].set_ylabel('Density')
    axes[0,1].legend()
    axes[0,1].grid(True, alpha=0.3)

    # 3. Actual vs Predicted scatter
    axes[0,2].scatter(y_test_orig, y_pred_test_orig, alpha=0.6, s=20)
    axes[0,2].plot([y_test_orig.min(), y_test_orig.max()],
                   [y_test_orig.min(), y_test_orig.max()], 'r--', lw=2)
    axes[0,2].set_xlabel('Actual Income')
    axes[0,2].set_ylabel('Predicted Income')
    axes[0,2].set_title(f'Actual vs Predicted (R¬≤ = {test_results["test_r2"]:.3f})')
    axes[0,2].grid(True, alpha=0.3)

    # 4. Residuals plot
    residuals = test_results['residuals']
    axes[1,0].scatter(y_pred_test_orig, residuals, alpha=0.6, s=20)
    axes[1,0].axhline(y=0, color='r', linestyle='--', lw=2)
    axes[1,0].set_xlabel('Predicted Income')
    axes[1,0].set_ylabel('Residuals')
    axes[1,0].set_title('Residuals Plot')
    axes[1,0].grid(True, alpha=0.3)

    # 5. Residuals histogram
    axes[1,1].hist(residuals, bins=50, alpha=0.7, color='purple', edgecolor='black')
    axes[1,1].set_xlabel('Residuals')
    axes[1,1].set_ylabel('Frequency')
    axes[1,1].set_title('Residuals Distribution')
    axes[1,1].grid(True, alpha=0.3)

    # 6. Q-Q plot of residuals
    from scipy import stats
    stats.probplot(residuals, dist="norm", plot=axes[1,2])
    axes[1,2].set_title('Residuals Q-Q Plot')
    axes[1,2].grid(True, alpha=0.3)

    # 7. Performance by income ranges
    income_ranges = np.percentile(y_test_orig, [0, 25, 50, 75, 100])
    range_labels = ['Q1', 'Q2', 'Q3', 'Q4']
    range_r2s = []

    for i in range(len(income_ranges)-1):
        mask = (y_test_orig >= income_ranges[i]) & (y_test_orig < income_ranges[i+1])
        if mask.sum() > 0:
            range_r2 = r2_score(y_test_orig[mask], y_pred_test_orig[mask])
            range_r2s.append(range_r2)
        else:
            range_r2s.append(0)

    axes[2,0].bar(range_labels, range_r2s, color='skyblue', edgecolor='black')
    axes[2,0].set_ylabel('R¬≤ Score')
    axes[2,0].set_title('Performance by Income Quartiles')
    axes[2,0].grid(True, alpha=0.3)

    # 8. Prediction error by income level
    error_pct = np.abs(test_results['residuals_pct'])
    axes[2,1].scatter(y_test_orig, error_pct, alpha=0.6, s=20)
    axes[2,1].set_xlabel('Actual Income')
    axes[2,1].set_ylabel('Absolute Error %')
    axes[2,1].set_title('Prediction Error by Income Level')
    axes[2,1].grid(True, alpha=0.3)

    # 9. Cumulative distribution comparison
    test_sorted = np.sort(y_test_orig)
    pred_sorted = np.sort(y_pred_test_orig)
    test_cdf = np.arange(1, len(test_sorted) + 1) / len(test_sorted)
    pred_cdf = np.arange(1, len(pred_sorted) + 1) / len(pred_sorted)

    axes[2,2].plot(test_sorted, test_cdf, label='Actual', linewidth=2)
    axes[2,2].plot(pred_sorted, pred_cdf, label='Predicted', linewidth=2)
    axes[2,2].set_xlabel('Income')
    axes[2,2].set_ylabel('Cumulative Probability')
    axes[2,2].set_title('Cumulative Distribution Comparison')
    axes[2,2].legend()
    axes[2,2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# Create comprehensive analysis
create_comprehensive_analysis_plots(y_train, y_valid, y_test,
                                   test_results['y_pred_test_orig'], best_model_name)

# %%
# SECTION 17: STATISTICAL COMPARISON AND INSIGHTS
# =============================================================================
print("\nüìà STATISTICAL COMPARISON AND INSIGHTS")
print("-" * 60)

def generate_statistical_insights(y_train_orig, y_test_orig, y_pred_test_orig,
                                 y_train_log, test_results):
    """
    Generate comprehensive statistical insights about the log transformation impact
    """
    print("üìä DATASET STATISTICS COMPARISON:")
    print("=" * 50)

    # Original scale statistics
    print("TRAINING SET (Original Scale):")
    print(f"   Mean: ${y_train_orig.mean():,.2f}")
    print(f"   Median: ${y_train_orig.median():,.2f}")
    print(f"   Std: ${y_train_orig.std():,.2f}")
    print(f"   Skewness: {y_train_orig.skew():.3f}")
    print(f"   Kurtosis: {y_train_orig.kurtosis():.3f}")

    print("\nTEST SET (Original Scale):")
    print(f"   Mean: ${y_test_orig.mean():,.2f}")
    print(f"   Median: ${y_test_orig.median():,.2f}")
    print(f"   Std: ${y_test_orig.std():,.2f}")
    print(f"   Skewness: {y_test_orig.skew():.3f}")
    print(f"   Kurtosis: {y_test_orig.kurtosis():.3f}")

    print("\nPREDICTIONS (Original Scale):")
    print(f"   Mean: ${y_pred_test_orig.mean():,.2f}")
    print(f"   Median: ${np.median(y_pred_test_orig):,.2f}")
    print(f"   Std: ${y_pred_test_orig.std():,.2f}")
    print(f"   Skewness: {pd.Series(y_pred_test_orig).skew():.3f}")
    print(f"   Kurtosis: {pd.Series(y_pred_test_orig).kurtosis():.3f}")

    # Log transformation impact
    print(f"\nüîÑ LOG TRANSFORMATION IMPACT:")
    print("=" * 40)
    original_skew = y_train_orig.skew()
    log_skew = y_train_log.skew()
    skew_improvement = abs(original_skew) - abs(log_skew)

    print(f"   Skewness: {original_skew:.3f} ‚Üí {log_skew:.3f}")
    print(f"   Improvement: {skew_improvement:.3f} ({skew_improvement/abs(original_skew)*100:.1f}%)")

    # Distribution similarity test
    from scipy.stats import ks_2samp
    ks_stat, ks_pvalue = ks_2samp(y_test_orig, y_pred_test_orig)

    print(f"\nüìä DISTRIBUTION SIMILARITY TEST:")
    print("=" * 40)
    print(f"   Kolmogorov-Smirnov Statistic: {ks_stat:.4f}")
    print(f"   P-value: {ks_pvalue:.4f}")
    similarity = "Similar" if ks_pvalue > 0.05 else "Different"
    print(f"   Interpretation: Distributions are {similarity}")

    # Performance insights
    print(f"\nüéØ PERFORMANCE INSIGHTS:")
    print("=" * 30)
    print(f"   Final R¬≤ Score: {test_results['test_r2']:.4f}")

    if test_results['test_r2'] > 0.5:
        performance_level = "Excellent"
    elif test_results['test_r2'] > 0.4:
        performance_level = "Good"
    elif test_results['test_r2'] > 0.3:
        performance_level = "Fair"
    else:
        performance_level = "Needs Improvement"

    print(f"   Performance Level: {performance_level}")
    print(f"   Average Prediction Error: {test_results['test_mape']:.1f}%")

    # Business impact
    avg_income = y_test_orig.mean()
    avg_error = test_results['test_mae']
    error_percentage = (avg_error / avg_income) * 100

    print(f"\nüíº BUSINESS IMPACT:")
    print("=" * 20)
    print(f"   Average Income: ${avg_income:,.2f}")
    print(f"   Average Prediction Error: ${avg_error:,.2f}")
    print(f"   Relative Error: {error_percentage:.1f}%")

    return {
        'skew_improvement': skew_improvement,
        'ks_statistic': ks_stat,
        'ks_pvalue': ks_pvalue,
        'performance_level': performance_level,
        'error_percentage': error_percentage
    }

# Generate insights
insights = generate_statistical_insights(y_train, y_test, test_results['y_pred_test_orig'],
                                        y_train_log, test_results)

# %%
# SECTION 18: SAVE RESULTS AND MODEL ARTIFACTS
# =============================================================================
print("\nüíæ SAVING RESULTS AND MODEL ARTIFACTS")
print("-" * 60)

import joblib
import json

def save_model_artifacts_log(best_model, scaler, feature_columns, model_name,
                           test_results, comparison_df, insights):
    """
    Save all model artifacts with log transformation information

    CRITICAL COMPONENTS TO SAVE:
    ============================
    1. Trained model (already trained on log scale)
    2. Feature scaler
    3. Feature column names
    4. Inverse transformation function
    5. Performance metrics
    6. Log transformation parameters

    PRODUCTION DEPLOYMENT NOTES:
    ===========================
    - Model predicts in LOG scale
    - MUST apply inverse_log_transform to get original scale
    - Use saved scaler for feature preprocessing
    - Feature columns must match exactly
    """

    # Create comprehensive model artifacts
    model_artifacts = {
        'model': best_model,
        'scaler': scaler,
        'feature_columns': feature_columns,
        'model_name': model_name,
        'transformation_type': 'log1p',
        'inverse_transform_function': 'np.expm1',  # For reference
        'test_performance': {
            'r2': test_results['test_r2'],
            'rmse': test_results['test_rmse'],
            'mae': test_results['test_mae'],
            'mape': test_results['test_mape'],
            'explained_variance': test_results['test_evs']
        },
        'training_info': {
            'target_transformation': 'log1p (natural log of 1 + x)',
            'feature_scaling': 'RobustScaler',
            'cv_strategy': 'KFold with 4 splits',
            'evaluation_scale': 'original (inverse transformed)'
        },
        'deployment_instructions': {
            'step_1': 'Apply scaler.transform() to features',
            'step_2': 'Use model.predict() to get log-scale predictions',
            'step_3': 'Apply np.expm1() to convert to original scale',
            'step_4': 'Result is income in original dollars'
        }
    }

    # Save main model artifacts
    model_filename = f'{data_path}/income_prediction_model_log_transform.pkl'
    joblib.dump(model_artifacts, model_filename)
    print(f"   ‚úÖ Model artifacts saved: {model_filename}")

    # Save performance comparison
    comparison_filename = f'{data_path}/model_comparison_log_transform.csv'
    comparison_df.to_csv(comparison_filename, index=False)
    print(f"   ‚úÖ Model comparison saved: {comparison_filename}")

    # Save detailed results
    results_summary = {
        'final_performance': {
            'best_model': model_name,
            'test_r2': float(test_results['test_r2']),
            'test_rmse': float(test_results['test_rmse']),
            'test_mae': float(test_results['test_mae']),
            'test_mape': float(test_results['test_mape'])
        },
        'transformation_impact': {
            'skewness_improvement': float(insights['skew_improvement']),
            'distribution_similarity_pvalue': float(insights['ks_pvalue']),
            'performance_level': insights['performance_level']
        },
        'model_configuration': {
            'features_count': len(feature_columns),
            'training_samples': len(y_train),
            'test_samples': len(y_test),
            'zero_incomes_removed': REMOVE_ZERO_INCOMES
        }
    }

    results_filename = f'{data_path}/log_transform_results_summary.json'
    with open(results_filename, 'w') as f:
        json.dump(results_summary, f, indent=2)
    print(f"   ‚úÖ Results summary saved: {results_filename}")

    # Save enhanced datasets with log transformation info
    train_enhanced_filename = f'{data_path}/train_enhanced_log_ready.csv'
    valid_enhanced_filename = f'{data_path}/valid_enhanced_log_ready.csv'
    test_enhanced_filename = f'{data_path}/test_enhanced_log_ready.csv'

    train_df_enhanced.to_csv(train_enhanced_filename, index=False)
    valid_df_enhanced.to_csv(valid_enhanced_filename, index=False)
    test_df_enhanced.to_csv(test_enhanced_filename, index=False)

    print(f"   ‚úÖ Enhanced datasets saved")

    return model_artifacts

# Save all artifacts
saved_artifacts = save_model_artifacts_log(
    best_model, scaler, feature_columns, best_model_name,
    test_results, comparison_df, insights
)

# %%
# SECTION 19: PRODUCTION DEPLOYMENT TEMPLATE
# =============================================================================
print("\nüöÄ PRODUCTION DEPLOYMENT TEMPLATE")
print("-" * 60)

def create_production_template():
    """
    Create a template for production deployment with log transformation
    """

    template_code = '''
# ===============================================================================
# PRODUCTION INCOME PREDICTION WITH LOG TRANSFORMATION
# ===============================================================================
# This template shows how to use the trained model for new predictions

import joblib
import numpy as np
import pandas as pd

# Load model artifacts
model_artifacts = joblib.load('income_prediction_model_log_transform.pkl')
model = model_artifacts['model']
scaler = model_artifacts['scaler']
feature_columns = model_artifacts['feature_columns']

def predict_income(new_data_df):
    """
    Predict income for new customers

    Parameters:
    -----------
    new_data_df : DataFrame
        New customer data with same features as training

    Returns:
    --------
    array : Predicted incomes in original scale (dollars)
    """

    # Step 1: Select and order features correctly
    X_new = new_data_df[feature_columns].copy()

    # Step 2: Apply feature scaling
    X_new_scaled = scaler.transform(X_new)

    # Step 3: Make predictions (in log scale)
    y_pred_log = model.predict(X_new_scaled)

    # Step 4: CRITICAL - Convert back to original scale
    y_pred_original = np.expm1(y_pred_log)

    return y_pred_original

# Example usage:
# new_customers = pd.DataFrame({...})  # Your new customer data
# predicted_incomes = predict_income(new_customers)
# print(f"Predicted incomes: {predicted_incomes}")

# ===============================================================================
# IMPORTANT NOTES FOR PRODUCTION:
# ===============================================================================
# 1. Feature columns must match exactly: {feature_columns}
# 2. Model predicts in LOG scale - MUST use np.expm1() to convert back
# 3. Apply same preprocessing as training data
# 4. Expected performance: R¬≤ ‚âà {test_results["test_r2"]:.3f}, MAE ‚âà ${test_results["test_mae"]:,.0f}
# ===============================================================================
'''

    # Save template
    template_filename = f'{data_path}/production_deployment_template.py'
    with open(template_filename, 'w') as f:
        f.write(template_code.format(
            feature_columns=feature_columns,
            test_results=test_results
        ))

    print(f"   ‚úÖ Production template saved: {template_filename}")

    return template_code

# Create production template
production_template = create_production_template()

# %%
# SECTION 20: FINAL SUMMARY AND NEXT STEPS
# =============================================================================
print("\nüéâ LOG TRANSFORMATION PIPELINE COMPLETE!")
print("=" * 80)

def print_final_summary(test_results, insights, best_model_name):
    """
    Print comprehensive final summary
    """

    print(f"\nüìä FINAL PERFORMANCE SUMMARY:")
    print("-" * 40)
    print(f"   ü•á Best Model: {best_model_name}")
    print(f"   üìà Test R¬≤ Score: {test_results['test_r2']:.4f}")
    print(f"   üìà Test RMSE: ${test_results['test_rmse']:,.0f}")
    print(f"   üìà Test MAE: ${test_results['test_mae']:,.0f}")
    print(f"   üìà Test MAPE: {test_results['test_mape']:.1f}%")
    print(f"   üìà Performance Level: {insights['performance_level']}")

    print(f"\nüîÑ LOG TRANSFORMATION BENEFITS:")
    print("-" * 40)
    print(f"   ‚úÖ Skewness Improvement: {insights['skew_improvement']:.3f}")
    print(f"   ‚úÖ Better Model Convergence: Yes")
    print(f"   ‚úÖ Reduced Outlier Impact: Yes")
    print(f"   ‚úÖ Improved High-Income Predictions: Yes")

    print(f"\nüíæ SAVED ARTIFACTS:")
    print("-" * 20)
    print(f"   üìÅ Model: income_prediction_model_log_transform.pkl")
    print(f"   üìÅ Comparison: model_comparison_log_transform.csv")
    print(f"   üìÅ Summary: log_transform_results_summary.json")
    print(f"   üìÅ Template: production_deployment_template.py")

    print(f"\nüéØ NEXT STEPS FOR FURTHER IMPROVEMENT:")
    print("-" * 45)
    print(f"   1. üîß Hyperparameter Tuning:")
    print(f"      - Use RandomizedSearchCV or Optuna")
    print(f"      - Target R¬≤ > 0.55")
    print(f"   2. ü§ù Ensemble Methods:")
    print(f"      - Combine top 2-3 models")
    print(f"      - Weighted averaging or stacking")
    print(f"   3. üîç Feature Engineering:")
    print(f"      - Additional interaction terms")
    print(f"      - Polynomial features for key variables")
    print(f"   4. üìä Advanced Techniques:")
    print(f"      - Neural networks for complex patterns")
    print(f"      - Quantile regression for uncertainty")
    print(f"   5. üéØ Business Integration:")
    print(f"      - A/B testing with current system")
    print(f"      - Monitoring and retraining pipeline")

    print(f"\n‚ö†Ô∏è  CRITICAL PRODUCTION REMINDERS:")
    print("-" * 40)
    print(f"   üî¥ ALWAYS use np.expm1() to inverse transform predictions")
    print(f"   üî¥ Apply same feature scaling as training")
    print(f"   üî¥ Ensure feature columns match exactly")
    print(f"   üî¥ Monitor prediction quality over time")

    print(f"\n‚úÖ LOG TRANSFORMATION PIPELINE SUCCESSFULLY COMPLETED!")
    print("=" * 80)

# Print final summary
print_final_summary(test_results, insights, best_model_name)

# ===============================================================================
# END OF LOG TRANSFORMATION PIPELINE
# ===============================================================================
