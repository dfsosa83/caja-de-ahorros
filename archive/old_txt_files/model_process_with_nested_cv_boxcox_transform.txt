# %%
# =============================================================================
# INCOME PREDICTION MODEL - NESTED CV WITH BOX-COX TRANSFORMATION
# =============================================================================
# Goal: Predict customer income using NESTED CV with BOX-COX TRANSFORMATION for optimal normalization
# Based on: model_process_with_nested_cv.txt (successful baseline)
# 
# Key Features:
# - Box-Cox Transformation: Optimal power transformation to achieve normality
# - Nested CV Structure: Outer CV (5 folds) for evaluation, Inner CV (3 folds) for hyperparameter tuning
# - Robust Metrics: RMSE/MAE on original scale for interpretability
# - Complete Pipeline: From preprocessed data to production-ready model
# - Comprehensive Analysis: Permutation importance, visualizations, model comparison
# 
# Models: XGBoost, LightGBM, Random Forest
# Target: Box-Cox(ingresos_reportados) -> back-transformed for evaluation
# =============================================================================

# %%
# SECTION 1: IMPORTS AND SETUP
# =============================================================================
import pandas as pd
import numpy as np
import warnings
from sklearn.preprocessing import RobustScaler, PowerTransformer
from sklearn.model_selection import (KFold, GroupKFold, cross_val_score, 
                                   RandomizedSearchCV)
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.inspection import permutation_importance
from scipy import stats
from scipy.stats import boxcox
import xgboost as xgb
import lightgbm as lgb
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
from collections import Counter

warnings.filterwarnings('ignore')

print("================================================================================")
print("NESTED CROSS-VALIDATION WITH BOX-COX TRANSFORMATION - INCOME PREDICTION MODEL")
print("================================================================================")
print("📋 Pipeline based on: model_process_with_nested_cv.txt")
print("🔄 Target Transformation: Box-Cox for optimal normalization")
print("🎯 Primary Metrics: RMSE/MAE on original scale (back-transformed)")
print("🔄 Nested CV: Outer (5-fold) for evaluation, Inner (3-fold) for hyperparameter tuning")

# %%
# SECTION 2: PREPARE FINAL DATASETS FOR NESTED CV MODELING
# =============================================================================
print("\n🎯 PREPARING FINAL DATASETS FOR NESTED CV WITH BOX-COX TRANSFORMATION")
print("-" * 80)

# EXPLICIT FEATURE SELECTION APPROACH (from original pipeline)
id_columns = ['cliente', 'identificador_unico']
target_column = 'ingresos_reportados'

# Use the selected features from the original pipeline
# NOTE: Adjust 'selected_features_final' to your actual feature list variable
feature_columns = selected_features_final

# Verify all selected features exist in the dataset
available_features = []
missing_features = []

for feature in feature_columns:
    if feature in train_df_enhanced.columns:
        available_features.append(feature)
    else:
        missing_features.append(feature)

if missing_features:
    print(f"⚠️  Missing features (will be skipped): {missing_features}")

feature_columns = available_features

print(f"   📊 Selected feature columns: {len(feature_columns)}")
print(f"   🎯 Target column: {target_column}")

# Create feature matrices and targets (from original pipeline structure)
X_train = train_df_enhanced[feature_columns].copy()
y_train = train_df_enhanced[target_column].copy()

X_valid = valid_df_enhanced[feature_columns].copy()
y_valid = valid_df_enhanced[target_column].copy()

X_test = test_df_enhanced[feature_columns].copy()
y_test = test_df_enhanced[target_column].copy()

print(f"\n📈 DATASET SHAPES:")
print(f"   X_train: {X_train.shape}")
print(f"   X_valid: {X_valid.shape}")
print(f"   X_test: {X_test.shape}")

# %%
# SECTION 3: BOX-COX TRANSFORMATION OF TARGET VARIABLE
# =============================================================================
print("\n🔄 BOX-COX TRANSFORMATION OF TARGET VARIABLE")
print("-" * 70)

# Analyze original target distribution
print("📊 ORIGINAL TARGET DISTRIBUTION:")
print(f"   Mean: ${y_train.mean():,.2f}")
print(f"   Median: ${y_train.median():,.2f}")
print(f"   Std: ${y_train.std():,.2f}")
print(f"   Skewness: {y_train.skew():.3f}")
print(f"   Min: ${y_train.min():,.2f}")
print(f"   Max: ${y_train.max():,.2f}")

# Check for zero or negative values (Box-Cox requires positive values)
zero_negative_count = (y_train <= 0).sum()
print(f"   Zero/negative values: {zero_negative_count} ({zero_negative_count/len(y_train)*100:.2f}%)")

# Combine train and validation for transformation fitting
y_train_full = pd.concat([y_train, y_valid], ignore_index=True)
X_train_full = pd.concat([X_train, X_valid], ignore_index=True)

print(f"   Combined training data: {len(y_train_full):,} samples")

# Handle zero/negative values for Box-Cox transformation
if zero_negative_count > 0:
    print(f"\n⚠️  Handling zero/negative values for Box-Cox transformation...")
    # Add small constant to make all values positive
    min_positive_shift = abs(y_train_full.min()) + 1
    y_train_full_positive = y_train_full + min_positive_shift
    y_train_positive = y_train + min_positive_shift
    y_valid_positive = y_valid + min_positive_shift
    y_test_positive = y_test + min_positive_shift
    print(f"   Added constant: {min_positive_shift}")
    print(f"   New minimum value: {y_train_full_positive.min()}")
else:
    print(f"\n✅ All values are positive - no adjustment needed")
    y_train_full_positive = y_train_full
    y_train_positive = y_train
    y_valid_positive = y_valid
    y_test_positive = y_test
    min_positive_shift = 0

# Apply Box-Cox transformation
print(f"\n🔄 Applying Box-Cox transformation...")

# Fit Box-Cox transformation on combined training data
y_train_full_boxcox, fitted_lambda = boxcox(y_train_full_positive)

print(f"   ✅ Box-Cox transformation fitted")
print(f"   📊 Optimal lambda (λ): {fitted_lambda:.4f}")

# Interpret lambda value
if abs(fitted_lambda) < 0.01:
    lambda_interpretation = "≈ Log transformation"
elif abs(fitted_lambda - 0.5) < 0.01:
    lambda_interpretation = "≈ Square root transformation"
elif abs(fitted_lambda - 1.0) < 0.01:
    lambda_interpretation = "≈ No transformation needed"
elif abs(fitted_lambda - 2.0) < 0.01:
    lambda_interpretation = "≈ Square transformation"
else:
    lambda_interpretation = f"Custom power transformation (λ={fitted_lambda:.3f})"

print(f"   💡 Lambda interpretation: {lambda_interpretation}")

# Apply transformation to all datasets
def apply_boxcox_transform(data, lambda_param):
    """Apply Box-Cox transformation with fitted lambda"""
    if abs(lambda_param) < 1e-6:  # lambda ≈ 0, use log
        return np.log(data)
    else:
        return (np.power(data, lambda_param) - 1) / lambda_param

def inverse_boxcox_transform(data, lambda_param):
    """Inverse Box-Cox transformation"""
    if abs(lambda_param) < 1e-6:  # lambda ≈ 0, use exp
        return np.exp(data)
    else:
        return np.power(lambda_param * data + 1, 1 / lambda_param)

# Transform all target variables
y_train_boxcox = apply_boxcox_transform(y_train_positive, fitted_lambda)
y_valid_boxcox = apply_boxcox_transform(y_valid_positive, fitted_lambda)
y_test_boxcox = apply_boxcox_transform(y_test_positive, fitted_lambda)

print(f"   X_train_full (for nested CV): {X_train_full.shape}")
print(f"   X_test (held out): {X_test.shape}")

# Analyze transformed target distribution
print(f"\n📊 BOX-COX TRANSFORMED TARGET DISTRIBUTION:")
print(f"   Mean: {y_train_full_boxcox.mean():.3f}")
print(f"   Median: {np.median(y_train_full_boxcox):.3f}")
print(f"   Std: {y_train_full_boxcox.std():.3f}")
print(f"   Skewness: {stats.skew(y_train_full_boxcox):.3f}")
print(f"   Min: {y_train_full_boxcox.min():.3f}")
print(f"   Max: {y_train_full_boxcox.max():.3f}")

# Calculate improvement in skewness
original_skew = y_train_full.skew()
transformed_skew = stats.skew(y_train_full_boxcox)
skew_improvement = abs(original_skew) - abs(transformed_skew)

print(f"\n✅ BOX-COX TRANSFORMATION EFFECTIVENESS:")
print(f"   Original skewness: {original_skew:.3f}")
print(f"   Box-Cox transformed skewness: {transformed_skew:.3f}")
print(f"   Skewness improvement: {skew_improvement:.3f} {'✅ Better' if skew_improvement > 0 else '⚠️ Worse'}")
print(f"   Optimal lambda: {fitted_lambda:.4f} ({lambda_interpretation})")

# Show selected features grouped by type
print(f"\n📋 SELECTED FEATURES ({len(feature_columns)} features):")
print("-" * 60)

# Group features by type for better readability (from original pipeline)
basic_features = []
age_features = []
freq_features = []
interaction_features = []
other_features = []

for feature in feature_columns:
    if feature.startswith('age_group_'):
        age_features.append(feature)
    elif feature.endswith('_freq'):
        freq_features.append(feature)
    elif '_x_' in feature or 'retired_x_' in feature or 'employer_x_' in feature or 'gender_x_' in feature:
        interaction_features.append(feature)
    elif feature in ['edad', 'letras_mensuales', 'monto_letra', 'saldo', 'is_retired']:
        basic_features.append(feature)
    else:
        other_features.append(feature)

print(f"🔢 BASIC FEATURES ({len(basic_features)}): {basic_features}")
print(f"👥 AGE GROUP FEATURES ({len(age_features)}): {age_features}")
print(f"📊 FREQUENCY FEATURES ({len(freq_features)}): {freq_features}")
print(f"⚡ INTERACTION FEATURES ({len(interaction_features)}): {interaction_features}")
print(f"🔧 OTHER FEATURES ({len(other_features)}): {other_features}")

# Verify data quality
print(f"\n✅ DATA QUALITY CHECKS:")
print(f"   Missing values in X_train_full: {X_train_full.isnull().sum().sum()}")
print(f"   Missing values in y_train_full_boxcox: {np.isnan(y_train_full_boxcox).sum()}")
print(f"   All features numeric: {all(X_train_full.dtypes.apply(lambda x: x in ['int64', 'float64']))}")

# Save transformation parameters and feature list
transformation_info = {
    'transformation_type': 'box_cox',
    'fitted_lambda': fitted_lambda,
    'lambda_interpretation': lambda_interpretation,
    'min_positive_shift': min_positive_shift,
    'original_skewness': original_skew,
    'transformed_skewness': transformed_skew,
    'skewness_improvement': skew_improvement
}

# Save feature list and transformation info
feature_list_df = pd.DataFrame({
    'feature_name': feature_columns,
    'feature_type': ['basic' if f in basic_features else
                    'age_group' if f in age_features else
                    'frequency' if f in freq_features else
                    'interaction' if f in interaction_features else
                    'other' for f in feature_columns]
})

feature_list_df.to_csv(data_path + '/nested_cv_boxcox_transform_feature_list.csv', index=False)

with open(data_path + '/boxcox_transformation_info.json', 'w') as f:
    json.dump(transformation_info, f, indent=2)

print(f"\n💾 Feature list saved to: nested_cv_boxcox_transform_feature_list.csv")
print(f"💾 Transformation info saved to: boxcox_transformation_info.json")

# %%
# SECTION 4: FEATURE SCALING
# =============================================================================
print("\n⚖️ FEATURE SCALING")
print("-" * 50)

# Apply robust scaling to features (from original pipeline)
print("   ⚖️ Applying RobustScaler...")
scaler = RobustScaler()

# Fit scaler on full training data (train + validation combined)
X_train_full_scaled = pd.DataFrame(
    scaler.fit_transform(X_train_full),
    columns=X_train_full.columns,
    index=X_train_full.index
)

# Transform test set using the same scaler
X_test_scaled = pd.DataFrame(
    scaler.transform(X_test),
    columns=X_test.columns,
    index=X_test.index
)

print("   ✅ Feature scaling complete")
print(f"   📊 Scaled training data: {X_train_full_scaled.shape}")
print(f"   📊 Scaled test data: {X_test_scaled.shape}")

# %%
# SECTION 5: VISUALIZATION OF BOX-COX TRANSFORMATION
# =============================================================================
print("\n📈 CREATING BOX-COX TRANSFORMATION VISUALIZATION")
print("-" * 70)

# Create comprehensive transformation analysis plots
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('Box-Cox Transformation Analysis', fontsize=16, fontweight='bold')

# 1. Original distribution
ax1 = axes[0, 0]
ax1.hist(y_train_full, bins=50, alpha=0.7, color='lightblue', edgecolor='black')
ax1.set_xlabel('Income ($)')
ax1.set_ylabel('Frequency')
ax1.set_title(f'Original Income Distribution\nSkewness: {original_skew:.3f}')
ax1.grid(True, alpha=0.3)

# 2. Box-Cox transformed distribution
ax2 = axes[0, 1]
ax2.hist(y_train_full_boxcox, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')
ax2.set_xlabel('Box-Cox Transformed Income')
ax2.set_ylabel('Frequency')
ax2.set_title(f'Box-Cox Transformed Distribution\nSkewness: {transformed_skew:.3f}, λ={fitted_lambda:.3f}')
ax2.grid(True, alpha=0.3)

# 3. Q-Q plot for original
ax3 = axes[0, 2]
stats.probplot(y_train_full, dist="norm", plot=ax3)
ax3.set_title('Q-Q Plot: Original Income vs Normal')
ax3.grid(True, alpha=0.3)

# 4. Q-Q plot for Box-Cox transformed
ax4 = axes[1, 0]
stats.probplot(y_train_full_boxcox, dist="norm", plot=ax4)
ax4.set_title('Q-Q Plot: Box-Cox Transformed vs Normal')
ax4.grid(True, alpha=0.3)

# 5. Lambda optimization curve (if we want to show different lambda values)
ax5 = axes[1, 1]
lambda_range = np.linspace(-2, 2, 100)
log_likelihoods = []

for lam in lambda_range:
    try:
        if abs(lam) < 1e-6:
            transformed = np.log(y_train_full_positive)
        else:
            transformed = (np.power(y_train_full_positive, lam) - 1) / lam

        # Calculate log-likelihood (simplified)
        log_likelihood = -0.5 * len(transformed) * np.log(np.var(transformed))
        log_likelihoods.append(log_likelihood)
    except:
        log_likelihoods.append(-np.inf)

ax5.plot(lambda_range, log_likelihoods, 'b-', linewidth=2)
ax5.axvline(x=fitted_lambda, color='red', linestyle='--', linewidth=2,
           label=f'Optimal λ = {fitted_lambda:.3f}')
ax5.set_xlabel('Lambda (λ)')
ax5.set_ylabel('Log-Likelihood')
ax5.set_title('Box-Cox Lambda Optimization')
ax5.legend()
ax5.grid(True, alpha=0.3)

# 6. Before vs After comparison
ax6 = axes[1, 2]
ax6.boxplot([y_train_full, y_train_full_boxcox],
           labels=['Original', 'Box-Cox'], patch_artist=True)
ax6.set_ylabel('Values')
ax6.set_title('Distribution Comparison')
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(data_path + '/boxcox_transformation_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print("   ✅ Box-Cox transformation visualization saved to: boxcox_transformation_analysis.png")

print(f"\n💡 BOX-COX TRANSFORMATION INSIGHTS:")
print(f"   📊 Original distribution: Skewed ({original_skew:.3f})")
print(f"   📊 Box-Cox transformed: More normal ({transformed_skew:.3f})")
print(f"   🔧 Optimal lambda: {fitted_lambda:.4f} ({lambda_interpretation})")
print(f"   ✅ Expected benefits: Optimal normalization, better model performance")
print(f"   🎯 Evaluation: All metrics will be back-transformed to original scale")

# %%
# SECTION 6: NESTED CROSS-VALIDATION SETUP WITH BOX-COX TRANSFORMATION
# =============================================================================
print("\n🔄 NESTED CROSS-VALIDATION SETUP WITH BOX-COX TRANSFORMATION")
print("-" * 80)

# Define CV strategies with robust metrics focus
OUTER_CV_FOLDS = 5  # For unbiased performance estimation
INNER_CV_FOLDS = 3  # For hyperparameter tuning
RANDOM_SEARCH_ITERATIONS = 15  # Increased for better hyperparameter search

# Create CV objects
outer_cv = KFold(n_splits=OUTER_CV_FOLDS, shuffle=True, random_state=42)
inner_cv = KFold(n_splits=INNER_CV_FOLDS, shuffle=True, random_state=42)

print(f"   🔄 Outer CV: {OUTER_CV_FOLDS} folds (for unbiased model evaluation)")
print(f"   🔄 Inner CV: {INNER_CV_FOLDS} folds (for hyperparameter tuning)")
print(f"   🔍 Random Search: {RANDOM_SEARCH_ITERATIONS} iterations per inner fold")
print(f"   📊 Total model trainings: {OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS} per model")
print(f"      ({OUTER_CV_FOLDS} outer × {INNER_CV_FOLDS} inner × {RANDOM_SEARCH_ITERATIONS} iterations)")

# %%
# SECTION 7: MODEL DEFINITIONS AND HYPERPARAMETER GRIDS
# =============================================================================
print("\n🤖 MODEL DEFINITIONS AND HYPERPARAMETER GRIDS")
print("-" * 60)

# Base models (from original pipeline with robust configurations)
base_models = {
    'XGBoost': xgb.XGBRegressor(random_state=42, n_jobs=-1),
    'LightGBM': lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),
    'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1)
}

# Comprehensive hyperparameter grids for nested CV
# These grids are designed for Box-Cox transformed income prediction optimization
param_grids = {
    'XGBoost': {
        'n_estimators': [300, 500, 700],
        'max_depth': [6, 8, 10],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.8, 0.85, 0.9],
        'colsample_bytree': [0.8, 0.85, 0.9],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [0.5, 1.0, 2.0],
        'min_child_weight': [1, 3, 5]
    },
    'LightGBM': {
        'n_estimators': [300, 500, 700],
        'max_depth': [8, 12, 16],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.8, 0.85, 0.9],
        'colsample_bytree': [0.8, 0.85, 0.9],
        'num_leaves': [50, 100, 150],
        'min_child_samples': [10, 20, 30],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [0.5, 1.0, 2.0]
    },
    'Random Forest': {
        'n_estimators': [200, 300, 400],
        'max_depth': [10, 15, 20, None],
        'min_samples_split': [5, 10, 15],
        'min_samples_leaf': [2, 5, 10],
        'max_features': ['sqrt', 'log2', 0.8],
        'max_samples': [0.7, 0.8, 0.9]
    }
}

print(f"   🤖 Models defined: {list(base_models.keys())}")
print(f"   🔧 Hyperparameter search space per model:")
for model_name, grid in param_grids.items():
    total_combinations = np.prod([len(values) for values in grid.values()])
    print(f"      {model_name}: {total_combinations:,} total combinations")

print(f"\n💡 Primary Optimization Metric: MSE on Box-Cox transformed target")
print(f"   📊 MSE optimized on Box-Cox scale for optimal normalization")
print(f"   💰 Final evaluation on original scale (back-transformed)")
print(f"   📈 Lower MSE on Box-Cox scale = Better model performance")
print(f"   🔧 Lambda = {fitted_lambda:.4f} ({lambda_interpretation})")

# %%
# SECTION 8: NESTED CROSS-VALIDATION IMPLEMENTATION WITH BOX-COX TRANSFORMATION
# =============================================================================
print("\n🎯 NESTED CROSS-VALIDATION IMPLEMENTATION WITH BOX-COX TRANSFORMATION")
print("=" * 90)

def nested_cross_validation_boxcox_transform(model, param_grid, X, y_boxcox, y_original,
                                           lambda_param, min_shift, outer_cv, inner_cv, model_name):
    """
    Perform nested cross-validation for Box-Cox transformed target with back-transformation for evaluation

    This implementation optimizes on Box-Cox transformed target but evaluates on original scale
    for interpretable results.

    Args:
        model: Base model to evaluate
        param_grid: Hyperparameter grid for tuning
        X, y_boxcox: Training data and Box-Cox transformed target
        y_original: Original target for back-transformed evaluation
        lambda_param: Fitted Box-Cox lambda parameter
        min_shift: Minimum shift applied to make values positive
        outer_cv, inner_cv: Cross-validation objects
        model_name: Name for reporting

    Returns:
        dict: Comprehensive nested CV results with back-transformed metrics
    """
    print(f"\n🔄 Starting Nested CV with Box-Cox Transform for {model_name}")
    print(f"   📊 Outer folds: {outer_cv.n_splits}, Inner folds: {inner_cv.n_splits}")
    print(f"   🎯 Optimization: MSE on Box-Cox transformed target (λ={lambda_param:.4f})")
    print(f"   📈 Evaluation: RMSE/MAE on original scale (back-transformed)")

    # Storage for results - back-transformed metrics
    outer_scores_rmse = []
    outer_scores_mae = []
    outer_scores_r2 = []
    outer_scores_boxcox_mse = []  # Track Box-Cox scale performance too
    best_params_per_fold = []
    fold_details = []

    # Outer CV loop - each iteration gives unbiased performance estimate
    for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X, y_boxcox), 1):
        print(f"\n   🔄 Outer Fold {fold_idx}/{outer_cv.n_splits}")

        # Split data for this outer fold
        X_train_outer = X.iloc[train_idx]
        X_val_outer = X.iloc[val_idx]
        y_train_outer_boxcox = y_boxcox[train_idx]
        y_val_outer_boxcox = y_boxcox[val_idx]
        y_val_outer_original = y_original.iloc[val_idx]  # For back-transformed evaluation

        print(f"      📊 Fold {fold_idx} sizes: Train={len(train_idx)}, Val={len(val_idx)}")

        # Inner CV: Hyperparameter tuning using MSE on Box-Cox transformed target
        print(f"      🔧 Inner CV: Hyperparameter tuning (optimizing Box-Cox scale MSE)...")

        random_search = RandomizedSearchCV(
            estimator=model,
            param_distributions=param_grid,
            n_iter=RANDOM_SEARCH_ITERATIONS,
            cv=inner_cv,
            scoring='neg_mean_squared_error',  # Optimizing MSE on Box-Cox scale
            n_jobs=-1,
            random_state=42,
            verbose=0
        )

        # Fit hyperparameter search on outer training data (Box-Cox transformed)
        random_search.fit(X_train_outer, y_train_outer_boxcox)

        # Get best model from inner CV
        best_model = random_search.best_estimator_
        best_params = random_search.best_params_
        best_params_per_fold.append(best_params)

        # Convert negative MSE back to MSE for reporting
        best_inner_mse_boxcox = -random_search.best_score_
        print(f"      ✅ Best inner CV MSE (Box-Cox scale): {best_inner_mse_boxcox:.4f}")

        # Evaluate best model on outer validation fold
        y_pred_outer_boxcox = best_model.predict(X_val_outer)

        # Back-transform predictions to original scale
        y_pred_outer_original = inverse_boxcox_transform(y_pred_outer_boxcox, lambda_param)

        # Adjust for the positive shift if it was applied
        if min_shift > 0:
            y_pred_outer_original = y_pred_outer_original - min_shift
            # Ensure no negative predictions
            y_pred_outer_original = np.maximum(y_pred_outer_original, 0)

        # Calculate comprehensive metrics on original scale
        fold_rmse = np.sqrt(mean_squared_error(y_val_outer_original, y_pred_outer_original))
        fold_mae = mean_absolute_error(y_val_outer_original, y_pred_outer_original)
        fold_r2 = r2_score(y_val_outer_original, y_pred_outer_original)
        fold_boxcox_mse = mean_squared_error(y_val_outer_boxcox, y_pred_outer_boxcox)  # Box-Cox scale MSE

        # Store scores
        outer_scores_rmse.append(fold_rmse)
        outer_scores_mae.append(fold_mae)
        outer_scores_r2.append(fold_r2)
        outer_scores_boxcox_mse.append(fold_boxcox_mse)

        print(f"      📊 Outer fold performance (original scale):")
        print(f"         🎯 RMSE: ${fold_rmse:.2f}")
        print(f"         🎯 MAE:  ${fold_mae:.2f}")
        print(f"         📈 R²:   {fold_r2:.4f}")
        print(f"         🔄 Box-Cox MSE: {fold_boxcox_mse:.4f}")

        # Calculate additional income-specific metrics
        # MAPE for incomes > $100 (avoid division by very small numbers)
        valid_mask = y_val_outer_original > 100
        if valid_mask.sum() > 0:
            mape = np.mean(np.abs((y_val_outer_original[valid_mask] - y_pred_outer_original[valid_mask]) / y_val_outer_original[valid_mask])) * 100
            print(f"         💰 MAPE (>$100): {mape:.1f}%")
        else:
            mape = np.nan

        # Store detailed results for this fold
        fold_details.append({
            'fold': fold_idx,
            'rmse': fold_rmse,
            'mae': fold_mae,
            'r2': fold_r2,
            'boxcox_mse': fold_boxcox_mse,
            'mape': mape,
            'best_params': best_params,
            'inner_cv_boxcox_mse': best_inner_mse_boxcox,
            'train_size': len(train_idx),
            'val_size': len(val_idx),
            'y_true_mean': y_val_outer_original.mean(),
            'y_pred_mean': y_pred_outer_original.mean()
        })

    # Calculate final nested CV results with back-transformed metrics
    nested_cv_results = {
        'model_name': model_name,
        # Primary metrics (RMSE/MAE on original scale)
        'outer_cv_rmse_mean': np.mean(outer_scores_rmse),
        'outer_cv_rmse_std': np.std(outer_scores_rmse),
        'outer_cv_mae_mean': np.mean(outer_scores_mae),
        'outer_cv_mae_std': np.std(outer_scores_mae),
        # Secondary metric (R² on original scale)
        'outer_cv_r2_mean': np.mean(outer_scores_r2),
        'outer_cv_r2_std': np.std(outer_scores_r2),
        # Box-Cox scale performance tracking
        'outer_cv_boxcox_mse_mean': np.mean(outer_scores_boxcox_mse),
        'outer_cv_boxcox_mse_std': np.std(outer_scores_boxcox_mse),
        # Raw scores for analysis
        'outer_scores_rmse': outer_scores_rmse,
        'outer_scores_mae': outer_scores_mae,
        'outer_scores_r2': outer_scores_r2,
        'outer_scores_boxcox_mse': outer_scores_boxcox_mse,
        # Hyperparameter analysis
        'best_params_per_fold': best_params_per_fold,
        'fold_details': fold_details,
        # Model selection criteria (lower is better for RMSE/MAE)
        'selection_metric': 'rmse',
        'selection_score': np.mean(outer_scores_rmse),
        # Transformation info
        'transformation': 'box_cox_transform',
        'lambda_parameter': lambda_param,
        'min_positive_shift': min_shift,
        'lambda_interpretation': lambda_interpretation
    }

    print(f"\n   🏆 {model_name} Nested CV Summary (Box-Cox Transform):")
    print(f"      🎯 RMSE (original): ${nested_cv_results['outer_cv_rmse_mean']:.2f} ± ${nested_cv_results['outer_cv_rmse_std']:.2f}")
    print(f"      🎯 MAE (original):  ${nested_cv_results['outer_cv_mae_mean']:.2f} ± ${nested_cv_results['outer_cv_mae_std']:.2f}")
    print(f"      📈 R² (original):   {nested_cv_results['outer_cv_r2_mean']:.4f} ± {nested_cv_results['outer_cv_r2_std']:.4f}")
    print(f"      🔄 Box-Cox MSE:     {nested_cv_results['outer_cv_boxcox_mse_mean']:.4f} ± {nested_cv_results['outer_cv_boxcox_mse_std']:.4f}")

    return nested_cv_results

print("✅ Nested CV function with Box-Cox transformation defined")

# %%
# SECTION 9: RUN NESTED CROSS-VALIDATION WITH BOX-COX TRANSFORMATION
# =============================================================================
print("\n🚀 RUNNING NESTED CROSS-VALIDATION WITH BOX-COX TRANSFORMATION")
print("=" * 90)
print("⚠️  This will take 20-60 minutes depending on your hardware...")
print(f"   Each model will be trained {OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS} times")
print(f"   ({OUTER_CV_FOLDS} outer × {INNER_CV_FOLDS} inner × {RANDOM_SEARCH_ITERATIONS} iterations)")
print("🎯 Optimizing for MSE on Box-Cox transformed target, evaluating on original scale")

# Storage for all nested CV results
nested_cv_results_boxcox = {}
total_start_time = datetime.now()

# Run nested CV for each model with Box-Cox transformation
for model_idx, (model_name, base_model) in enumerate(base_models.items(), 1):
    print(f"\n{'='*100}")
    print(f"NESTED CV WITH BOX-COX TRANSFORM {model_idx}/{len(base_models)}: {model_name.upper()}")
    print(f"{'='*100}")

    model_start_time = datetime.now()

    # Run nested cross-validation with Box-Cox transformation
    results = nested_cross_validation_boxcox_transform(
        model=base_model,
        param_grid=param_grids[model_name],
        X=X_train_full_scaled,
        y_boxcox=y_train_full_boxcox,  # Box-Cox transformed target for optimization
        y_original=y_train_full,  # Original target for evaluation
        lambda_param=fitted_lambda,  # Fitted Box-Cox lambda
        min_shift=min_positive_shift,  # Positive shift applied
        outer_cv=outer_cv,
        inner_cv=inner_cv,
        model_name=model_name
    )

    nested_cv_results_boxcox[model_name] = results

    model_end_time = datetime.now()
    model_duration = (model_end_time - model_start_time).total_seconds() / 60

    print(f"\n   ⏱️ {model_name} completed in {model_duration:.1f} minutes")

    # Show progress
    remaining_models = len(base_models) - model_idx
    if remaining_models > 0:
        estimated_remaining = model_duration * remaining_models
        print(f"   📊 Progress: {model_idx}/{len(base_models)} models complete")
        print(f"   ⏰ Estimated remaining time: {estimated_remaining:.1f} minutes")

total_end_time = datetime.now()
total_duration = (total_end_time - total_start_time).total_seconds() / 60

print(f"\n🎉 ALL NESTED CV WITH BOX-COX TRANSFORMATION COMPLETED!")
print(f"⏱️ Total execution time: {total_duration:.1f} minutes")

# %%
# SECTION 10: BOX-COX TRANSFORM RESULTS ANALYSIS AND MODEL COMPARISON
# =============================================================================
print("\n📊 BOX-COX TRANSFORM NESTED CV RESULTS ANALYSIS")
print("=" * 90)

# Create comprehensive comparison DataFrame with robust metrics focus
comparison_data_boxcox = []
for model_name, results in nested_cv_results_boxcox.items():
    comparison_data_boxcox.append({
        'Model': model_name,
        # Primary metrics (lower is better) - on original scale
        'BoxCox_CV_RMSE_Mean': results['outer_cv_rmse_mean'],
        'BoxCox_CV_RMSE_Std': results['outer_cv_rmse_std'],
        'BoxCox_CV_MAE_Mean': results['outer_cv_mae_mean'],
        'BoxCox_CV_MAE_Std': results['outer_cv_mae_std'],
        # Secondary metric - on original scale
        'BoxCox_CV_R2_Mean': results['outer_cv_r2_mean'],
        'BoxCox_CV_R2_Std': results['outer_cv_r2_std'],
        # Box-Cox scale performance
        'BoxCox_Scale_MSE_Mean': results['outer_cv_boxcox_mse_mean'],
        'BoxCox_Scale_MSE_Std': results['outer_cv_boxcox_mse_std'],
        # Selection score (RMSE for ranking)
        'Selection_Score': results['selection_score']
    })

nested_comparison_boxcox_df = pd.DataFrame(comparison_data_boxcox)
# Sort by RMSE (lower is better) - primary metric for income prediction
nested_comparison_boxcox_df = nested_comparison_boxcox_df.sort_values('BoxCox_CV_RMSE_Mean', ascending=True)

print("\n🏆 BOX-COX TRANSFORM NESTED CV PERFORMANCE COMPARISON (Sorted by RMSE - Lower is Better):")
print("=" * 110)
print(nested_comparison_boxcox_df.round(4).to_string(index=False))

# Identify best model based on RMSE (robust metric for income prediction)
best_model_boxcox = nested_comparison_boxcox_df.iloc[0]['Model']
best_rmse_boxcox = nested_comparison_boxcox_df.iloc[0]['BoxCox_CV_RMSE_Mean']
best_rmse_std_boxcox = nested_comparison_boxcox_df.iloc[0]['BoxCox_CV_RMSE_Std']
best_mae_boxcox = nested_comparison_boxcox_df.iloc[0]['BoxCox_CV_MAE_Mean']
best_mae_std_boxcox = nested_comparison_boxcox_df.iloc[0]['BoxCox_CV_MAE_Std']
best_r2_boxcox = nested_comparison_boxcox_df.iloc[0]['BoxCox_CV_R2_Mean']
best_r2_std_boxcox = nested_comparison_boxcox_df.iloc[0]['BoxCox_CV_R2_Std']

print(f"\n🥇 BEST MODEL WITH BOX-COX TRANSFORM (Based on RMSE): {best_model_boxcox}")
print(f"   🎯 Unbiased RMSE: ${best_rmse_boxcox:.2f} ± ${best_rmse_std_boxcox:.2f}")
print(f"   🎯 Unbiased MAE:  ${best_mae_boxcox:.2f} ± ${best_mae_std_boxcox:.2f}")
print(f"   📈 Unbiased R²:   {best_r2_boxcox:.4f} ± {best_r2_std_boxcox:.4f}")

# Calculate confidence intervals for RMSE (primary metric)
rmse_ci_lower_boxcox = best_rmse_boxcox - 1.96 * best_rmse_std_boxcox
rmse_ci_upper_boxcox = best_rmse_boxcox + 1.96 * best_rmse_std_boxcox
print(f"   📊 95% Confidence Interval (RMSE): [${rmse_ci_lower_boxcox:.2f}, ${rmse_ci_upper_boxcox:.2f}]")

# Performance assessment based on RMSE
print(f"\n📈 BOX-COX TRANSFORM PERFORMANCE ASSESSMENT:")
if best_rmse_boxcox <= 600:
    performance_level = "EXCELLENT"
    emoji = "🎉"
elif best_rmse_boxcox <= 800:
    performance_level = "GOOD"
    emoji = "✅"
elif best_rmse_boxcox <= 1000:
    performance_level = "ACCEPTABLE"
    emoji = "👍"
else:
    performance_level = "NEEDS IMPROVEMENT"
    emoji = "⚠️"

print(f"   {emoji} Performance Level: {performance_level}")
print(f"   💰 RMSE = ${best_rmse_boxcox:.2f} (Average prediction error)")
print(f"   💰 MAE = ${best_mae_boxcox:.2f} (Median prediction error)")

# Model ranking summary
print(f"\n📋 MODEL RANKING WITH BOX-COX TRANSFORM (by RMSE):")
for idx, row in nested_comparison_boxcox_df.iterrows():
    rank = nested_comparison_boxcox_df.index.get_loc(idx) + 1
    model = row['Model']
    rmse = row['BoxCox_CV_RMSE_Mean']
    rmse_std = row['BoxCox_CV_RMSE_Std']
    print(f"   {rank}. {model:<15}: RMSE = ${rmse:.2f} ± ${rmse_std:.2f}")

print(f"\n💡 BOX-COX TRANSFORMATION BENEFITS:")
print(f"   🔧 Optimal normalization: Lambda = {fitted_lambda:.4f} ({lambda_interpretation})")
print(f"   📊 Data-driven transformation: Maximum likelihood estimation")
print(f"   🎯 Flexible power transformation: Adapts to data distribution")
print(f"   📈 Evaluation clarity: Results back-transformed to original scale for interpretation")

# %%
# SECTION 11: FINAL MODEL TRAINING WITH BOX-COX TRANSFORMATION
# =============================================================================
print("\n🎯 FINAL MODEL TRAINING WITH BOX-COX TRANSFORMATION")
print("=" * 80)

def get_most_frequent_params(best_params_list):
    """Get the most frequently selected hyperparameters across CV folds"""
    all_param_names = set()
    for params in best_params_list:
        all_param_names.update(params.keys())

    final_params = {}
    for param_name in all_param_names:
        param_values = [params.get(param_name) for params in best_params_list if param_name in params]
        if param_values:
            most_common = Counter(param_values).most_common(1)[0][0]
            final_params[param_name] = most_common

    return final_params

# Get best hyperparameters for the winning model
best_model_results_boxcox = nested_cv_results_boxcox[best_model_boxcox]
best_params_final_boxcox = get_most_frequent_params(best_model_results_boxcox['best_params_per_fold'])

print(f"🏆 Training final {best_model_boxcox} model with Box-Cox transformation:")
print("📋 Final hyperparameters (most frequent across CV folds):")
for param, value in sorted(best_params_final_boxcox.items()):
    print(f"   {param:<25}: {value}")

# Create and train final model on Box-Cox transformed target
print(f"\n🚀 Training final {best_model_boxcox} model on Box-Cox transformed target...")
final_model_boxcox = base_models[best_model_boxcox].set_params(**best_params_final_boxcox)
final_model_boxcox.fit(X_train_full_scaled, y_train_full_boxcox)  # Train on Box-Cox transformed target

print(f"✅ Final {best_model_boxcox} model trained with Box-Cox transformation")
print(f"   📊 Training data: {X_train_full_scaled.shape[0]:,} samples")
print(f"   🔧 Features: {len(feature_columns)} variables")
print(f"   🎯 Expected RMSE: ${best_rmse_boxcox:.2f} ± ${best_rmse_std_boxcox:.2f}")
print(f"   🔄 Target: Box-Cox(income, λ={fitted_lambda:.4f}) -> back-transformed for evaluation")

# %%
# SECTION 12: TEST SET EVALUATION WITH BOX-COX TRANSFORMATION
# =============================================================================
print("\n🎯 TEST SET EVALUATION WITH BOX-COX TRANSFORMATION")
print("-" * 80)

# Make predictions on test set (Box-Cox scale)
y_pred_test_boxcox = final_model_boxcox.predict(X_test_scaled)

# Back-transform predictions to original scale
y_pred_test_original = inverse_boxcox_transform(y_pred_test_boxcox, fitted_lambda)

# Adjust for the positive shift if it was applied
if min_positive_shift > 0:
    y_pred_test_original = y_pred_test_original - min_positive_shift
    # Ensure no negative predictions
    y_pred_test_original = np.maximum(y_pred_test_original, 0)

# Calculate comprehensive test metrics on original scale
test_rmse_boxcox = np.sqrt(mean_squared_error(y_test, y_pred_test_original))
test_mae_boxcox = mean_absolute_error(y_test, y_pred_test_original)
test_r2_boxcox = r2_score(y_test, y_pred_test_original)

# Calculate Box-Cox scale MSE for comparison
test_mse_boxcox_scale = mean_squared_error(y_test_boxcox, y_pred_test_boxcox)

# Calculate MAPE for incomes > $100
valid_mask = y_test > 100
if valid_mask.sum() > 0:
    test_mape_boxcox = np.mean(np.abs((y_test[valid_mask] - y_pred_test_original[valid_mask]) / y_test[valid_mask])) * 100
else:
    test_mape_boxcox = np.nan

print(f"🏆 FINAL TEST PERFORMANCE WITH BOX-COX TRANSFORM ({best_model_boxcox}):")
print("=" * 80)
print(f"   🎯 Test RMSE: ${test_rmse_boxcox:.2f}")
print(f"   🎯 Test MAE:  ${test_mae_boxcox:.2f}")
print(f"   📈 Test R²:   {test_r2_boxcox:.4f}")
print(f"   🔄 Box-Cox MSE: {test_mse_boxcox_scale:.4f}")
if not np.isnan(test_mape_boxcox):
    print(f"   💰 Test MAPE (>$100): {test_mape_boxcox:.1f}%")

# Compare with nested CV estimates
print(f"\n📊 BOX-COX TRANSFORM: NESTED CV vs TEST SET COMPARISON:")
print("-" * 80)
print(f"   Metric    | Nested CV Estimate | Test Set | Difference")
print(f"   ----------|-------------------|----------|----------")
print(f"   RMSE      | ${best_rmse_boxcox:.2f} ± ${best_rmse_std_boxcox:.2f}     | ${test_rmse_boxcox:.2f}     | ${abs(test_rmse_boxcox - best_rmse_boxcox):.2f}")
print(f"   MAE       | ${best_mae_boxcox:.2f} ± ${best_mae_std_boxcox:.2f}     | ${test_mae_boxcox:.2f}     | ${abs(test_mae_boxcox - best_mae_boxcox):.2f}")
print(f"   R²        | {best_r2_boxcox:.4f} ± {best_r2_std_boxcox:.4f} | {test_r2_boxcox:.4f}   | {abs(test_r2_boxcox - best_r2_boxcox):.4f}")

# Assess nested CV prediction accuracy
rmse_within_ci_boxcox = abs(test_rmse_boxcox - best_rmse_boxcox) <= 2 * best_rmse_std_boxcox
mae_within_ci_boxcox = abs(test_mae_boxcox - best_mae_boxcox) <= 2 * best_mae_std_boxcox
r2_within_ci_boxcox = abs(test_r2_boxcox - best_r2_boxcox) <= 2 * best_r2_std_boxcox

print(f"\n✅ BOX-COX TRANSFORM NESTED CV VALIDATION:")
print(f"   RMSE within 95% CI: {'✅ YES' if rmse_within_ci_boxcox else '⚠️ NO'}")
print(f"   MAE within 95% CI:  {'✅ YES' if mae_within_ci_boxcox else '⚠️ NO'}")
print(f"   R² within 95% CI:   {'✅ YES' if r2_within_ci_boxcox else '⚠️ NO'}")

if rmse_within_ci_boxcox and mae_within_ci_boxcox:
    print("   🎉 EXCELLENT: Box-Cox transform nested CV provided accurate performance estimates!")
elif rmse_within_ci_boxcox or mae_within_ci_boxcox:
    print("   👍 GOOD: Box-Cox transform nested CV estimates reasonably accurate")
else:
    print("   ⚠️ WARNING: Test performance differs significantly from nested CV estimates")

# Target distribution comparison
print(f"\n📈 TARGET DISTRIBUTION COMPARISON (BOX-COX TRANSFORM):")
print(f"   Training Full - Mean: ${y_train_full.mean():,.2f}, Std: ${y_train_full.std():,.2f}")
print(f"   Test Set      - Mean: ${y_test.mean():,.2f}, Std: ${y_test.std():,.2f}")
print(f"   Predictions   - Mean: ${y_pred_test_original.mean():,.2f}, Std: ${y_pred_test_original.std():,.2f}")

print(f"\n🔄 BOX-COX TRANSFORMATION SUMMARY:")
print(f"   📊 Training target skewness: {y_train_full.skew():.3f} -> {stats.skew(y_train_full_boxcox):.3f}")
print(f"   🔧 Optimal lambda: {fitted_lambda:.4f} ({lambda_interpretation})")
print(f"   ✅ Transformation effectiveness: {'Improved' if abs(stats.skew(y_train_full_boxcox)) < abs(y_train_full.skew()) else 'No improvement'}")
print(f"   🎯 Final model optimized on Box-Cox scale, evaluated on original scale")
print(f"   📈 Expected benefits: Optimal normalization, better statistical properties")

# %%
# SECTION 13: PERMUTATION IMPORTANCE ANALYSIS WITH BOX-COX TRANSFORMATION
# =============================================================================
print("\n🔍 PERMUTATION IMPORTANCE ANALYSIS WITH BOX-COX TRANSFORMATION")
print("-" * 80)

print("🔄 Computing permutation importance on test set...")
print("   ⚠️ This may take a few minutes...")

# Calculate permutation importance on test set (using original scale RMSE)
# We'll use a custom scorer that back-transforms predictions for fair comparison
def boxcox_rmse_scorer(estimator, X, y_true_original):
    """Custom scorer that predicts on Box-Cox scale but evaluates on original scale"""
    # Predict on Box-Cox scale
    y_pred_boxcox = estimator.predict(X)

    # Back-transform to original scale
    y_pred_original = inverse_boxcox_transform(y_pred_boxcox, fitted_lambda)

    # Adjust for positive shift if applied
    if min_positive_shift > 0:
        y_pred_original = y_pred_original - min_positive_shift
        y_pred_original = np.maximum(y_pred_original, 0)

    # Calculate RMSE on original scale (negative because sklearn expects higher = better)
    rmse = np.sqrt(mean_squared_error(y_true_original, y_pred_original))
    return -rmse  # Negative because permutation_importance expects higher = better

# Compute permutation importance
perm_importance_boxcox = permutation_importance(
    final_model_boxcox,
    X_test_scaled,
    y_test,  # Original scale target for evaluation
    scoring=boxcox_rmse_scorer,
    n_repeats=10,
    random_state=42,
    n_jobs=-1
)

# Create importance DataFrame
importance_df_boxcox = pd.DataFrame({
    'feature': feature_columns,
    'importance_mean': perm_importance_boxcox.importances_mean,
    'importance_std': perm_importance_boxcox.importances_std
}).sort_values('importance_mean', ascending=True)

print("✅ Permutation importance analysis complete")

# Display top 10 most important features
print(f"\n🏆 TOP 10 MOST IMPORTANT FEATURES (Box-Cox Transform):")
print("-" * 70)
top_10_boxcox = importance_df_boxcox.tail(10)
for idx, row in top_10_boxcox.iterrows():
    feature = row['feature']
    importance = row['importance_mean']
    std = row['importance_std']
    print(f"   {feature:<35}: {importance:.4f} ± {std:.4f}")

# Create visualization
plt.figure(figsize=(12, 8))
top_features_boxcox = importance_df_boxcox.tail(10)
plt.barh(range(len(top_features_boxcox)), top_features_boxcox['importance_mean'],
         xerr=top_features_boxcox['importance_std'], capsize=5, alpha=0.7, color='lightcoral')
plt.yticks(range(len(top_features_boxcox)), top_features_boxcox['feature'])
plt.xlabel('MSE Increase When Feature Permuted')
plt.title(f'Permutation Importance - Top 10 Features\n({best_model_boxcox} Model with Box-Cox Transform)')
plt.grid(True, alpha=0.3)

# Add importance values as text
for i, (importance, std) in enumerate(zip(top_features_boxcox['importance_mean'],
                                         top_features_boxcox['importance_std'])):
    plt.text(importance + std + 0.001, i, f'{importance:.4f}',
             va='center', fontsize=9)

plt.tight_layout()
plt.savefig(data_path + '/boxcox_permutation_importance.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\n💾 Permutation importance plot saved to: boxcox_permutation_importance.png")

# Save complete results
print(f"\n💾 SAVING BOX-COX TRANSFORMATION RESULTS:")
print("-" * 60)

# Save nested CV results
nested_cv_results_df_boxcox = pd.DataFrame(comparison_data_boxcox)
nested_cv_results_df_boxcox.to_csv(data_path + '/nested_cv_boxcox_results.csv', index=False)

# Save permutation importance
importance_df_boxcox.to_csv(data_path + '/boxcox_permutation_importance.csv', index=False)

# Save final model
joblib.dump(final_model_boxcox, data_path + '/final_model_boxcox_transform.pkl')
joblib.dump(scaler, data_path + '/scaler_boxcox_transform.pkl')

# Save test predictions
test_predictions_boxcox = pd.DataFrame({
    'y_true': y_test,
    'y_pred': y_pred_test_original,
    'y_pred_boxcox_scale': y_pred_test_boxcox
})
test_predictions_boxcox.to_csv(data_path + '/test_predictions_boxcox.csv', index=False)

print(f"   ✅ Nested CV results: nested_cv_boxcox_results.csv")
print(f"   ✅ Permutation importance: boxcox_permutation_importance.csv")
print(f"   ✅ Final model: final_model_boxcox_transform.pkl")
print(f"   ✅ Scaler: scaler_boxcox_transform.pkl")
print(f"   ✅ Test predictions: test_predictions_boxcox.csv")
print(f"   ✅ Transformation info: boxcox_transformation_info.json")

print(f"\n🎉 BOX-COX TRANSFORMATION PIPELINE COMPLETE!")
print("=" * 80)
print(f"🏆 Best Model: {best_model_boxcox}")
print(f"🎯 Final Test RMSE: ${test_rmse_boxcox:.2f}")
print(f"🔧 Box-Cox Lambda: {fitted_lambda:.4f} ({lambda_interpretation})")
print(f"📊 Transformation improved skewness: {y_train_full.skew():.3f} -> {stats.skew(y_train_full_boxcox):.3f}")
print(f"✅ All results saved to: {data_path}")
print("🚀 Ready for production deployment!")
