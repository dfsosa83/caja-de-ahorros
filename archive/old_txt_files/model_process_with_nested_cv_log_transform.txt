# %%
# =============================================================================
# INCOME PREDICTION MODEL - NESTED CV WITH LOG TRANSFORMATION
# =============================================================================
# Goal: Predict customer income using NESTED CV with LOG TRANSFORMATION for better performance
# Based on: model_process_with_nested_cv.txt (successful baseline)
# 
# Key Features:
# - Log Transformation: log(income + 1) to handle skewed income distribution
# - Nested CV Structure: Outer CV (5 folds) for evaluation, Inner CV (3 folds) for hyperparameter tuning
# - Robust Metrics: RMSE/MAE on original scale for interpretability
# - Complete Pipeline: From preprocessed data to production-ready model
# - Comprehensive Analysis: Permutation importance, visualizations, model comparison
# 
# Models: XGBoost, LightGBM, Random Forest
# Target: log(ingresos_reportados + 1) -> back-transformed for evaluation
# =============================================================================

# %%
# SECTION 1: IMPORTS AND SETUP
# =============================================================================
import pandas as pd
import numpy as np
import warnings
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import (KFold, GroupKFold, cross_val_score, 
                                   RandomizedSearchCV)
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.inspection import permutation_importance
import xgboost as xgb
import lightgbm as lgb
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
from collections import Counter

warnings.filterwarnings('ignore')

print("================================================================================")
print("NESTED CROSS-VALIDATION WITH LOG TRANSFORMATION - INCOME PREDICTION MODEL")
print("================================================================================")
print("📋 Pipeline based on: model_process_with_nested_cv.txt")
print("🔄 Target Transformation: log(income + 1) for better distribution handling")
print("🎯 Primary Metrics: RMSE/MAE on original scale (back-transformed)")
print("🔄 Nested CV: Outer (5-fold) for evaluation, Inner (3-fold) for hyperparameter tuning")

# %%
# SECTION 2: PREPARE FINAL DATASETS FOR NESTED CV MODELING
# =============================================================================
print("\n🎯 PREPARING FINAL DATASETS FOR NESTED CV WITH LOG TRANSFORMATION")
print("-" * 70)

# EXPLICIT FEATURE SELECTION APPROACH (from original pipeline)
id_columns = ['cliente', 'identificador_unico']
target_column = 'ingresos_reportados'

# Use the selected features from the original pipeline
# NOTE: Adjust 'selected_features_final' to your actual feature list variable
feature_columns = selected_features_final

# Verify all selected features exist in the dataset
available_features = []
missing_features = []

for feature in feature_columns:
    if feature in train_df_enhanced.columns:
        available_features.append(feature)
    else:
        missing_features.append(feature)

if missing_features:
    print(f"⚠️  Missing features (will be skipped): {missing_features}")

feature_columns = available_features

print(f"   📊 Selected feature columns: {len(feature_columns)}")
print(f"   🎯 Target column: {target_column}")

# Create feature matrices and targets (from original pipeline structure)
X_train = train_df_enhanced[feature_columns].copy()
y_train = train_df_enhanced[target_column].copy()

X_valid = valid_df_enhanced[feature_columns].copy()
y_valid = valid_df_enhanced[target_column].copy()

X_test = test_df_enhanced[feature_columns].copy()
y_test = test_df_enhanced[target_column].copy()

print(f"\n📈 DATASET SHAPES:")
print(f"   X_train: {X_train.shape}")
print(f"   X_valid: {X_valid.shape}")
print(f"   X_test: {X_test.shape}")

# %%
# SECTION 3: LOG TRANSFORMATION OF TARGET VARIABLE
# =============================================================================
print("\n🔄 LOG TRANSFORMATION OF TARGET VARIABLE")
print("-" * 60)

# Analyze original target distribution
print("📊 ORIGINAL TARGET DISTRIBUTION:")
print(f"   Mean: ${y_train.mean():,.2f}")
print(f"   Median: ${y_train.median():,.2f}")
print(f"   Std: ${y_train.std():,.2f}")
print(f"   Skewness: {y_train.skew():.3f}")
print(f"   Min: ${y_train.min():,.2f}")
print(f"   Max: ${y_train.max():,.2f}")

# Check for zero or negative values
zero_negative_count = (y_train <= 0).sum()
print(f"   Zero/negative values: {zero_negative_count} ({zero_negative_count/len(y_train)*100:.2f}%)")

# Apply log transformation: log(income + 1) to handle zeros
print(f"\n🔄 Applying log transformation: log(income + 1)")

# Transform all target variables
y_train_log = np.log1p(y_train)  # log1p = log(1 + x)
y_valid_log = np.log1p(y_valid)
y_test_log = np.log1p(y_test)

# Combine train and validation for nested CV (test set remains untouched)
X_train_full = pd.concat([X_train, X_valid], ignore_index=True)
y_train_full = pd.concat([y_train, y_valid], ignore_index=True)
y_train_full_log = np.log1p(y_train_full)

print(f"   X_train_full (for nested CV): {X_train_full.shape}")
print(f"   X_test (held out): {X_test.shape}")

# Analyze transformed target distribution
print(f"\n📊 LOG-TRANSFORMED TARGET DISTRIBUTION:")
print(f"   Mean: {y_train_log.mean():.3f}")
print(f"   Median: {y_train_log.median():.3f}")
print(f"   Std: {y_train_log.std():.3f}")
print(f"   Skewness: {y_train_log.skew():.3f}")
print(f"   Min: {y_train_log.min():.3f}")
print(f"   Max: {y_train_log.max():.3f}")

# Calculate improvement in skewness
skew_improvement = abs(y_train.skew()) - abs(y_train_log.skew())
print(f"\n✅ TRANSFORMATION EFFECTIVENESS:")
print(f"   Original skewness: {y_train.skew():.3f}")
print(f"   Log-transformed skewness: {y_train_log.skew():.3f}")
print(f"   Skewness improvement: {skew_improvement:.3f} {'✅ Better' if skew_improvement > 0 else '⚠️ Worse'}")

# Show selected features grouped by type
print(f"\n📋 SELECTED FEATURES ({len(feature_columns)} features):")
print("-" * 60)

# Group features by type for better readability (from original pipeline)
basic_features = []
age_features = []
freq_features = []
interaction_features = []
other_features = []

for feature in feature_columns:
    if feature.startswith('age_group_'):
        age_features.append(feature)
    elif feature.endswith('_freq'):
        freq_features.append(feature)
    elif '_x_' in feature or 'retired_x_' in feature or 'employer_x_' in feature or 'gender_x_' in feature:
        interaction_features.append(feature)
    elif feature in ['edad', 'letras_mensuales', 'monto_letra', 'saldo', 'is_retired']:
        basic_features.append(feature)
    else:
        other_features.append(feature)

print(f"🔢 BASIC FEATURES ({len(basic_features)}): {basic_features}")
print(f"👥 AGE GROUP FEATURES ({len(age_features)}): {age_features}")
print(f"📊 FREQUENCY FEATURES ({len(freq_features)}): {freq_features}")
print(f"⚡ INTERACTION FEATURES ({len(interaction_features)}): {interaction_features}")
print(f"🔧 OTHER FEATURES ({len(other_features)}): {other_features}")

# Verify data quality
print(f"\n✅ DATA QUALITY CHECKS:")
print(f"   Missing values in X_train_full: {X_train_full.isnull().sum().sum()}")
print(f"   Missing values in y_train_full_log: {np.isnan(y_train_full_log).sum()}")
print(f"   All features numeric: {all(X_train_full.dtypes.apply(lambda x: x in ['int64', 'float64']))}")

# Save feature list to file for reference
feature_list_df = pd.DataFrame({
    'feature_name': feature_columns,
    'feature_type': ['basic' if f in basic_features else
                    'age_group' if f in age_features else
                    'frequency' if f in freq_features else
                    'interaction' if f in interaction_features else
                    'other' for f in feature_columns]
})

feature_list_df.to_csv(data_path + '/nested_cv_log_transform_feature_list.csv', index=False)
print(f"\n💾 Feature list saved to: nested_cv_log_transform_feature_list.csv")

# %%
# SECTION 4: FEATURE SCALING
# =============================================================================
print("\n⚖️ FEATURE SCALING")
print("-" * 50)

# Apply robust scaling to features (from original pipeline)
print("   ⚖️ Applying RobustScaler...")
scaler = RobustScaler()

# Fit scaler on full training data (train + validation combined)
X_train_full_scaled = pd.DataFrame(
    scaler.fit_transform(X_train_full),
    columns=X_train_full.columns,
    index=X_train_full.index
)

# Transform test set using the same scaler
X_test_scaled = pd.DataFrame(
    scaler.transform(X_test),
    columns=X_test.columns,
    index=X_test.index
)

print("   ✅ Feature scaling complete")
print(f"   📊 Scaled training data: {X_train_full_scaled.shape}")
print(f"   📊 Scaled test data: {X_test_scaled.shape}")

# %%
# SECTION 5: VISUALIZATION OF TARGET TRANSFORMATION
# =============================================================================
print("\n📈 CREATING TARGET TRANSFORMATION VISUALIZATION")
print("-" * 60)

# Create comparison plots
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Target Variable Transformation Analysis', fontsize=16, fontweight='bold')

# 1. Original distribution
ax1 = axes[0, 0]
ax1.hist(y_train_full, bins=50, alpha=0.7, color='lightblue', edgecolor='black')
ax1.set_xlabel('Income ($)')
ax1.set_ylabel('Frequency')
ax1.set_title(f'Original Income Distribution\nSkewness: {y_train_full.skew():.3f}')
ax1.grid(True, alpha=0.3)

# 2. Log-transformed distribution
ax2 = axes[0, 1]
ax2.hist(y_train_full_log, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')
ax2.set_xlabel('Log(Income + 1)')
ax2.set_ylabel('Frequency')
ax2.set_title(f'Log-Transformed Distribution\nSkewness: {y_train_full_log.skew():.3f}')
ax2.grid(True, alpha=0.3)

# 3. Q-Q plot for original
from scipy import stats
ax3 = axes[1, 0]
stats.probplot(y_train_full, dist="norm", plot=ax3)
ax3.set_title('Q-Q Plot: Original Income vs Normal')
ax3.grid(True, alpha=0.3)

# 4. Q-Q plot for log-transformed
ax4 = axes[1, 1]
stats.probplot(y_train_full_log, dist="norm", plot=ax4)
ax4.set_title('Q-Q Plot: Log-Transformed vs Normal')
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(data_path + '/target_log_transformation_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print("   ✅ Target transformation visualization saved to: target_log_transformation_analysis.png")

print(f"\n💡 TRANSFORMATION INSIGHTS:")
print(f"   📊 Original distribution: Highly skewed ({y_train_full.skew():.3f})")
print(f"   📊 Log-transformed: More normal ({y_train_full_log.skew():.3f})")
print(f"   ✅ Expected benefits: Better model performance, more stable predictions")
print(f"   🎯 Evaluation: All metrics will be back-transformed to original scale")

# %%
# SECTION 6: NESTED CROSS-VALIDATION SETUP WITH LOG TRANSFORMATION
# =============================================================================
print("\n🔄 NESTED CROSS-VALIDATION SETUP WITH LOG TRANSFORMATION")
print("-" * 70)

# Define CV strategies with robust metrics focus
OUTER_CV_FOLDS = 5  # For unbiased performance estimation
INNER_CV_FOLDS = 3  # For hyperparameter tuning
RANDOM_SEARCH_ITERATIONS = 15  # Increased for better hyperparameter search

# Create CV objects
outer_cv = KFold(n_splits=OUTER_CV_FOLDS, shuffle=True, random_state=42)
inner_cv = KFold(n_splits=INNER_CV_FOLDS, shuffle=True, random_state=42)

print(f"   🔄 Outer CV: {OUTER_CV_FOLDS} folds (for unbiased model evaluation)")
print(f"   🔄 Inner CV: {INNER_CV_FOLDS} folds (for hyperparameter tuning)")
print(f"   🔍 Random Search: {RANDOM_SEARCH_ITERATIONS} iterations per inner fold")
print(f"   📊 Total model trainings: {OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS} per model")
print(f"      ({OUTER_CV_FOLDS} outer × {INNER_CV_FOLDS} inner × {RANDOM_SEARCH_ITERATIONS} iterations)")

# %%
# SECTION 7: MODEL DEFINITIONS AND HYPERPARAMETER GRIDS
# =============================================================================
print("\n🤖 MODEL DEFINITIONS AND HYPERPARAMETER GRIDS")
print("-" * 60)

# Base models (from original pipeline with robust configurations)
base_models = {
    'XGBoost': xgb.XGBRegressor(random_state=42, n_jobs=-1),
    'LightGBM': lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),
    'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1)
}

# Comprehensive hyperparameter grids for nested CV
# These grids are designed for log-transformed income prediction optimization
param_grids = {
    'XGBoost': {
        'n_estimators': [300, 500, 700],
        'max_depth': [6, 8, 10],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.8, 0.85, 0.9],
        'colsample_bytree': [0.8, 0.85, 0.9],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [0.5, 1.0, 2.0],
        'min_child_weight': [1, 3, 5]
    },
    'LightGBM': {
        'n_estimators': [300, 500, 700],
        'max_depth': [8, 12, 16],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.8, 0.85, 0.9],
        'colsample_bytree': [0.8, 0.85, 0.9],
        'num_leaves': [50, 100, 150],
        'min_child_samples': [10, 20, 30],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [0.5, 1.0, 2.0]
    },
    'Random Forest': {
        'n_estimators': [200, 300, 400],
        'max_depth': [10, 15, 20, None],
        'min_samples_split': [5, 10, 15],
        'min_samples_leaf': [2, 5, 10],
        'max_features': ['sqrt', 'log2', 0.8],
        'max_samples': [0.7, 0.8, 0.9]
    }
}

print(f"   🤖 Models defined: {list(base_models.keys())}")
print(f"   🔧 Hyperparameter search space per model:")
for model_name, grid in param_grids.items():
    total_combinations = np.prod([len(values) for values in grid.values()])
    print(f"      {model_name}: {total_combinations:,} total combinations")

print(f"\n💡 Primary Optimization Metric: MSE on log-transformed target")
print(f"   📊 MSE optimized on log scale for better distribution handling")
print(f"   💰 Final evaluation on original scale (back-transformed)")
print(f"   📈 Lower MSE on log scale = Better model performance")

# %%
# SECTION 8: NESTED CROSS-VALIDATION IMPLEMENTATION WITH LOG TRANSFORMATION
# =============================================================================
print("\n🎯 NESTED CROSS-VALIDATION IMPLEMENTATION WITH LOG TRANSFORMATION")
print("=" * 80)

def nested_cross_validation_log_transform(model, param_grid, X, y_log, y_original, outer_cv, inner_cv, model_name):
    """
    Perform nested cross-validation for log-transformed target with back-transformation for evaluation

    This implementation optimizes on log-transformed target but evaluates on original scale
    for interpretable results.

    Args:
        model: Base model to evaluate
        param_grid: Hyperparameter grid for tuning
        X, y_log: Training data and log-transformed target
        y_original: Original target for back-transformed evaluation
        outer_cv, inner_cv: Cross-validation objects
        model_name: Name for reporting

    Returns:
        dict: Comprehensive nested CV results with back-transformed metrics
    """
    print(f"\n🔄 Starting Nested CV with Log Transform for {model_name}")
    print(f"   📊 Outer folds: {outer_cv.n_splits}, Inner folds: {inner_cv.n_splits}")
    print(f"   🎯 Optimization: MSE on log-transformed target")
    print(f"   📈 Evaluation: RMSE/MAE on original scale (back-transformed)")

    # Storage for results - back-transformed metrics
    outer_scores_rmse = []
    outer_scores_mae = []
    outer_scores_r2 = []
    outer_scores_log_mse = []  # Track log-scale performance too
    best_params_per_fold = []
    fold_details = []

    # Outer CV loop - each iteration gives unbiased performance estimate
    for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X, y_log), 1):
        print(f"\n   🔄 Outer Fold {fold_idx}/{outer_cv.n_splits}")

        # Split data for this outer fold
        X_train_outer = X.iloc[train_idx]
        X_val_outer = X.iloc[val_idx]
        y_train_outer_log = y_log.iloc[train_idx]
        y_val_outer_log = y_log.iloc[val_idx]
        y_val_outer_original = y_original.iloc[val_idx]  # For back-transformed evaluation

        print(f"      📊 Fold {fold_idx} sizes: Train={len(train_idx)}, Val={len(val_idx)}")

        # Inner CV: Hyperparameter tuning using MSE on log-transformed target
        print(f"      🔧 Inner CV: Hyperparameter tuning (optimizing log-scale MSE)...")

        random_search = RandomizedSearchCV(
            estimator=model,
            param_distributions=param_grid,
            n_iter=RANDOM_SEARCH_ITERATIONS,
            cv=inner_cv,
            scoring='neg_mean_squared_error',  # Optimizing MSE on log scale
            n_jobs=-1,
            random_state=42,
            verbose=0
        )

        # Fit hyperparameter search on outer training data (log-transformed)
        random_search.fit(X_train_outer, y_train_outer_log)

        # Get best model from inner CV
        best_model = random_search.best_estimator_
        best_params = random_search.best_params_
        best_params_per_fold.append(best_params)

        # Convert negative MSE back to MSE for reporting
        best_inner_mse_log = -random_search.best_score_
        print(f"      ✅ Best inner CV MSE (log scale): {best_inner_mse_log:.4f}")

        # Evaluate best model on outer validation fold
        y_pred_outer_log = best_model.predict(X_val_outer)

        # Back-transform predictions to original scale
        y_pred_outer_original = np.expm1(y_pred_outer_log)  # expm1 = exp(x) - 1

        # Calculate comprehensive metrics on original scale
        fold_rmse = np.sqrt(mean_squared_error(y_val_outer_original, y_pred_outer_original))
        fold_mae = mean_absolute_error(y_val_outer_original, y_pred_outer_original)
        fold_r2 = r2_score(y_val_outer_original, y_pred_outer_original)
        fold_log_mse = mean_squared_error(y_val_outer_log, y_pred_outer_log)  # Log scale MSE

        # Store scores
        outer_scores_rmse.append(fold_rmse)
        outer_scores_mae.append(fold_mae)
        outer_scores_r2.append(fold_r2)
        outer_scores_log_mse.append(fold_log_mse)

        print(f"      📊 Outer fold performance (original scale):")
        print(f"         🎯 RMSE: ${fold_rmse:.2f}")
        print(f"         🎯 MAE:  ${fold_mae:.2f}")
        print(f"         📈 R²:   {fold_r2:.4f}")
        print(f"         🔄 Log MSE: {fold_log_mse:.4f}")

        # Calculate additional income-specific metrics
        # MAPE for incomes > $100 (avoid division by very small numbers)
        valid_mask = y_val_outer_original > 100
        if valid_mask.sum() > 0:
            mape = np.mean(np.abs((y_val_outer_original[valid_mask] - y_pred_outer_original[valid_mask]) / y_val_outer_original[valid_mask])) * 100
            print(f"         💰 MAPE (>$100): {mape:.1f}%")
        else:
            mape = np.nan

        # Store detailed results for this fold
        fold_details.append({
            'fold': fold_idx,
            'rmse': fold_rmse,
            'mae': fold_mae,
            'r2': fold_r2,
            'log_mse': fold_log_mse,
            'mape': mape,
            'best_params': best_params,
            'inner_cv_log_mse': best_inner_mse_log,
            'train_size': len(train_idx),
            'val_size': len(val_idx),
            'y_true_mean': y_val_outer_original.mean(),
            'y_pred_mean': y_pred_outer_original.mean()
        })

    # Calculate final nested CV results with back-transformed metrics
    nested_cv_results = {
        'model_name': model_name,
        # Primary metrics (RMSE/MAE on original scale)
        'outer_cv_rmse_mean': np.mean(outer_scores_rmse),
        'outer_cv_rmse_std': np.std(outer_scores_rmse),
        'outer_cv_mae_mean': np.mean(outer_scores_mae),
        'outer_cv_mae_std': np.std(outer_scores_mae),
        # Secondary metric (R² on original scale)
        'outer_cv_r2_mean': np.mean(outer_scores_r2),
        'outer_cv_r2_std': np.std(outer_scores_r2),
        # Log-scale performance tracking
        'outer_cv_log_mse_mean': np.mean(outer_scores_log_mse),
        'outer_cv_log_mse_std': np.std(outer_scores_log_mse),
        # Raw scores for analysis
        'outer_scores_rmse': outer_scores_rmse,
        'outer_scores_mae': outer_scores_mae,
        'outer_scores_r2': outer_scores_r2,
        'outer_scores_log_mse': outer_scores_log_mse,
        # Hyperparameter analysis
        'best_params_per_fold': best_params_per_fold,
        'fold_details': fold_details,
        # Model selection criteria (lower is better for RMSE/MAE)
        'selection_metric': 'rmse',
        'selection_score': np.mean(outer_scores_rmse),
        # Transformation info
        'transformation': 'log_transform',
        'target_transform_function': 'log1p',
        'inverse_transform_function': 'expm1'
    }

    print(f"\n   🏆 {model_name} Nested CV Summary (Log Transform):")
    print(f"      🎯 RMSE (original): ${nested_cv_results['outer_cv_rmse_mean']:.2f} ± ${nested_cv_results['outer_cv_rmse_std']:.2f}")
    print(f"      🎯 MAE (original):  ${nested_cv_results['outer_cv_mae_mean']:.2f} ± ${nested_cv_results['outer_cv_mae_std']:.2f}")
    print(f"      📈 R² (original):   {nested_cv_results['outer_cv_r2_mean']:.4f} ± {nested_cv_results['outer_cv_r2_std']:.4f}")
    print(f"      🔄 Log MSE:         {nested_cv_results['outer_cv_log_mse_mean']:.4f} ± {nested_cv_results['outer_cv_log_mse_std']:.4f}")

    return nested_cv_results

print("✅ Nested CV function with log transformation defined")

# %%
# SECTION 9: RUN NESTED CROSS-VALIDATION WITH LOG TRANSFORMATION
# =============================================================================
print("\n🚀 RUNNING NESTED CROSS-VALIDATION WITH LOG TRANSFORMATION")
print("=" * 80)
print("⚠️  This will take 20-60 minutes depending on your hardware...")
print(f"   Each model will be trained {OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS} times")
print(f"   ({OUTER_CV_FOLDS} outer × {INNER_CV_FOLDS} inner × {RANDOM_SEARCH_ITERATIONS} iterations)")
print("🎯 Optimizing for MSE on log-transformed target, evaluating on original scale")

# Storage for all nested CV results
nested_cv_results_log = {}
total_start_time = datetime.now()

# Run nested CV for each model with log transformation
for model_idx, (model_name, base_model) in enumerate(base_models.items(), 1):
    print(f"\n{'='*90}")
    print(f"NESTED CV WITH LOG TRANSFORM {model_idx}/{len(base_models)}: {model_name.upper()}")
    print(f"{'='*90}")

    model_start_time = datetime.now()

    # Run nested cross-validation with log transformation
    results = nested_cross_validation_log_transform(
        model=base_model,
        param_grid=param_grids[model_name],
        X=X_train_full_scaled,
        y_log=y_train_full_log,  # Log-transformed target for optimization
        y_original=y_train_full,  # Original target for evaluation
        outer_cv=outer_cv,
        inner_cv=inner_cv,
        model_name=model_name
    )

    nested_cv_results_log[model_name] = results

    model_end_time = datetime.now()
    model_duration = (model_end_time - model_start_time).total_seconds() / 60

    print(f"\n   ⏱️ {model_name} completed in {model_duration:.1f} minutes")

    # Show progress
    remaining_models = len(base_models) - model_idx
    if remaining_models > 0:
        estimated_remaining = model_duration * remaining_models
        print(f"   📊 Progress: {model_idx}/{len(base_models)} models complete")
        print(f"   ⏰ Estimated remaining time: {estimated_remaining:.1f} minutes")

total_end_time = datetime.now()
total_duration = (total_end_time - total_start_time).total_seconds() / 60

print(f"\n🎉 ALL NESTED CV WITH LOG TRANSFORMATION COMPLETED!")
print(f"⏱️ Total execution time: {total_duration:.1f} minutes")

# %%
# SECTION 10: LOG TRANSFORM RESULTS ANALYSIS AND MODEL COMPARISON
# =============================================================================
print("\n📊 LOG TRANSFORM NESTED CV RESULTS ANALYSIS")
print("=" * 80)

# Create comprehensive comparison DataFrame with robust metrics focus
comparison_data_log = []
for model_name, results in nested_cv_results_log.items():
    comparison_data_log.append({
        'Model': model_name,
        # Primary metrics (lower is better) - on original scale
        'Log_CV_RMSE_Mean': results['outer_cv_rmse_mean'],
        'Log_CV_RMSE_Std': results['outer_cv_rmse_std'],
        'Log_CV_MAE_Mean': results['outer_cv_mae_mean'],
        'Log_CV_MAE_Std': results['outer_cv_mae_std'],
        # Secondary metric - on original scale
        'Log_CV_R2_Mean': results['outer_cv_r2_mean'],
        'Log_CV_R2_Std': results['outer_cv_r2_std'],
        # Log scale performance
        'Log_Scale_MSE_Mean': results['outer_cv_log_mse_mean'],
        'Log_Scale_MSE_Std': results['outer_cv_log_mse_std'],
        # Selection score (RMSE for ranking)
        'Selection_Score': results['selection_score']
    })

nested_comparison_log_df = pd.DataFrame(comparison_data_log)
# Sort by RMSE (lower is better) - primary metric for income prediction
nested_comparison_log_df = nested_comparison_log_df.sort_values('Log_CV_RMSE_Mean', ascending=True)

print("\n🏆 LOG TRANSFORM NESTED CV PERFORMANCE COMPARISON (Sorted by RMSE - Lower is Better):")
print("=" * 100)
print(nested_comparison_log_df.round(4).to_string(index=False))

# Identify best model based on RMSE (robust metric for income prediction)
best_model_log = nested_comparison_log_df.iloc[0]['Model']
best_rmse_log = nested_comparison_log_df.iloc[0]['Log_CV_RMSE_Mean']
best_rmse_std_log = nested_comparison_log_df.iloc[0]['Log_CV_RMSE_Std']
best_mae_log = nested_comparison_log_df.iloc[0]['Log_CV_MAE_Mean']
best_mae_std_log = nested_comparison_log_df.iloc[0]['Log_CV_MAE_Std']
best_r2_log = nested_comparison_log_df.iloc[0]['Log_CV_R2_Mean']
best_r2_std_log = nested_comparison_log_df.iloc[0]['Log_CV_R2_Std']

print(f"\n🥇 BEST MODEL WITH LOG TRANSFORM (Based on RMSE): {best_model_log}")
print(f"   🎯 Unbiased RMSE: ${best_rmse_log:.2f} ± ${best_rmse_std_log:.2f}")
print(f"   🎯 Unbiased MAE:  ${best_mae_log:.2f} ± ${best_mae_std_log:.2f}")
print(f"   📈 Unbiased R²:   {best_r2_log:.4f} ± {best_r2_std_log:.4f}")

# Calculate confidence intervals for RMSE (primary metric)
rmse_ci_lower_log = best_rmse_log - 1.96 * best_rmse_std_log
rmse_ci_upper_log = best_rmse_log + 1.96 * best_rmse_std_log
print(f"   📊 95% Confidence Interval (RMSE): [${rmse_ci_lower_log:.2f}, ${rmse_ci_upper_log:.2f}]")

# Performance assessment based on RMSE
print(f"\n📈 LOG TRANSFORM PERFORMANCE ASSESSMENT:")
if best_rmse_log <= 600:
    performance_level = "EXCELLENT"
    emoji = "🎉"
elif best_rmse_log <= 800:
    performance_level = "GOOD"
    emoji = "✅"
elif best_rmse_log <= 1000:
    performance_level = "ACCEPTABLE"
    emoji = "👍"
else:
    performance_level = "NEEDS IMPROVEMENT"
    emoji = "⚠️"

print(f"   {emoji} Performance Level: {performance_level}")
print(f"   💰 RMSE = ${best_rmse_log:.2f} (Average prediction error)")
print(f"   💰 MAE = ${best_mae_log:.2f} (Median prediction error)")

# Model ranking summary
print(f"\n📋 MODEL RANKING WITH LOG TRANSFORM (by RMSE):")
for idx, row in nested_comparison_log_df.iterrows():
    rank = nested_comparison_log_df.index.get_loc(idx) + 1
    model = row['Model']
    rmse = row['Log_CV_RMSE_Mean']
    rmse_std = row['Log_CV_RMSE_Std']
    print(f"   {rank}. {model:<15}: RMSE = ${rmse:.2f} ± ${rmse_std:.2f}")

print(f"\n💡 LOG TRANSFORMATION BENEFITS:")
print(f"   🔄 Target normalization: Reduces skewness for better model training")
print(f"   📊 Stable predictions: Less sensitive to extreme income values")
print(f"   🎯 Better optimization: Models work better with normalized distributions")
print(f"   📈 Evaluation clarity: Results back-transformed to original scale for interpretation")

# %%
# SECTION 11: FINAL MODEL TRAINING WITH LOG TRANSFORMATION
# =============================================================================
print("\n🎯 FINAL MODEL TRAINING WITH LOG TRANSFORMATION")
print("=" * 70)

def get_most_frequent_params(best_params_list):
    """Get the most frequently selected hyperparameters across CV folds"""
    all_param_names = set()
    for params in best_params_list:
        all_param_names.update(params.keys())

    final_params = {}
    for param_name in all_param_names:
        param_values = [params.get(param_name) for params in best_params_list if param_name in params]
        if param_values:
            most_common = Counter(param_values).most_common(1)[0][0]
            final_params[param_name] = most_common

    return final_params

# Get best hyperparameters for the winning model
best_model_results_log = nested_cv_results_log[best_model_log]
best_params_final_log = get_most_frequent_params(best_model_results_log['best_params_per_fold'])

print(f"🏆 Training final {best_model_log} model with log transformation:")
print("📋 Final hyperparameters (most frequent across CV folds):")
for param, value in sorted(best_params_final_log.items()):
    print(f"   {param:<25}: {value}")

# Create and train final model on log-transformed target
print(f"\n🚀 Training final {best_model_log} model on log-transformed target...")
final_model_log = base_models[best_model_log].set_params(**best_params_final_log)
final_model_log.fit(X_train_full_scaled, y_train_full_log)  # Train on log-transformed target

print(f"✅ Final {best_model_log} model trained with log transformation")
print(f"   📊 Training data: {X_train_full_scaled.shape[0]:,} samples")
print(f"   🔧 Features: {len(feature_columns)} variables")
print(f"   🎯 Expected RMSE: ${best_rmse_log:.2f} ± ${best_rmse_std_log:.2f}")
print(f"   🔄 Target: log(income + 1) -> back-transformed for evaluation")

# %%
# SECTION 12: TEST SET EVALUATION WITH LOG TRANSFORMATION
# =============================================================================
print("\n🎯 TEST SET EVALUATION WITH LOG TRANSFORMATION")
print("-" * 70)

# Make predictions on test set (log scale)
y_pred_test_log = final_model_log.predict(X_test_scaled)

# Back-transform predictions to original scale
y_pred_test_original = np.expm1(y_pred_test_log)  # expm1 = exp(x) - 1

# Calculate comprehensive test metrics on original scale
test_rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_test_original))
test_mae_log = mean_absolute_error(y_test, y_pred_test_original)
test_r2_log = r2_score(y_test, y_pred_test_original)

# Calculate log-scale MSE for comparison
test_mse_log_scale = mean_squared_error(y_test_log, y_pred_test_log)

# Calculate MAPE for incomes > $100
valid_mask = y_test > 100
if valid_mask.sum() > 0:
    test_mape_log = np.mean(np.abs((y_test[valid_mask] - y_pred_test_original[valid_mask]) / y_test[valid_mask])) * 100
else:
    test_mape_log = np.nan

print(f"🏆 FINAL TEST PERFORMANCE WITH LOG TRANSFORM ({best_model_log}):")
print("=" * 70)
print(f"   🎯 Test RMSE: ${test_rmse_log:.2f}")
print(f"   🎯 Test MAE:  ${test_mae_log:.2f}")
print(f"   📈 Test R²:   {test_r2_log:.4f}")
print(f"   🔄 Log MSE:   {test_mse_log_scale:.4f}")
if not np.isnan(test_mape_log):
    print(f"   💰 Test MAPE (>$100): {test_mape_log:.1f}%")

# Compare with nested CV estimates
print(f"\n📊 LOG TRANSFORM: NESTED CV vs TEST SET COMPARISON:")
print("-" * 70)
print(f"   Metric    | Nested CV Estimate | Test Set | Difference")
print(f"   ----------|-------------------|----------|----------")
print(f"   RMSE      | ${best_rmse_log:.2f} ± ${best_rmse_std_log:.2f}     | ${test_rmse_log:.2f}     | ${abs(test_rmse_log - best_rmse_log):.2f}")
print(f"   MAE       | ${best_mae_log:.2f} ± ${best_mae_std_log:.2f}     | ${test_mae_log:.2f}     | ${abs(test_mae_log - best_mae_log):.2f}")
print(f"   R²        | {best_r2_log:.4f} ± {best_r2_std_log:.4f} | {test_r2_log:.4f}   | {abs(test_r2_log - best_r2_log):.4f}")

# Assess nested CV prediction accuracy
rmse_within_ci_log = abs(test_rmse_log - best_rmse_log) <= 2 * best_rmse_std_log
mae_within_ci_log = abs(test_mae_log - best_mae_log) <= 2 * best_mae_std_log
r2_within_ci_log = abs(test_r2_log - best_r2_log) <= 2 * best_r2_std_log

print(f"\n✅ LOG TRANSFORM NESTED CV VALIDATION:")
print(f"   RMSE within 95% CI: {'✅ YES' if rmse_within_ci_log else '⚠️ NO'}")
print(f"   MAE within 95% CI:  {'✅ YES' if mae_within_ci_log else '⚠️ NO'}")
print(f"   R² within 95% CI:   {'✅ YES' if r2_within_ci_log else '⚠️ NO'}")

if rmse_within_ci_log and mae_within_ci_log:
    print("   🎉 EXCELLENT: Log transform nested CV provided accurate performance estimates!")
elif rmse_within_ci_log or mae_within_ci_log:
    print("   👍 GOOD: Log transform nested CV estimates reasonably accurate")
else:
    print("   ⚠️ WARNING: Test performance differs significantly from nested CV estimates")

# Target distribution comparison
print(f"\n📈 TARGET DISTRIBUTION COMPARISON (LOG TRANSFORM):")
print(f"   Training Full - Mean: ${y_train_full.mean():,.2f}, Std: ${y_train_full.std():,.2f}")
print(f"   Test Set      - Mean: ${y_test.mean():,.2f}, Std: ${y_test.std():,.2f}")
print(f"   Predictions   - Mean: ${y_pred_test_original.mean():,.2f}, Std: ${y_pred_test_original.std():,.2f}")

print(f"\n🔄 LOG TRANSFORMATION SUMMARY:")
print(f"   📊 Training target skewness: {y_train_full.skew():.3f} -> {y_train_full_log.skew():.3f}")
print(f"   ✅ Transformation effectiveness: {'Improved' if abs(y_train_full_log.skew()) < abs(y_train_full.skew()) else 'No improvement'}")
print(f"   🎯 Final model optimized on log scale, evaluated on original scale")
print(f"   📈 Expected benefits: Better handling of income distribution, more stable predictions")
