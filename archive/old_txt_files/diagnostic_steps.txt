# %%
# =============================================================================
# DIAGNOSTIC STEPS FOR MODEL PERFORMANCE ISSUES
# =============================================================================
print("\nğŸ” COMPREHENSIVE MODEL DIAGNOSTICS")
print("=" * 60)

# =============================================================================
# STEP 1: BASIC DATA VALIDATION
# =============================================================================
print("\nğŸ“Š STEP 1: Basic Data Validation")
print("-" * 40)

# Check if we're using the right dataset
print(f"ğŸ” Dataset being used: {type(df_original).__name__}")
print(f"ğŸ“ Original dataset shape: {df_original.shape}")

# Check if target variable exists and has reasonable values
if 'ingresos_reportados' in df_original.columns:
    target_stats = df_original['ingresos_reportados'].describe()
    print(f"\nğŸ’° Target variable (ingresos_reportados) statistics:")
    print(f"   Count: {target_stats['count']:,.0f}")
    print(f"   Mean: ${target_stats['mean']:,.2f}")
    print(f"   Median: ${target_stats['50%']:,.2f}")
    print(f"   Min: ${target_stats['min']:,.2f}")
    print(f"   Max: ${target_stats['max']:,.2f}")
    print(f"   Std: ${target_stats['std']:,.2f}")
    
    # Check for missing values in target
    missing_target = df_original['ingresos_reportados'].isnull().sum()
    print(f"   Missing values: {missing_target:,} ({missing_target/len(df_original)*100:.1f}%)")
else:
    print("âŒ ERROR: Target variable 'ingresos_reportados' not found!")

# =============================================================================
# STEP 2: FEATURE SELECTION VALIDATION
# =============================================================================
print(f"\nğŸ¯ STEP 2: Feature Selection Validation")
print("-" * 40)

print(f"ğŸ“ˆ Selected features count: {len(selected_features_final)}")
print(f"ğŸ² Noise features count: {len(noise_features)}")

# Check if critical features are included
critical_features = ['edad', 'saldo', 'monto_letra', 'ocupacion_consolidated_freq', 
                    'ciudad_consolidated_freq', 'letras_mensuales']

print(f"\nğŸ”‘ Critical features check:")
missing_critical = []
for feature in critical_features:
    if feature in selected_features_final:
        print(f"   âœ… {feature}: INCLUDED")
    else:
        print(f"   âŒ {feature}: MISSING")
        missing_critical.append(feature)

if missing_critical:
    print(f"\nâš ï¸  WARNING: {len(missing_critical)} critical features are missing!")
    print("   This could explain the poor performance.")

# Show top 10 selected features
print(f"\nğŸ“‹ Top 10 selected features:")
for i, feature in enumerate(selected_features_final[:10], 1):
    print(f"   {i:2d}. {feature}")

# =============================================================================
# STEP 3: DATA PIPELINE VALIDATION
# =============================================================================
print(f"\nğŸ”§ STEP 3: Data Pipeline Validation")
print("-" * 40)

# Check final dataset shapes
print(f"ğŸ“Š Final dataset shapes:")
print(f"   X_train_final: {X_train_final.shape if 'X_train_final' in locals() else 'NOT CREATED'}")
print(f"   X_valid_final: {X_valid_final.shape if 'X_valid_final' in locals() else 'NOT CREATED'}")
print(f"   X_test_final: {X_test_final.shape if 'X_test_final' in locals() else 'NOT CREATED'}")

# Check for NaN values
if 'X_train_final' in locals():
    nan_count = X_train_final.isnull().sum().sum()
    print(f"   NaN values in X_train_final: {nan_count}")
    
    if nan_count > 0:
        print("   âŒ WARNING: NaN values detected in training data!")
        nan_features = X_train_final.isnull().sum()
        nan_features = nan_features[nan_features > 0]
        print(f"   Features with NaN: {list(nan_features.index)}")

# Check target variable after cleaning
if 'y_train_cleaned' in locals():
    print(f"\nğŸ’° Cleaned target statistics:")
    print(f"   y_train_cleaned shape: {y_train_cleaned.shape}")
    print(f"   Mean: ${y_train_cleaned.mean():,.2f}")
    print(f"   Min: ${y_train_cleaned.min():,.2f}")
    print(f"   Max: ${y_train_cleaned.max():,.2f}")
    
    # Check if winsorization was too aggressive
    if 'train_df_enhanced' in locals() and 'ingresos_reportados' in train_df_enhanced.columns:
        original_target = train_df_enhanced['ingresos_reportados']
        winsorized_count = (original_target != y_train_cleaned).sum()
        print(f"   Winsorized values: {winsorized_count} ({winsorized_count/len(y_train_cleaned)*100:.1f}%)")

# =============================================================================
# STEP 4: MODEL VALIDATION
# =============================================================================
print(f"\nğŸ¤– STEP 4: Model Validation")
print("-" * 40)

# Check if models were trained properly
if 'best_model' in locals():
    print(f"âœ… Best model available: {type(best_model).__name__}")
    print(f"âœ… Best model name: {best_model_name if 'best_model_name' in locals() else 'Unknown'}")
else:
    print("âŒ ERROR: No best_model found!")

# Check prediction range
if 'test_predictions' in locals():
    print(f"\nğŸ“ˆ Prediction statistics:")
    print(f"   Shape: {test_predictions.shape}")
    print(f"   Mean: ${test_predictions.mean():,.2f}")
    print(f"   Min: ${test_predictions.min():,.2f}")
    print(f"   Max: ${test_predictions.max():,.2f}")
    print(f"   Std: ${test_predictions.std():,.2f}")
    
    # Check for unrealistic predictions
    if test_predictions.min() < 0:
        print("   âŒ WARNING: Negative predictions detected!")
    if test_predictions.max() > 50000:
        print("   âš ï¸  WARNING: Very high predictions detected!")
else:
    print("âŒ ERROR: No test_predictions found!")

# =============================================================================
# STEP 5: QUICK PERFORMANCE CHECK WITH BASELINE
# =============================================================================
print(f"\nâš¡ STEP 5: Quick Baseline Performance Check")
print("-" * 40)

# Try a simple baseline model with core features only
try:
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import r2_score, mean_squared_error
    
    # Define minimal core features that should always work
    core_features = []
    potential_core = ['edad', 'saldo', 'monto_letra', 'letras_mensuales']
    
    for feature in potential_core:
        if feature in X_train_final.columns:
            core_features.append(feature)
    
    if len(core_features) >= 3:
        print(f"ğŸ”§ Testing baseline with {len(core_features)} core features: {core_features}")
        
        # Simple baseline model
        baseline_model = RandomForestRegressor(n_estimators=50, random_state=42)
        baseline_model.fit(X_train_final[core_features], y_train_cleaned)
        
        # Quick prediction
        baseline_pred = baseline_model.predict(X_test_final[core_features])
        baseline_r2 = r2_score(y_test_cleaned, baseline_pred)
        baseline_rmse = np.sqrt(mean_squared_error(y_test_cleaned, baseline_pred))
        
        print(f"   ğŸ“Š Baseline RÂ²: {baseline_r2:.4f}")
        print(f"   ğŸ“Š Baseline RMSE: {baseline_rmse:.2f}")
        
        if baseline_r2 > 0.2:
            print("   âœ… Baseline performs reasonably - issue is likely in feature selection")
        else:
            print("   âŒ Even baseline performs poorly - issue is in data pipeline")
    else:
        print("   âŒ Cannot create baseline - core features missing")
        
except Exception as e:
    print(f"   âŒ Baseline test failed: {str(e)}")

# =============================================================================
# STEP 6: RECOMMENDATIONS BASED ON FINDINGS
# =============================================================================
print(f"\nğŸ’¡ STEP 6: Diagnostic Recommendations")
print("-" * 40)

recommendations = []

# Check feature count
if len(selected_features_final) < 10:
    recommendations.append("ğŸ”§ CRITICAL: Too few features selected - relax noise filtering")

# Check for missing critical features
if missing_critical:
    recommendations.append("ğŸ”§ CRITICAL: Add missing critical features manually")

# Check for data issues
if 'X_train_final' in locals() and X_train_final.isnull().sum().sum() > 0:
    recommendations.append("ğŸ”§ HIGH: Fix NaN values in training data")

# Check target variable
if 'y_train_cleaned' in locals():
    if y_train_cleaned.std() < 100:
        recommendations.append("ğŸ”§ HIGH: Target variable may be over-winsorized")

# Check predictions
if 'test_predictions' in locals():
    if test_predictions.std() < 100:
        recommendations.append("ğŸ”§ HIGH: Predictions have very low variance - model not learning")

print("ğŸ¯ IMMEDIATE ACTION ITEMS:")
for i, rec in enumerate(recommendations, 1):
    print(f"   {i}. {rec}")

if not recommendations:
    print("   âœ… No critical issues detected in basic diagnostics")
    print("   ğŸ” Issue may be in model hyperparameters or feature interactions")

print(f"\nâœ… Diagnostic complete! Check the recommendations above.")

# =============================================================================
# STEP 7: EMERGENCY FIXES TO TRY
# =============================================================================
print(f"\nğŸš¨ STEP 7: Emergency Fixes to Try")
print("-" * 40)

print("ğŸ”§ FIX 1: Force include critical features")
print("# Add this code to force include critical features:")
print("""
# Force include critical features regardless of noise filtering
critical_features_to_add = ['edad', 'saldo', 'monto_letra', 'letras_mensuales',
                           'ocupacion_consolidated_freq', 'ciudad_consolidated_freq']

for feature in critical_features_to_add:
    if feature in train_df_enhanced.columns and feature not in selected_features_final:
        selected_features_final.append(feature)
        print(f"   âœ… Force added: {feature}")

print(f"Updated feature count: {len(selected_features_final)}")
""")

print("\nğŸ”§ FIX 2: Relax noise filtering thresholds")
print("# Replace the noise filtering section with more lenient thresholds:")
print("""
# More lenient thresholds (use 50th percentile instead of 60th/70th)
rf_threshold = np.percentile(importances1, 50)  # Top 50% of features
lgbm_threshold = np.percentile(importances2, 50)  # Top 50% of features
ridge_threshold = np.percentile(importances3, 50)  # Top 50% of features

# Lower minimum strategy support to 1
min_strategy_support = 1
""")

print("\nğŸ”§ FIX 3: Ensure minimum feature count")
print("# Add this after feature selection:")
print("""
# Ensure we have at least 20 features
if len(selected_features_final) < 20:
    print(f"âš ï¸ Only {len(selected_features_final)} features selected, adding more...")

    # Get top features by importance
    additional_features = real_features_df.head(30)['Feature'].tolist()
    for feature in additional_features:
        if feature not in selected_features_final and feature not in noise_features:
            selected_features_final.append(feature)
            if len(selected_features_final) >= 20:
                break

    print(f"âœ… Updated to {len(selected_features_final)} features")
""")

print("\nğŸ”§ FIX 4: Check for data leakage in scaling")
print("# Make sure scaling is done correctly:")
print("""
# Fit scaler only on training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_final)
X_valid_scaled = scaler.transform(X_valid_final)  # Use transform, not fit_transform
X_test_scaled = scaler.transform(X_test_final)    # Use transform, not fit_transform
""")

print("\nğŸ”§ FIX 5: Verify target variable consistency")
print("# Add this check before model training:")
print("""
# Verify target variables are consistent
print(f"Target variable checks:")
print(f"   y_train shape: {y_train_cleaned.shape}")
print(f"   y_train mean: ${y_train_cleaned.mean():,.2f}")
print(f"   y_train std: ${y_train_cleaned.std():,.2f}")
print(f"   y_train range: ${y_train_cleaned.min():,.2f} to ${y_train_cleaned.max():,.2f}")

# Check if target is too constrained
if y_train_cleaned.std() < 200:
    print("âš ï¸ WARNING: Target variable has very low variance!")
    print("   Consider less aggressive winsorization")
""")

print("\nğŸ¯ PRIORITY ORDER:")
print("   1. Run the diagnostic first")
print("   2. Apply FIX 1 (force critical features)")
print("   3. Apply FIX 3 (ensure minimum features)")
print("   4. If still poor, apply FIX 2 (relax thresholds)")
print("   5. Check FIX 4 and FIX 5 for data issues")

print("\nâœ… Emergency fixes ready to apply!")






-------------------------------------
ğŸ” COMPREHENSIVE MODEL DIAGNOSTICS
============================================================

ğŸ“Š STEP 1: Basic Data Validation
----------------------------------------
ğŸ” Dataset being used: DataFrame
ğŸ“ Original dataset shape: (15000, 32)

ğŸ’° Target variable (ingresos_reportados) statistics:
   Count: 15,000
   Mean: $135,767.41
   Median: $1,300.00
   Min: $500.00
   Max: $999,999,999.00
   Std: $11,546,884.21
   Missing values: 0 (0.0%)

ğŸ¯ STEP 2: Feature Selection Validation
----------------------------------------
ğŸ“ˆ Selected features count: 50
ğŸ² Noise features count: 8

ğŸ”‘ Critical features check:
   âœ… edad: INCLUDED
   âœ… saldo: INCLUDED
   âœ… monto_letra: INCLUDED
   âœ… ocupacion_consolidated_freq: INCLUDED
   âŒ ciudad_consolidated_freq: MISSING
   âŒ letras_mensuales: MISSING

âš ï¸  WARNING: 2 critical features are missing!
   This could explain the poor performance.

ğŸ“‹ Top 10 selected features:
    1. estado_civil_consolidated_Others_x_age_group_56-65
    2. occupation_city_age_interaction
    3. fecha_inicio_days
    4. estado_civil_consolidated_Others_x_age_group_65+
    5. payment_per_age
    6. saldo
    7. monto_letra
    8. contract_duration
    9. ocupacion_consolidated_freq
   10. retired_x_age_group_46-55

ğŸ”§ STEP 3: Data Pipeline Validation
----------------------------------------
ğŸ“Š Final dataset shapes:
   X_train_final: NOT CREATED
   X_valid_final: NOT CREATED
   X_test_final: (2250, 50)

ğŸ’° Cleaned target statistics:
   y_train_cleaned shape: (10500,)
   Mean: $1,706.63
   Min: $500.00
   Max: $47,225.51
   Winsorized values: 0 (0.0%)

ğŸ¤– STEP 4: Model Validation
----------------------------------------
âœ… Best model available: XGBRegressor
âœ… Best model name: XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.85, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             feature_weights=None, gamma=None, grow_policy=None,
             importance_type=None, interaction_constraints=None,
             learning_rate=0.05, max_bin=None, max_cat_threshold=None,
             max_cat_to_onehot=None, max_delta_step=None, max_depth=6,
             max_leaves=None, min_child_weight=5, missing=nan,
             monotone_constraints=None, multi_strategy=None, n_estimators=300,
             n_jobs=-1, num_parallel_tree=None, ...)

ğŸ“ˆ Prediction statistics:
   Shape: (2250,)
   Mean: $1,680.40
   Min: $-110.62
   Max: $23,789.01
   Std: $1,077.81
   âŒ WARNING: Negative predictions detected!

âš¡ STEP 5: Quick Baseline Performance Check
----------------------------------------
   âŒ Baseline test failed: name 'X_train_final' is not defined

ğŸ’¡ STEP 6: Diagnostic Recommendations
----------------------------------------
ğŸ¯ IMMEDIATE ACTION ITEMS:
   1. ğŸ”§ CRITICAL: Add missing critical features manually

âœ… Diagnostic complete! Check the recommendations above.

ğŸš¨ STEP 7: Emergency Fixes to Try
----------------------------------------
ğŸ”§ FIX 1: Force include critical features
# Add this code to force include critical features:

# Force include critical features regardless of noise filtering
critical_features_to_add = ['edad', 'saldo', 'monto_letra', 'letras_mensuales',
                           'ocupacion_consolidated_freq', 'ciudad_consolidated_freq']

for feature in critical_features_to_add:
    if feature in train_df_enhanced.columns and feature not in selected_features_final:
        selected_features_final.append(feature)
        print(f"   âœ… Force added: {feature}")

print(f"Updated feature count: {len(selected_features_final)}")


ğŸ”§ FIX 2: Relax noise filtering thresholds
# Replace the noise filtering section with more lenient thresholds:

# More lenient thresholds (use 50th percentile instead of 60th/70th)
rf_threshold = np.percentile(importances1, 50)  # Top 50% of features
lgbm_threshold = np.percentile(importances2, 50)  # Top 50% of features
ridge_threshold = np.percentile(importances3, 50)  # Top 50% of features

# Lower minimum strategy support to 1
min_strategy_support = 1


ğŸ”§ FIX 3: Ensure minimum feature count
# Add this after feature selection:

# Ensure we have at least 20 features
if len(selected_features_final) < 20:
    print(f"âš ï¸ Only {len(selected_features_final)} features selected, adding more...")

    # Get top features by importance
    additional_features = real_features_df.head(30)['Feature'].tolist()
    for feature in additional_features:
        if feature not in selected_features_final and feature not in noise_features:
            selected_features_final.append(feature)
            if len(selected_features_final) >= 20:
                break

    print(f"âœ… Updated to {len(selected_features_final)} features")


ğŸ”§ FIX 4: Check for data leakage in scaling
# Make sure scaling is done correctly:

# Fit scaler only on training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_final)
X_valid_scaled = scaler.transform(X_valid_final)  # Use transform, not fit_transform
X_test_scaled = scaler.transform(X_test_final)    # Use transform, not fit_transform


ğŸ”§ FIX 5: Verify target variable consistency
# Add this check before model training:

# Verify target variables are consistent
print(f"Target variable checks:")
print(f"   y_train shape: {y_train_cleaned.shape}")
print(f"   y_train mean: ${y_train_cleaned.mean():,.2f}")
print(f"   y_train std: ${y_train_cleaned.std():,.2f}")
print(f"   y_train range: ${y_train_cleaned.min():,.2f} to ${y_train_cleaned.max():,.2f}")

# Check if target is too constrained
if y_train_cleaned.std() < 200:
    print("âš ï¸ WARNING: Target variable has very low variance!")
    print("   Consider less aggressive winsorization")


ğŸ¯ PRIORITY ORDER:
   1. Run the diagnostic first
   2. Apply FIX 1 (force critical features)
   3. Apply FIX 3 (ensure minimum features)
   4. If still poor, apply FIX 2 (relax thresholds)
   5. Check FIX 4 and FIX 5 for data issues

âœ… Emergency fixes ready to apply!