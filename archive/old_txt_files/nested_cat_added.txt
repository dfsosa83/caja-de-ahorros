# %%
# =============================================================================
# INCOME PREDICTION MODEL - NESTED CROSS-VALIDATION PIPELINE WITH CATBOOST
# =============================================================================
# Goal: Predict customer income using NESTED CV for unbiased performance estimates
# Based on: model_process_with_no_tranformations.txt preprocessing structure
#
# Key Features:
# - Nested CV Structure: Outer CV (5 folds) for evaluation, Inner CV (3 folds) for hyperparameter tuning
# - Robust Metrics: Primary focus on RMSE/MAE instead of RÂ² for better income prediction assessment
# - Complete Pipeline: From preprocessed data to production-ready model
# - Comprehensive Analysis: Permutation importance, visualizations, model comparison
#
# Models: XGBoost, LightGBM, Random Forest, CatBoost
# =============================================================================

# %%
# SECTION 1: IMPORTS AND SETUP
# =============================================================================
# NOTE: Before running this notebook, install CatBoost:
# pip install catboost
# or
# conda install -c conda-forge catboost

import pandas as pd
import numpy as np
import warnings
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import (KFold, GroupKFold, cross_val_score,
                                   RandomizedSearchCV)
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.inspection import permutation_importance
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
from collections import Counter

warnings.filterwarnings('ignore')

print("================================================================================")
print("NESTED CROSS-VALIDATION INCOME PREDICTION MODEL WITH CATBOOST - STARTED")
print("================================================================================")
print("ğŸ“‹ Pipeline based on: model_process_with_no_tranformations.txt")
print("ğŸ¯ Primary Metrics: RMSE (Root Mean Square Error) and MAE (Mean Absolute Error)")
print("ğŸ”„ Nested CV: Outer (5-fold) for evaluation, Inner (3-fold) for hyperparameter tuning")
print("ğŸš€ Models: Linear Regression (Baseline), XGBoost, LightGBM, Random Forest, CatBoost")

# %%
# SECTION 2: PREPARE FINAL DATASETS FOR NESTED CV MODELING
# =============================================================================
print("\nğŸ¯ PREPARING FINAL DATASETS FOR NESTED CV")
print("-" * 60)

# EXPLICIT FEATURE SELECTION APPROACH (from original pipeline)
id_columns = ['cliente', 'identificador_unico']
target_column = 'ingresos_reportados'

# Use the selected features from the original pipeline
# NOTE: Adjust 'selected_features_final' to your actual feature list variable
feature_columns = selected_features_final

# Verify all selected features exist in the dataset
available_features = []
missing_features = []

for feature in feature_columns:
    if feature in train_df_enhanced.columns:
        available_features.append(feature)
    else:
        missing_features.append(feature)

if missing_features:
    print(f"âš ï¸  Missing features (will be skipped): {missing_features}")

feature_columns = available_features

print(f"   ğŸ“Š Selected feature columns: {len(feature_columns)}")
print(f"   ğŸ¯ Target column: {target_column}")

# Create feature matrices and targets (from original pipeline structure)
X_train = train_df_enhanced[feature_columns].copy()
y_train = train_df_enhanced[target_column].copy()

X_valid = valid_df_enhanced[feature_columns].copy()
y_valid = valid_df_enhanced[target_column].copy()

X_test = test_df_enhanced[feature_columns].copy()
y_test = test_df_enhanced[target_column].copy()

print(f"\nğŸ“ˆ DATASET SHAPES:")
print(f"   X_train: {X_train.shape}")
print(f"   X_valid: {X_valid.shape}")
print(f"   X_test: {X_test.shape}")

# Combine train and validation for nested CV (test set remains untouched)
X_train_full = pd.concat([X_train, X_valid], ignore_index=True)
y_train_full = pd.concat([y_train, y_valid], ignore_index=True)

print(f"   X_train_full (for nested CV): {X_train_full.shape}")
print(f"   X_test (held out): {X_test.shape}")

# Show selected features grouped by type
print(f"\nğŸ“‹ SELECTED FEATURES ({len(feature_columns)} features):")
print("-" * 60)

# Group features by type for better readability (from original pipeline)
basic_features = []
age_features = []
freq_features = []
interaction_features = []
other_features = []

for feature in feature_columns:
    if feature.startswith('age_group_'):
        age_features.append(feature)
    elif feature.endswith('_freq'):
        freq_features.append(feature)
    elif '_x_' in feature or 'retired_x_' in feature or 'employer_x_' in feature or 'gender_x_' in feature:
        interaction_features.append(feature)
    elif feature in ['edad', 'letras_mensuales', 'monto_letra', 'saldo', 'is_retired']:
        basic_features.append(feature)
    else:
        other_features.append(feature)

print(f"ğŸ”¢ BASIC FEATURES ({len(basic_features)}): {basic_features}")
print(f"ğŸ‘¥ AGE GROUP FEATURES ({len(age_features)}): {age_features}")
print(f"ğŸ“Š FREQUENCY FEATURES ({len(freq_features)}): {freq_features}")
print(f"âš¡ INTERACTION FEATURES ({len(interaction_features)}): {interaction_features}")
print(f"ğŸ”§ OTHER FEATURES ({len(other_features)}): {other_features}")

# Verify data quality
print(f"\nâœ… DATA QUALITY CHECKS:")
print(f"   Missing values in X_train_full: {X_train_full.isnull().sum().sum()}")
print(f"   Missing values in y_train_full: {y_train_full.isnull().sum()}")
print(f"   All features numeric: {all(X_train_full.dtypes.apply(lambda x: x in ['int64', 'float64']))}")

# Save feature list to file for reference
feature_list_df = pd.DataFrame({
    'feature_name': feature_columns,
    'feature_type': ['basic' if f in basic_features else
                    'age_group' if f in age_features else
                    'frequency' if f in freq_features else
                    'interaction' if f in interaction_features else
                    'other' for f in feature_columns]
})

feature_list_df.to_csv(data_path + '/nested_cv_catboost_feature_list.csv', index=False)
print(f"\nğŸ’¾ Feature list saved to: nested_cv_catboost_feature_list.csv")

# %%
# SECTION 2.5: EXTRACT AND SAVE FREQUENCY MAPPINGS FOR PRODUCTION
# =============================================================================
print("\nğŸ’¾ EXTRACTING FREQUENCY MAPPINGS FOR PRODUCTION DEPLOYMENT")
print("-" * 60)

# Extract frequency mappings from the training data for production use
# These mappings ensure consistent encoding when predicting single customers

frequency_mappings = {}

# Find frequency-encoded features in your selected features
freq_features_in_model = [f for f in feature_columns if f.endswith('_freq')]

print(f"   ğŸ“Š Found {len(freq_features_in_model)} frequency-encoded features:")
for freq_feature in freq_features_in_model:
    print(f"      â€¢ {freq_feature}")

# Extract mappings from the enhanced training dataframes
# We'll use the full training data (train + validation) to get complete mappings
print(f"\nğŸ” Extracting frequency mappings from training data...")

for freq_feature in freq_features_in_model:
    if freq_feature in train_df_enhanced.columns:
        # Get the original categorical column name
        original_col = freq_feature.replace('_freq', '')
        
        if original_col in train_df_enhanced.columns:
            # Extract the frequency mapping from training data
            freq_mapping = train_df_enhanced[original_col].value_counts().to_dict()
            frequency_mappings[freq_feature] = freq_mapping
            
            print(f"   âœ… {freq_feature}:")
            print(f"      Original column: {original_col}")
            print(f"      Categories: {len(freq_mapping)}")
            print(f"      Top 3: {list(freq_mapping.keys())[:3]}")
        else:
            print(f"   âš ï¸ Original column '{original_col}' not found for {freq_feature}")
    else:
        print(f"   âš ï¸ Frequency feature '{freq_feature}' not found in training data")

# Save frequency mappings for production use
import pickle
import json

# Save as pickle for Python production systems
frequency_mappings_path = data_path + '/production_frequency_mappings_catboost.pkl'
with open(frequency_mappings_path, 'wb') as f:
    pickle.dump(frequency_mappings, f)

# Save as JSON for cross-platform compatibility
frequency_mappings_json_path = data_path + '/production_frequency_mappings_catboost.json'
with open(frequency_mappings_json_path, 'w') as f:
    json.dump(frequency_mappings, f, indent=2)

print(f"\nğŸ’¾ FREQUENCY MAPPINGS SAVED:")
print(f"   ğŸ“¦ Pickle format: production_frequency_mappings_catboost.pkl")
print(f"   ğŸ“„ JSON format: production_frequency_mappings_catboost.json")
print(f"   ğŸ¯ Total mappings: {len(frequency_mappings)}")

# Create a summary of the mappings for documentation
mappings_summary = {}
for freq_feature, mapping in frequency_mappings.items():
    mappings_summary[freq_feature] = {
        'total_categories': len(mapping),
        'top_5_categories': dict(list(mapping.items())[:5]),
        'min_frequency': min(mapping.values()),
        'max_frequency': max(mapping.values()),
        'others_frequency': mapping.get('Others', 'Not found')
    }

# Save summary for documentation
summary_path = data_path + '/frequency_mappings_summary_catboost.json'
with open(summary_path, 'w') as f:
    json.dump(mappings_summary, f, indent=2)

print(f"   ğŸ“‹ Summary saved: frequency_mappings_summary_catboost.json")

print(f"\nğŸ¯ PRODUCTION USAGE:")
print(f"   1. Load mappings: frequency_mappings = pickle.load(open('production_frequency_mappings_catboost.pkl', 'rb'))")
print(f"   2. Apply to new customer: customer['ocupacion_consolidated_freq'] = frequency_mappings['ocupacion_consolidated_freq'][customer['ocupacion_consolidated']]")
print(f"   3. Handle new values: Map unknown categories to 'Others' frequency")

print(f"\nâœ… FREQUENCY MAPPINGS EXTRACTION COMPLETE!")

# %%
# SECTION 3: FEATURE SCALING
# =============================================================================
print("\nâš–ï¸ FEATURE SCALING")
print("-" * 50)

# Apply robust scaling to features (from original pipeline)
print("   âš–ï¸ Applying RobustScaler...")
scaler = RobustScaler()

# Fit scaler on full training data (train + validation combined)
X_train_full_scaled = pd.DataFrame(
    scaler.fit_transform(X_train_full),
    columns=X_train_full.columns,
    index=X_train_full.index
)

# Transform test set using the same scaler
X_test_scaled = pd.DataFrame(
    scaler.transform(X_test),
    columns=X_test.columns,
    index=X_test.index
)

print("   âœ… Feature scaling complete")
print(f"   ğŸ“Š Scaled training data: {X_train_full_scaled.shape}")
print(f"   ğŸ“Š Scaled test data: {X_test_scaled.shape}")

# %%
# SECTION 4: NESTED CROSS-VALIDATION SETUP
# =============================================================================
print("\nğŸ”„ NESTED CROSS-VALIDATION SETUP")
print("-" * 60)

# Define CV strategies with robust metrics focus
OUTER_CV_FOLDS = 5  # For unbiased performance estimation
INNER_CV_FOLDS = 3  # For hyperparameter tuning
RANDOM_SEARCH_ITERATIONS = 80  # Increased for better hyperparameter search

# Create CV objects
outer_cv = KFold(n_splits=OUTER_CV_FOLDS, shuffle=True, random_state=42)
inner_cv = KFold(n_splits=INNER_CV_FOLDS, shuffle=True, random_state=42)

print(f"   ğŸ”„ Outer CV: {OUTER_CV_FOLDS} folds (for unbiased model evaluation)")
print(f"   ğŸ”„ Inner CV: {INNER_CV_FOLDS} folds (for hyperparameter tuning)")
print(f"   ğŸ” Random Search: {RANDOM_SEARCH_ITERATIONS} iterations per inner fold")
print(f"   ğŸ“Š Total model trainings: {OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS} per model")
print(f"      ({OUTER_CV_FOLDS} outer Ã— {INNER_CV_FOLDS} inner Ã— {RANDOM_SEARCH_ITERATIONS} iterations)")


# %%
# SECTION 5: MODEL DEFINITIONS AND HYPERPARAMETER GRIDS WITH CATBOOST
# =============================================================================
print("\nğŸ¤– MODEL DEFINITIONS AND HYPERPARAMETER GRIDS WITH CATBOOST")
print("-" * 60)

# Base models (from simple to complex: Linear Regression â†’ Tree-based â†’ Gradient Boosting)
base_models = {
    'Linear Regression': LinearRegression(n_jobs=-1),
    'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1),
    'XGBoost': xgb.XGBRegressor(random_state=42, n_jobs=-1),
    'LightGBM': lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),
    'CatBoost': cb.CatBoostRegressor(random_state=42, verbose=False, thread_count=-1)
}

# Comprehensive hyperparameter grids for nested CV
# These grids are designed for income prediction optimization
param_grids = {
    'Linear Regression': {
        # Linear Regression has no hyperparameters to tune
        # We include an empty dict to maintain consistency with the nested CV framework
    },
    'Random Forest': {
        'n_estimators': [200, 300, 400],
        'max_depth': [10, 15, 20, None],
        'min_samples_split': [5, 10, 15],
        'min_samples_leaf': [2, 5, 10],
        'max_features': ['sqrt', 'log2', 0.8],
        'max_samples': [0.7, 0.8, 0.9]
    },
    'XGBoost': {
        'n_estimators': [300, 800, 1100],
        'max_depth': [6, 8, 10],
        'learning_rate': [0.005, 0.007, 0.01],
        'subsample': [0.8, 0.85, 0.9],
        'colsample_bytree': [0.8, 0.85, 0.9],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [0.5, 1.0, 2.0],
        'min_child_weight': [1, 3, 5]
    },
    'LightGBM': {
        'n_estimators': [300, 800, 1100],
        'max_depth': [6, 8, 10],
        'learning_rate': [0.005, 0.007, 0.01],
        'subsample': [0.7, 0.8, 0.85],
        'colsample_bytree': [0.7, 0.8, 0.85],
        'num_leaves': [30, 50, 80],
        'min_child_samples': [30, 40, 50],
        'reg_alpha': [0.5, 1.0, 1.5],
        'reg_lambda': [2.0, 3.0, 4.0]
        },
    'CatBoost': {
        'iterations': [300, 800, 1100],
        'depth': [6, 8, 10],
        'learning_rate': [0.005, 0.007, 0.01],
        'subsample': [0.8, 0.85, 0.9],
        'colsample_bylevel': [0.8, 0.85, 0.9],
        'l2_leaf_reg': [1, 3, 5],
        'border_count': [32, 64, 128],
        'bagging_temperature': [0, 0.5, 1.0]
    }
}

print(f"   ğŸ¤– Models defined: {list(base_models.keys())}")
print(f"   ğŸ”§ Hyperparameter search space per model:")
for model_name, grid in param_grids.items():
    total_combinations = np.prod([len(values) for values in grid.values()])
    print(f"      {model_name}: {total_combinations:,} total combinations")

print(f"\nğŸ’¡ Primary Optimization Metric: RMSE (Root Mean Square Error)")
print(f"   ğŸ“Š RMSE penalizes large prediction errors more heavily")
print(f"   ğŸ’° Better suited for income prediction than RÂ² alone")
print(f"   ğŸ“ˆ Lower RMSE = Better model performance")
print(f"\nğŸš€ CatBoost Advantages:")
print(f"   ğŸ¯ Excellent with categorical features (even after encoding)")
print(f"   ğŸ”§ Built-in regularization and overfitting protection")
print(f"   âš¡ Often competitive with XGBoost/LightGBM")
print(f"   ğŸ“Š Robust hyperparameter defaults")


# %%
# SECTION 6: NESTED CROSS-VALIDATION IMPLEMENTATION
# =============================================================================
print("\nğŸ¯ NESTED CROSS-VALIDATION IMPLEMENTATION")
print("=" * 70)

def nested_cross_validation(model, param_grid, X, y, outer_cv, inner_cv, model_name):
    """
    Perform nested cross-validation for unbiased model evaluation with robust metrics focus

    This implementation prioritizes RMSE and MAE as primary metrics for income prediction,
    while still tracking RÂ² for comparison purposes.

    Args:
        model: Base model to evaluate
        param_grid: Hyperparameter grid for tuning
        X, y: Training data and target
        outer_cv, inner_cv: Cross-validation objects
        model_name: Name for reporting

    Returns:
        dict: Comprehensive nested CV results with robust metrics
    """
    print(f"\nğŸ”„ Starting Nested CV for {model_name}")
    print(f"   ğŸ“Š Outer folds: {outer_cv.n_splits}, Inner folds: {inner_cv.n_splits}")
    print(f"   ğŸ¯ Primary metrics: RMSE (lower is better), MAE (lower is better)")

    # Storage for results - prioritizing robust metrics
    outer_scores_rmse = []
    outer_scores_mae = []
    outer_scores_r2 = []  # Still track RÂ² for comparison
    best_params_per_fold = []
    fold_details = []

    # Outer CV loop - each iteration gives unbiased performance estimate
    for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X, y), 1):
        print(f"\n   ğŸ”„ Outer Fold {fold_idx}/{outer_cv.n_splits}")

        # Split data for this outer fold
        X_train_outer = X.iloc[train_idx]
        X_val_outer = X.iloc[val_idx]
        y_train_outer = y.iloc[train_idx]
        y_val_outer = y.iloc[val_idx]

        print(f"      ğŸ“Š Fold {fold_idx} sizes: Train={len(train_idx)}, Val={len(val_idx)}")

        # Inner CV: Hyperparameter tuning using RMSE as optimization metric
        print(f"      ğŸ”§ Inner CV: Hyperparameter tuning (optimizing RMSE)...")

        # Handle Linear Regression (no hyperparameters to tune)
        if model_name == 'Linear Regression':
            # Linear Regression has no hyperparameters, so just fit the model
            best_model = model
            best_model.fit(X_train_outer, y_train_outer)
            best_params = {}  # No parameters to store
            best_params_per_fold.append(best_params)

            # Calculate RMSE on inner CV for consistency
            inner_cv_scores = cross_val_score(model, X_train_outer, y_train_outer,
                                            cv=inner_cv, scoring='neg_mean_squared_error', n_jobs=-1)
            best_inner_rmse = np.sqrt(-inner_cv_scores.mean())
            print(f"      âœ… Linear Regression CV RMSE: ${best_inner_rmse:.2f} (no hyperparameters to tune)")
        else:
            # For other models, perform hyperparameter search
            random_search = RandomizedSearchCV(
                estimator=model,
                param_distributions=param_grid,
                n_iter=RANDOM_SEARCH_ITERATIONS,  # Use global variable
                cv=inner_cv,
                scoring='neg_mean_squared_error',  # Optimizing RMSE (MSE negated)
                n_jobs=-1,
                random_state=42,
                verbose=0
            )

            # Fit hyperparameter search on outer training data
            random_search.fit(X_train_outer, y_train_outer)

            # Get best model from inner CV
            best_model = random_search.best_estimator_
            best_params = random_search.best_params_
            best_params_per_fold.append(best_params)

            # Convert negative MSE back to RMSE for reporting
            best_inner_rmse = np.sqrt(-random_search.best_score_)
            print(f"      âœ… Best inner CV RMSE: ${best_inner_rmse:.2f}")

        # Evaluate best model on outer validation fold
        y_pred_outer = best_model.predict(X_val_outer)

        # Calculate comprehensive metrics for this outer fold
        fold_rmse = np.sqrt(mean_squared_error(y_val_outer, y_pred_outer))
        fold_mae = mean_absolute_error(y_val_outer, y_pred_outer)
        fold_r2 = r2_score(y_val_outer, y_pred_outer)

        # Store scores
        outer_scores_rmse.append(fold_rmse)
        outer_scores_mae.append(fold_mae)
        outer_scores_r2.append(fold_r2)

        print(f"      ğŸ“Š Outer fold performance:")
        print(f"         ğŸ¯ RMSE: ${fold_rmse:.2f}")
        print(f"         ğŸ¯ MAE:  ${fold_mae:.2f}")
        print(f"         ğŸ“ˆ RÂ²:   {fold_r2:.4f}")

        # Calculate additional income-specific metrics
        # Mean Absolute Percentage Error (MAPE) - but handle low incomes carefully
        valid_mask = y_val_outer > 100  # Avoid division by very small numbers
        if valid_mask.sum() > 0:
            mape = np.mean(np.abs((y_val_outer[valid_mask] - y_pred_outer[valid_mask]) / y_val_outer[valid_mask])) * 100
            print(f"         ğŸ’° MAPE (>$100): {mape:.1f}%")
        else:
            mape = np.nan

        # Store detailed results for this fold
        fold_details.append({
            'fold': fold_idx,
            'rmse': fold_rmse,
            'mae': fold_mae,
            'r2': fold_r2,
            'mape': mape,
            'best_params': best_params,
            'inner_cv_rmse': best_inner_rmse,
            'train_size': len(train_idx),
            'val_size': len(val_idx),
            'y_true_mean': y_val_outer.mean(),
            'y_pred_mean': y_pred_outer.mean()
        })

    # Calculate final nested CV results with robust metrics emphasis
    nested_cv_results = {
        'model_name': model_name,
        # Primary metrics (RMSE/MAE)
        'outer_cv_rmse_mean': np.mean(outer_scores_rmse),
        'outer_cv_rmse_std': np.std(outer_scores_rmse),
        'outer_cv_mae_mean': np.mean(outer_scores_mae),
        'outer_cv_mae_std': np.std(outer_scores_mae),
        # Secondary metric (RÂ²)
        'outer_cv_r2_mean': np.mean(outer_scores_r2),
        'outer_cv_r2_std': np.std(outer_scores_r2),
        # Raw scores for analysis
        'outer_scores_rmse': outer_scores_rmse,
        'outer_scores_mae': outer_scores_mae,
        'outer_scores_r2': outer_scores_r2,
        # Hyperparameter analysis
        'best_params_per_fold': best_params_per_fold,
        'fold_details': fold_details,
        # Model selection criteria (lower is better for RMSE/MAE)
        'selection_metric': 'rmse',  # Primary metric for model selection
        'selection_score': np.mean(outer_scores_rmse)
    }

    print(f"\n   ğŸ† {model_name} Nested CV Summary:")
    print(f"      ğŸ¯ RMSE: ${nested_cv_results['outer_cv_rmse_mean']:.2f} Â± ${nested_cv_results['outer_cv_rmse_std']:.2f}")
    print(f"      ğŸ¯ MAE:  ${nested_cv_results['outer_cv_mae_mean']:.2f} Â± ${nested_cv_results['outer_cv_mae_std']:.2f}")
    print(f"      ğŸ“ˆ RÂ²:   {nested_cv_results['outer_cv_r2_mean']:.4f} Â± {nested_cv_results['outer_cv_r2_std']:.4f}")

    return nested_cv_results

print("âœ… Nested CV function defined with robust metrics focus")

# %%
# SECTION 7: RUN NESTED CROSS-VALIDATION FOR ALL MODELS (INCLUDING CATBOOST)
# =============================================================================
print("\nğŸš€ RUNNING NESTED CROSS-VALIDATION FOR ALL MODELS (BASELINE TO ADVANCED)")
print("=" * 70)
print("âš ï¸  This will take 30-90 minutes depending on your hardware...")
print(f"   Each complex model will be trained {OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS} times")
print(f"   ({OUTER_CV_FOLDS} outer Ã— {INNER_CV_FOLDS} inner Ã— {RANDOM_SEARCH_ITERATIONS} iterations)")
print("ğŸ¯ Optimizing for RMSE (Root Mean Square Error) - best metric for income prediction")
print("ğŸ“Š Model progression: Linear Regression (baseline) â†’ Tree-based â†’ Gradient Boosting")
print("ğŸš€ Including CatBoost for comprehensive model comparison!")

# Storage for all nested CV results
nested_cv_results = {}
total_start_time = datetime.now()

# Run nested CV for each model
for model_idx, (model_name, base_model) in enumerate(base_models.items(), 1):
    print(f"\n{'='*80}")
    print(f"NESTED CV {model_idx}/{len(base_models)}: {model_name.upper()}")
    print(f"{'='*80}")

    model_start_time = datetime.now()

    # Run nested cross-validation
    results = nested_cross_validation(
        model=base_model,
        param_grid=param_grids[model_name],
        X=X_train_full_scaled,  # Use full training data (train + validation)
        y=y_train_full,
        outer_cv=outer_cv,
        inner_cv=inner_cv,
        model_name=model_name
    )

    nested_cv_results[model_name] = results

    model_end_time = datetime.now()
    model_duration = (model_end_time - model_start_time).total_seconds() / 60

    print(f"\n   â±ï¸ {model_name} completed in {model_duration:.1f} minutes")

    # Show progress
    remaining_models = len(base_models) - model_idx
    if remaining_models > 0:
        estimated_remaining = model_duration * remaining_models
        print(f"   ğŸ“Š Progress: {model_idx}/{len(base_models)} models complete")
        print(f"   â° Estimated remaining time: {estimated_remaining:.1f} minutes")

total_end_time = datetime.now()
total_duration = (total_end_time - total_start_time).total_seconds() / 60

print(f"\nğŸ‰ ALL NESTED CV COMPLETED (BASELINE TO ADVANCED MODELS)!")
print(f"â±ï¸ Total execution time: {total_duration:.1f} minutes")

# %%
# SECTION 8: NESTED CV RESULTS ANALYSIS AND MODEL COMPARISON (BASELINE TO ADVANCED)
# =============================================================================
print("\nğŸ“Š NESTED CV RESULTS ANALYSIS AND MODEL COMPARISON (BASELINE TO ADVANCED)")
print("=" * 70)

# Create comprehensive comparison DataFrame with robust metrics focus
comparison_data = []
for model_name, results in nested_cv_results.items():
    comparison_data.append({
        'Model': model_name,
        # Primary metrics (lower is better)
        'Nested_CV_RMSE_Mean': results['outer_cv_rmse_mean'],
        'Nested_CV_RMSE_Std': results['outer_cv_rmse_std'],
        'Nested_CV_MAE_Mean': results['outer_cv_mae_mean'],
        'Nested_CV_MAE_Std': results['outer_cv_mae_std'],
        # Secondary metric
        'Nested_CV_R2_Mean': results['outer_cv_r2_mean'],
        'Nested_CV_R2_Std': results['outer_cv_r2_std'],
        # Selection score (RMSE for ranking)
        'Selection_Score': results['selection_score']
    })

nested_comparison_df = pd.DataFrame(comparison_data)
# Sort by RMSE (lower is better) - primary metric for income prediction
nested_comparison_df = nested_comparison_df.sort_values('Nested_CV_RMSE_Mean', ascending=True)

print("\nğŸ† NESTED CV PERFORMANCE COMPARISON (Sorted by RMSE - Lower is Better):")
print("=" * 90)
print(nested_comparison_df.round(4).to_string(index=False))

# Identify best model based on RMSE (robust metric for income prediction)
best_model_nested = nested_comparison_df.iloc[0]['Model']
best_rmse_nested = nested_comparison_df.iloc[0]['Nested_CV_RMSE_Mean']
best_rmse_std_nested = nested_comparison_df.iloc[0]['Nested_CV_RMSE_Std']
best_mae_nested = nested_comparison_df.iloc[0]['Nested_CV_MAE_Mean']
best_mae_std_nested = nested_comparison_df.iloc[0]['Nested_CV_MAE_Std']
best_r2_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Mean']
best_r2_std_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Std']

print(f"\nğŸ¥‡ BEST MODEL (Based on RMSE): {best_model_nested}")
print(f"   ğŸ¯ Unbiased RMSE: ${best_rmse_nested:.2f} Â± ${best_rmse_std_nested:.2f}")
print(f"   ğŸ¯ Unbiased MAE:  ${best_mae_nested:.2f} Â± ${best_mae_std_nested:.2f}")
print(f"   ğŸ“ˆ Unbiased RÂ²:   {best_r2_nested:.4f} Â± {best_r2_std_nested:.4f}")

# Calculate confidence intervals for RMSE (primary metric)
rmse_ci_lower = best_rmse_nested - 1.96 * best_rmse_std_nested
rmse_ci_upper = best_rmse_nested + 1.96 * best_rmse_std_nested
print(f"   ğŸ“Š 95% Confidence Interval (RMSE): [${rmse_ci_lower:.2f}, ${rmse_ci_upper:.2f}]")

# Calculate confidence intervals for MAE and RÂ² as well
mae_ci_lower = best_mae_nested - 1.96 * best_mae_std_nested
mae_ci_upper = best_mae_nested + 1.96 * best_mae_std_nested
print(f"   ğŸ“Š 95% Confidence Interval (MAE): [${mae_ci_lower:.2f}, ${mae_ci_upper:.2f}]")

r2_ci_lower = best_r2_nested - 1.96 * best_r2_std_nested
r2_ci_upper = best_r2_nested + 1.96 * best_r2_std_nested
print(f"   ğŸ“Š 95% Confidence Interval (RÂ²): [{r2_ci_lower:.4f}, {r2_ci_upper:.4f}]")

# Performance assessment based on RMSE
print(f"\nğŸ“ˆ PERFORMANCE ASSESSMENT:")
if best_rmse_nested <= 600:
    performance_level = "EXCELLENT"
    emoji = "ğŸ‰"
elif best_rmse_nested <= 800:
    performance_level = "GOOD"
    emoji = "âœ…"
elif best_rmse_nested <= 1000:
    performance_level = "ACCEPTABLE"
    emoji = "ğŸ‘"
else:
    performance_level = "NEEDS IMPROVEMENT"
    emoji = "âš ï¸"

print(f"   {emoji} Performance Level: {performance_level}")
print(f"   ğŸ’° RMSE = ${best_rmse_nested:.2f} (Average prediction error)")
print(f"   ğŸ’° MAE = ${best_mae_nested:.2f} (Median prediction error)")

# Model ranking summary
print(f"\nğŸ“‹ MODEL RANKING (by RMSE):")
for idx, row in nested_comparison_df.iterrows():
    rank = nested_comparison_df.index.get_loc(idx) + 1
    model = row['Model']
    rmse = row['Nested_CV_RMSE_Mean']
    rmse_std = row['Nested_CV_RMSE_Std']
    print(f"   {rank}. {model:<15}: RMSE = ${rmse:.2f} Â± ${rmse_std:.2f}")

print(f"\nğŸ’¡ INTERPRETATION:")
print(f"   ğŸ¯ RMSE measures average prediction error in dollars")
print(f"   ğŸ¯ MAE measures typical prediction error (less sensitive to outliers)")
print(f"   ğŸ“ˆ RÂ² measures proportion of variance explained (0-1 scale)")
print(f"   âœ… Lower RMSE/MAE = Better model for income prediction")
print(f"   ğŸ“Š Linear Regression provides interpretable baseline performance")
print(f"   ğŸš€ CatBoost often excels with categorical features and provides robust results")

# %%
# SECTION 8.5: BASELINE COMPARISON ANALYSIS
# =============================================================================
print("\nğŸ“ˆ BASELINE COMPARISON ANALYSIS")
print("-" * 60)

if 'Linear Regression' in nested_cv_results:
    lr_results = nested_cv_results['Linear Regression']
    lr_rmse = lr_results['outer_cv_rmse_mean']
    lr_rmse_std = lr_results['outer_cv_rmse_std']
    lr_rank = nested_comparison_df[nested_comparison_df['Model'] == 'Linear Regression'].index[0] + 1

    print(f"ğŸ¯ LINEAR REGRESSION BASELINE PERFORMANCE:")
    print(f"   ğŸ“Š RMSE: ${lr_rmse:.2f} Â± ${lr_rmse_std:.2f}")
    print(f"   ğŸ† Ranking: #{lr_rank} out of {len(base_models)} models")

    # Calculate improvement over baseline for each model
    print(f"\nğŸ“Š IMPROVEMENT OVER LINEAR REGRESSION BASELINE:")
    for idx, row in nested_comparison_df.iterrows():
        model = row['Model']
        rmse = row['Nested_CV_RMSE_Mean']

        if model != 'Linear Regression':
            improvement = ((lr_rmse - rmse) / lr_rmse) * 100
            improvement_dollars = lr_rmse - rmse

            if improvement > 0:
                print(f"   âœ… {model:<15}: {improvement:+5.1f}% improvement (${improvement_dollars:+6.0f})")
            else:
                print(f"   âŒ {model:<15}: {improvement:+5.1f}% worse (${improvement_dollars:+6.0f})")

    # Business interpretation
    print(f"\nğŸ’¡ BASELINE INSIGHTS:")
    if lr_rank == 1:
        print(f"   ğŸš¨ LINEAR REGRESSION WINS! Complex models may be overfitting.")
        print(f"   ğŸ’¼ Consider using Linear Regression for production (simpler, interpretable)")
    elif lr_rank <= 3:
        print(f"   ğŸ‘ Linear Regression performs competitively")
        print(f"   ğŸ’¼ Complex models provide modest but meaningful improvements")
    else:
        print(f"   âœ… Complex models significantly outperform baseline")
        print(f"   ğŸ’¼ Investment in complex models is justified")

    print(f"\nğŸ” LINEAR REGRESSION ADVANTAGES:")
    print(f"   âœ… Highly interpretable (feature coefficients)")
    print(f"   âœ… Fast training and prediction")
    print(f"   âœ… No hyperparameter tuning needed")
    print(f"   âœ… Robust to overfitting")
    print(f"   âœ… Easy to deploy and maintain")

else:
    print("   âš ï¸ Linear Regression results not found.")

# %%
# SECTION 9: CATBOOST SPECIFIC ANALYSIS
# =============================================================================
print("\nğŸš€ CATBOOST SPECIFIC ANALYSIS")
print("-" * 60)

if 'CatBoost' in nested_cv_results:
    catboost_results = nested_cv_results['CatBoost']
    catboost_rmse = catboost_results['outer_cv_rmse_mean']
    catboost_rmse_std = catboost_results['outer_cv_rmse_std']
    catboost_rank = nested_comparison_df[nested_comparison_df['Model'] == 'CatBoost'].index[0] + 1

    print(f"ğŸ¯ CATBOOST PERFORMANCE:")
    print(f"   ğŸ“Š RMSE: ${catboost_rmse:.2f} Â± ${catboost_rmse_std:.2f}")
    print(f"   ğŸ† Ranking: #{catboost_rank} out of {len(base_models)} models")

    # Compare with other gradient boosting models
    if 'XGBoost' in nested_cv_results and 'LightGBM' in nested_cv_results:
        xgb_rmse = nested_cv_results['XGBoost']['outer_cv_rmse_mean']
        lgb_rmse = nested_cv_results['LightGBM']['outer_cv_rmse_mean']

        print(f"\nğŸ“Š GRADIENT BOOSTING COMPARISON:")
        print(f"   CatBoost: ${catboost_rmse:.2f}")
        print(f"   XGBoost:  ${xgb_rmse:.2f}")
        print(f"   LightGBM: ${lgb_rmse:.2f}")

        if catboost_rmse <= min(xgb_rmse, lgb_rmse):
            print(f"   ğŸ¥‡ CatBoost WINS among gradient boosting models!")
        elif catboost_rmse <= max(xgb_rmse, lgb_rmse):
            print(f"   ğŸ¥ˆ CatBoost performs competitively with other gradient boosting models")
        else:
            print(f"   ğŸ“Š CatBoost provides alternative perspective to XGBoost/LightGBM")

    print(f"\nğŸš€ CATBOOST ADVANTAGES FOR THIS DATASET:")
    print(f"   ğŸ¯ Excellent handling of categorical features (even after frequency encoding)")
    print(f"   ğŸ”§ Built-in overfitting protection and regularization")
    print(f"   âš¡ Often requires less hyperparameter tuning than XGBoost/LightGBM")
    print(f"   ğŸ“Š Robust performance across different data distributions")
    print(f"   ğŸ’° Well-suited for financial/income prediction tasks")

    # Hyperparameter stability analysis for CatBoost
    catboost_params = catboost_results['best_params_per_fold']
    print(f"\nğŸ”§ CATBOOST HYPERPARAMETER STABILITY:")

    # Check most common parameters
    param_stability = {}
    for param_name in ['iterations', 'depth', 'learning_rate']:
        if param_name in catboost_params[0]:
            values = [params.get(param_name) for params in catboost_params]
            unique_values = len(set(values))
            most_common = max(set(values), key=values.count)
            param_stability[param_name] = {
                'unique_values': unique_values,
                'most_common': most_common,
                'stability': 'High' if unique_values <= 2 else 'Moderate' if unique_values <= 3 else 'Low'
            }

    for param, info in param_stability.items():
        print(f"   {param}: {info['stability']} stability (most common: {info['most_common']})")

else:
    print("   âš ï¸ CatBoost results not found. Make sure CatBoost was included in the model comparison.")

print(f"\nâœ… CATBOOST ANALYSIS COMPLETE!")

# %%
# FINAL SUMMARY: COMPREHENSIVE MODEL COMPARISON WITH CATBOOST
# =============================================================================
print("\nğŸ‰ FINAL SUMMARY: COMPREHENSIVE MODEL COMPARISON WITH CATBOOST")
print("=" * 80)

print(f"ğŸ“Š MODELS EVALUATED: {len(base_models)} (Baseline to Advanced)")
for i, model in enumerate(base_models.keys(), 1):
    if model == 'Linear Regression':
        print(f"   {i}. {model} (ğŸ“Š BASELINE)")
    elif model in ['XGBoost', 'LightGBM', 'CatBoost']:
        print(f"   {i}. {model} (ğŸš€ ADVANCED)")
    else:
        print(f"   {i}. {model}")

print(f"\nğŸ† FINAL RANKINGS (by RMSE):")
for idx, row in nested_comparison_df.iterrows():
    rank = nested_comparison_df.index.get_loc(idx) + 1
    model = row['Model']
    rmse = row['Nested_CV_RMSE_Mean']
    rmse_std = row['Nested_CV_RMSE_Std']

    if rank == 1:
        emoji = "ğŸ¥‡"
    elif rank == 2:
        emoji = "ğŸ¥ˆ"
    elif rank == 3:
        emoji = "ğŸ¥‰"
    else:
        emoji = f"{rank}."

    print(f"   {emoji} {model}: ${rmse:.2f} Â± ${rmse_std:.2f}")

print(f"\nğŸ¯ WINNER: {best_model_nested}")
print(f"   ğŸ’° Expected RMSE: ${best_rmse_nested:.2f}")
print(f"   ğŸ“Š This represents the average prediction error in dollars")

print(f"\nğŸ“Š BASELINE TO ADVANCED PROGRESSION:")
print(f"   âœ… Linear Regression: Interpretable baseline performance")
print(f"   âœ… Random Forest: Tree-based ensemble approach")
print(f"   âœ… XGBoost/LightGBM/CatBoost: Advanced gradient boosting")
print(f"   âœ… Comprehensive comparison across algorithmic complexity")
print(f"   âœ… Demonstrates value of complex models vs simple baseline")

print(f"\nğŸ’¾ NEXT STEPS:")
print(f"   1. Use the best model ({best_model_nested}) for final training")
print(f"   2. Apply saved frequency mappings for production predictions")
print(f"   3. Monitor model performance with expected RMSE: ${best_rmse_nested:.2f}")
print(f"   4. Consider ensemble approaches if multiple models perform similarly")

print(f"\nğŸ‰ NESTED CV WITH CATBOOST PIPELINE COMPLETE!")


# %%
# SECTION 10: HYPERPARAMETER ANALYSIS AND STABILITY ASSESSMENT
# =============================================================================
print("\nğŸ”§ HYPERPARAMETER ANALYSIS AND STABILITY ASSESSMENT")
print("-" * 70)

# Analyze hyperparameter consistency across folds for model robustness
for model_name, results in nested_cv_results.items():
    print(f"\nğŸ“‹ {model_name} - Hyperparameter Stability Analysis:")
    print("-" * 50)

    best_params_list = results['best_params_per_fold']

    # Get all unique parameter names
    all_param_names = set()
    for params in best_params_list:
        all_param_names.update(params.keys())

    # Analyze each parameter for consistency
    stable_params = 0
    total_params = len(all_param_names)

    for param_name in sorted(all_param_names):
        param_values = [params.get(param_name, 'N/A') for params in best_params_list]
        unique_values = list(set(param_values))

        if len(unique_values) == 1:
            consistency = "âœ… STABLE"
            stable_params += 1
        elif len(unique_values) == 2:
            consistency = "âš ï¸ MODERATE"
        else:
            consistency = "âŒ UNSTABLE"

        # Show parameter values across folds
        value_counts = Counter(param_values)
        most_common = value_counts.most_common(1)[0]

        print(f"   {param_name:<25}: {consistency}")
        print(f"      Values: {param_values}")
        print(f"      Most frequent: {most_common[0]} ({most_common[1]}/{len(param_values)} folds)")

    # Calculate stability percentage
    stability_pct = (stable_params / total_params) * 100
    print(f"\n   ğŸ“Š Hyperparameter Stability: {stable_params}/{total_params} stable ({stability_pct:.1f}%)")

    if stability_pct >= 80:
        print("   âœ… HIGH STABILITY: Model hyperparameters are robust across data splits")
    elif stability_pct >= 60:
        print("   ğŸ‘ MODERATE STABILITY: Some hyperparameter variation across splits")
    else:
        print("   âš ï¸ LOW STABILITY: High hyperparameter sensitivity to data splits")



# %%
# SECTION 11: NESTED CV RESULTS ANALYSIS
# =============================================================================
print("\nğŸ“Š NESTED CV RESULTS ANALYSIS")
print("=" * 60)

# Create comparison DataFrame
comparison_data = []
for model_name, results in nested_cv_results.items():
    comparison_data.append({
        'Model': model_name,
        'Nested_CV_R2_Mean': results['outer_cv_r2_mean'],
        'Nested_CV_R2_Std': results['outer_cv_r2_std'],
        'Nested_CV_RMSE_Mean': results['outer_cv_rmse_mean'],
        'Nested_CV_RMSE_Std': results['outer_cv_rmse_std'],
        'Nested_CV_MAE_Mean': results['outer_cv_mae_mean'],
        'Nested_CV_MAE_Std': results['outer_cv_mae_std']
    })

nested_comparison_df = pd.DataFrame(comparison_data)
nested_comparison_df = nested_comparison_df.sort_values('Nested_CV_R2_Mean', ascending=False)

print("\nğŸ† NESTED CV PERFORMANCE COMPARISON:")
print("=" * 80)
print(nested_comparison_df.round(4).to_string(index=False))

# Identify best model from nested CV
best_model_nested = nested_comparison_df.iloc[0]['Model']
best_r2_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Mean']
best_r2_std_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Std']

print(f"\nğŸ¥‡ BEST MODEL (Nested CV): {best_model_nested}")
print(f"   ğŸ“Š Unbiased RÂ²: {best_r2_nested:.4f} Â± {best_r2_std_nested:.4f}")
print(f"   ğŸ“Š 95% Confidence Interval: [{best_r2_nested - 1.96*best_r2_std_nested:.4f}, {best_r2_nested + 1.96*best_r2_std_nested:.4f}]")


# %%
# SECTION 12: FINAL MODEL TRAINING WITH AGGREGATED BEST HYPERPARAMETERS
# =============================================================================
print("\nğŸ¯ FINAL MODEL TRAINING WITH AGGREGATED BEST HYPERPARAMETERS")
print("=" * 70)

def get_most_frequent_params(best_params_list):
    """
    Get the most frequently selected hyperparameters across CV folds
    This provides robust hyperparameter selection for the final model
    """
    # Get all parameter names
    all_param_names = set()
    for params in best_params_list:
        all_param_names.update(params.keys())

    # Find most frequent value for each parameter
    final_params = {}
    for param_name in all_param_names:
        param_values = [params.get(param_name) for params in best_params_list if param_name in params]
        if param_values:
            # Get most common value
            most_common = Counter(param_values).most_common(1)[0][0]
            final_params[param_name] = most_common

    return final_params

# Get best hyperparameters for the winning model
best_model_results = nested_cv_results[best_model_nested]
best_params_final = get_most_frequent_params(best_model_results['best_params_per_fold'])

print(f"ğŸ† Training final {best_model_nested} model with aggregated best parameters:")
print("ğŸ“‹ Final hyperparameters (most frequent across CV folds):")
for param, value in sorted(best_params_final.items()):
    print(f"   {param:<25}: {value}")

# Create and train final model on full training data
print(f"\nğŸš€ Training final {best_model_nested} model...")
final_model = base_models[best_model_nested].set_params(**best_params_final)
final_model.fit(X_train_full_scaled, y_train_full)

print(f"âœ… Final {best_model_nested} model trained on full training set")
print(f"   ğŸ“Š Training data: {X_train_full_scaled.shape[0]:,} samples")
print(f"   ğŸ”§ Features: {len(feature_columns)} variables")
print(f"   ğŸ¯ Expected RMSE: ${best_rmse_nested:.2f} Â± ${best_rmse_std_nested:.2f}")


# %%
# SECTION 13: FINAL MODEL EVALUATION ON TEST SET
# =============================================================================
print("\nğŸ¯ FINAL MODEL EVALUATION ON TEST SET")
print("-" * 60)

# Make predictions on test set (held-out data never seen during nested CV)
y_pred_test = final_model.predict(X_test_scaled)

# Calculate comprehensive test metrics
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
test_mae = mean_absolute_error(y_test, y_pred_test)
test_r2 = r2_score(y_test, y_pred_test)

# Calculate additional income-specific metrics
# MAPE for incomes > $100 (avoid division by very small numbers)
valid_mask = y_test > 100
if valid_mask.sum() > 0:
    test_mape = np.mean(np.abs((y_test[valid_mask] - y_pred_test[valid_mask]) / y_test[valid_mask])) * 100
else:
    test_mape = np.nan

print(f"ğŸ† FINAL TEST PERFORMANCE ({best_model_nested}):")
print("=" * 60)
print(f"   ğŸ¯ Test RMSE: ${test_rmse:.2f}")
print(f"   ğŸ¯ Test MAE:  ${test_mae:.2f}")
print(f"   ğŸ“ˆ Test RÂ²:   {test_r2:.4f}")
if not np.isnan(test_mape):
    print(f"   ğŸ’° Test MAPE (>$100): {test_mape:.1f}%")

# Compare with nested CV estimates (validation of nested CV effectiveness)
print(f"\nğŸ“Š NESTED CV vs TEST SET COMPARISON:")
print("-" * 50)
print(f"   Metric    | Nested CV Estimate | Test Set | Difference")
print(f"   ----------|-------------------|----------|----------")
print(f"   RMSE      | ${best_rmse_nested:.2f} Â± ${best_rmse_std_nested:.2f}     | ${test_rmse:.2f}     | ${abs(test_rmse - best_rmse_nested):.2f}")
print(f"   MAE       | ${best_mae_nested:.2f} Â± ${best_mae_std_nested:.2f}     | ${test_mae:.2f}     | ${abs(test_mae - best_mae_nested):.2f}")
print(f"   RÂ²        | {best_r2_nested:.4f} Â± {best_r2_std_nested:.4f} | {test_r2:.4f}   | {abs(test_r2 - best_r2_nested):.4f}")

# Assess nested CV prediction accuracy
rmse_within_ci = abs(test_rmse - best_rmse_nested) <= 2 * best_rmse_std_nested
mae_within_ci = abs(test_mae - best_mae_nested) <= 2 * best_mae_std_nested
r2_within_ci = abs(test_r2 - best_r2_nested) <= 2 * best_r2_std_nested

print(f"\nâœ… NESTED CV VALIDATION:")
print(f"   RMSE within 95% CI: {'âœ… YES' if rmse_within_ci else 'âš ï¸ NO'}")
print(f"   MAE within 95% CI:  {'âœ… YES' if mae_within_ci else 'âš ï¸ NO'}")
print(f"   RÂ² within 95% CI:   {'âœ… YES' if r2_within_ci else 'âš ï¸ NO'}")

if rmse_within_ci and mae_within_ci:
    print("   ğŸ‰ EXCELLENT: Nested CV provided accurate performance estimates!")
elif rmse_within_ci or mae_within_ci:
    print("   ğŸ‘ GOOD: Nested CV estimates reasonably accurate")
else:
    print("   âš ï¸ WARNING: Test performance differs significantly from nested CV estimates")

# Target distribution comparison (from original pipeline)
print(f"\nğŸ“ˆ TARGET DISTRIBUTION COMPARISON:")
print(f"   Training Full - Mean: ${y_train_full.mean():,.2f}, Std: ${y_train_full.std():,.2f}")
print(f"   Test Set      - Mean: ${y_test.mean():,.2f}, Std: ${y_test.std():,.2f}")
print(f"   Predictions   - Mean: ${y_pred_test.mean():,.2f}, Std: ${y_pred_test.std():,.2f}")


# %%
# SECTION 14: FINAL MODEL TRAINING WITH BEST HYPERPARAMETERS
# =============================================================================
print("\nğŸ¯ FINAL MODEL TRAINING WITH BEST HYPERPARAMETERS")
print("=" * 60)

def get_most_frequent_params(best_params_list):
    """
    Get the most frequently selected hyperparameters across CV folds
    """
    from collections import Counter

    # Get all parameter names
    all_param_names = set()
    for params in best_params_list:
        all_param_names.update(params.keys())

    # Find most frequent value for each parameter
    final_params = {}
    for param_name in all_param_names:
        param_values = [params.get(param_name) for params in best_params_list if param_name in params]
        if param_values:
            # Get most common value
            most_common = Counter(param_values).most_common(1)[0][0]
            final_params[param_name] = most_common

    return final_params

# Get best hyperparameters for the winning model
best_model_results = nested_cv_results[best_model_nested]
best_params_final = get_most_frequent_params(best_model_results['best_params_per_fold'])

print(f"ğŸ† Training final {best_model_nested} model with aggregated best parameters:")
print("ğŸ“‹ Final hyperparameters (most frequent across CV folds):")
for param, value in best_params_final.items():
    print(f"   {param}: {value}")

# Create and train final model
final_model = base_models[best_model_nested].set_params(**best_params_final)
final_model.fit(X_train_full_scaled, y_train_full)

print(f"\nâœ… Final {best_model_nested} model trained on full training set")


# %%
# SECTION 15: PERMUTATION IMPORTANCE ANALYSIS
# =============================================================================
print("\nğŸ” PERMUTATION IMPORTANCE ANALYSIS")
print("-" * 60)

# Calculate permutation importance on test set (from original pipeline)
print("ğŸ”„ Calculating permutation importance (this may take a few minutes)...")
perm_importance = permutation_importance(
    final_model,
    X_test_scaled,
    y_test,
    n_repeats=15,  # More repeats for robust estimates
    random_state=42,
    scoring='neg_mean_squared_error'  # Use RMSE-based scoring
)

# Fix the importance calculation - use the raw importance values
# The permutation importance already gives us the decrease in performance
feature_importance_mean = -perm_importance.importances_mean  # Convert from negative to positive
feature_importance_std = perm_importance.importances_std

# Create importance DataFrame
importance_df = pd.DataFrame({
    'feature': feature_columns,
    'importance_mean': feature_importance_mean,
    'importance_std': feature_importance_std
}).sort_values('importance_mean', ascending=False)

print(f"\nğŸ“Š TOP 15 MOST IMPORTANT FEATURES (by MSE increase when permuted):")
print("-" * 60)
for i, (_, row) in enumerate(importance_df.head(15).iterrows(), 1):
    print(f"   {i:2d}. {row['feature']:<30} {row['importance_mean']:>8.4f} Â± {row['importance_std']:>6.4f}")

# Save importance results
importance_df.to_csv(data_path + '/nested_cv_catboost_permutation_importance.csv', index=False)
print(f"\nğŸ’¾ Permutation importance saved to: nested_cv_catboost_permutation_importance.csv")

# Create permutation importance visualization (from original pipeline)
print(f"\nğŸ“ˆ Creating permutation importance visualization...")

plt.figure(figsize=(12, 8))

# Get top 20 features for visualization
num_features_to_plot = min(20, len(feature_importance_mean))
top_features = importance_df.head(num_features_to_plot)

# Create horizontal bar plot
y_pos = np.arange(num_features_to_plot)
bars = plt.barh(y_pos, top_features['importance_mean'],
                xerr=top_features['importance_std'],
                capsize=3, alpha=0.7, color='steelblue')

plt.yticks(y_pos, top_features['feature'])
plt.xlabel('MSE Increase When Feature Permuted')
plt.title(f'Permutation Importance - Top {num_features_to_plot} Features\n({best_model_nested} Model)')
plt.grid(True, alpha=0.3, axis='x')

# Add value labels on bars
for i, (bar, mean_val) in enumerate(zip(bars, top_features['importance_mean'])):
    plt.text(bar.get_width() + max(top_features['importance_mean']) * 0.01,
             bar.get_y() + bar.get_height()/2,
             f'{mean_val:.4f}',
             va='center', fontsize=8)

plt.tight_layout()

# Save plot
plt.savefig(data_path + '/nested_cv_catboost_permutation_importance.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"âœ… Permutation importance visualization saved to: nested_cv_catboost_permutation_importance.png")

# Feature importance insights
print(f"\nğŸ’¡ FEATURE IMPORTANCE INSIGHTS:")
top_5_features = importance_df.head(5)['feature'].tolist()
print(f"   ğŸ” Top 5 features: {', '.join(top_5_features)}")

# Analyze feature types in top 10
top_10_features = importance_df.head(10)['feature'].tolist()
top_basic = [f for f in top_10_features if f in basic_features]
top_age = [f for f in top_10_features if f in age_features]
top_interaction = [f for f in top_10_features if f in interaction_features]

print(f"   ğŸ“Š In top 10 - Basic: {len(top_basic)}, Age: {len(top_age)}, Interaction: {len(top_interaction)}")
print(f"   ğŸ’° Feature importance measured as MSE increase when feature is randomly shuffled")

# Additional analysis: Check for zero importance features
zero_importance = importance_df[importance_df['importance_mean'] <= 0]
if len(zero_importance) > 0:
    print(f"\nâš ï¸  Features with zero or negative importance: {len(zero_importance)}")
    print("   These features may not be contributing to model performance")
else:
    print(f"\nâœ… All features show positive importance")

# Show importance statistics
print(f"\nğŸ“Š IMPORTANCE STATISTICS:")
print(f"   Mean importance: {importance_df['importance_mean'].mean():.4f}")
print(f"   Std importance: {importance_df['importance_mean'].std():.4f}")
print(f"   Max importance: {importance_df['importance_mean'].max():.4f}")
print(f"   Min importance: {importance_df['importance_mean'].min():.4f}")

# %%
# SECTION 16: COMPREHENSIVE VISUALIZATIONS
# =============================================================================
print("\nğŸ“ˆ CREATING COMPREHENSIVE NESTED CV VISUALIZATIONS")
print("-" * 60)

# Create comprehensive visualization dashboard
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('Nested Cross-Validation Results Dashboard (with CatBoost)', fontsize=16, fontweight='bold')

# 1. Model comparison by RMSE (primary metric)
ax1 = axes[0, 0]
models = nested_comparison_df['Model']
rmse_means = nested_comparison_df['Nested_CV_RMSE_Mean']
rmse_stds = nested_comparison_df['Nested_CV_RMSE_Std']

bars = ax1.bar(models, rmse_means, yerr=rmse_stds, capsize=5, alpha=0.7,
               color=['gold' if m == best_model_nested else 'lightblue' for m in models])
ax1.set_ylabel('RMSE ($)')
ax1.set_title('Model Comparison by RMSE (Lower is Better)')
ax1.grid(True, alpha=0.3)
ax1.tick_params(axis='x', rotation=45)

# Add value labels
for bar, mean, std in zip(bars, rmse_means, rmse_stds):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 10,
             f'${mean:.0f}Â±{std:.0f}', ha='center', va='bottom', fontsize=9)

# 2. RMSE scores across CV folds for best model
ax2 = axes[0, 1]
best_rmse_scores = nested_cv_results[best_model_nested]['outer_scores_rmse']
fold_numbers = range(1, len(best_rmse_scores) + 1)

ax2.plot(fold_numbers, best_rmse_scores, 'o-', linewidth=2, markersize=8, color='red')
ax2.axhline(y=np.mean(best_rmse_scores), color='red', linestyle='--', alpha=0.7,
            label=f'Mean: ${np.mean(best_rmse_scores):.0f}')
ax2.fill_between(fold_numbers,
                 np.mean(best_rmse_scores) - np.std(best_rmse_scores),
                 np.mean(best_rmse_scores) + np.std(best_rmse_scores),
                 alpha=0.2, color='red')
ax2.set_xlabel('CV Fold')
ax2.set_ylabel('RMSE ($)')
ax2.set_title(f'{best_model_nested} - RMSE Across CV Folds')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. MAE comparison
ax3 = axes[0, 2]
mae_means = nested_comparison_df['Nested_CV_MAE_Mean']
mae_stds = nested_comparison_df['Nested_CV_MAE_Std']

bars = ax3.bar(models, mae_means, yerr=mae_stds, capsize=5, alpha=0.7,
               color=['coral' if m == best_model_nested else 'lightgreen' for m in models])
ax3.set_ylabel('MAE ($)')
ax3.set_title('Model Comparison by MAE (Lower is Better)')
ax3.grid(True, alpha=0.3)
ax3.tick_params(axis='x', rotation=45)

# 4. Nested CV vs Test Set comparison
ax4 = axes[1, 0]
metrics = ['RMSE', 'MAE', 'RÂ²']
nested_values = [best_rmse_nested, best_mae_nested, best_r2_nested]
test_values = [test_rmse, test_mae, test_r2]

x = np.arange(len(metrics))
width = 0.35

bars1 = ax4.bar(x - width/2, nested_values, width, label='Nested CV', alpha=0.7, color='skyblue')
bars2 = ax4.bar(x + width/2, test_values, width, label='Test Set', alpha=0.7, color='orange')

ax4.set_ylabel('Value')
ax4.set_title('Nested CV vs Test Set Performance')
ax4.set_xticks(x)
ax4.set_xticklabels(metrics)
ax4.legend()
ax4.grid(True, alpha=0.3)

# 5. Prediction vs Actual scatter plot
ax5 = axes[1, 1]
ax5.scatter(y_test, y_pred_test, alpha=0.6, s=20)
min_val = min(y_test.min(), y_pred_test.min())
max_val = max(y_test.max(), y_pred_test.max())
ax5.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
ax5.set_xlabel('Actual Income ($)')
ax5.set_ylabel('Predicted Income ($)')
ax5.set_title(f'Predictions vs Actual ({best_model_nested})')
ax5.legend()
ax5.grid(True, alpha=0.3)

# 6. Residuals plot
ax6 = axes[1, 2]
residuals = y_test - y_pred_test
ax6.scatter(y_pred_test, residuals, alpha=0.6, s=20)
ax6.axhline(y=0, color='r', linestyle='--', linewidth=2)
ax6.set_xlabel('Predicted Income ($)')
ax6.set_ylabel('Residuals ($)')
ax6.set_title('Residuals Plot')
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(data_path + '/nested_cv_catboost_comprehensive_results.png', dpi=300, bbox_inches='tight')
plt.show()

print("   âœ… Comprehensive visualizations saved to: nested_cv_catboost_comprehensive_results.png")


# %%
# SECTION 17: SAVE NESTED CV RESULTS AND ARTIFACTS
# =============================================================================
print("\nğŸ’¾ SAVING NESTED CV RESULTS AND ARTIFACTS (WITH CATBOOST)")
print("-" * 60)

# Create comprehensive nested CV summary
nested_cv_summary = {
    'methodology': {
        'approach': 'Nested Cross-Validation with CatBoost',
        'outer_folds': OUTER_CV_FOLDS,
        'inner_folds': INNER_CV_FOLDS,
        'random_search_iterations': RANDOM_SEARCH_ITERATIONS,
        'primary_metric': 'RMSE',
        'models_evaluated': list(base_models.keys()),
        'total_model_trainings_per_model': OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS
    },
    'nested_cv_results': nested_cv_results,
    'best_model_name': best_model_nested,
    'best_hyperparameters': best_params_final,
    'performance_estimates': {
        'nested_cv_rmse_mean': best_rmse_nested,
        'nested_cv_rmse_std': best_rmse_std_nested,
        'nested_cv_mae_mean': best_mae_nested,
        'nested_cv_mae_std': best_mae_std_nested,
        'nested_cv_r2_mean': best_r2_nested,
        'nested_cv_r2_std': best_r2_std_nested
    },
    'test_performance': {
        'test_rmse': test_rmse,
        'test_mae': test_mae,
        'test_r2': test_r2,
        'test_mape': test_mape if not np.isnan(test_mape) else None
    },
    'model_comparison': nested_comparison_df.to_dict('records'),
    'feature_info': {
        'total_features': len(feature_columns),
        'feature_types': {
            'basic': len(basic_features),
            'age_group': len(age_features),
            'frequency': len(freq_features),
            'interaction': len(interaction_features),
            'other': len(other_features)
        }
    },
    'execution_info': {
        'total_duration_minutes': total_duration,
        'execution_date': datetime.now().isoformat()
    }
}

# Save comprehensive results to JSON
with open(data_path + '/nested_cv_catboost_comprehensive_results.json', 'w') as f:
    # Convert numpy types for JSON serialization
    def convert_numpy(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return obj

    json.dump(nested_cv_summary, f, indent=2, default=convert_numpy)

print("   âœ… Comprehensive results saved: nested_cv_catboost_comprehensive_results.json")

# Save comparison table
nested_comparison_df.to_csv(data_path + '/nested_cv_catboost_model_comparison.csv', index=False)
print("   âœ… Model comparison saved: nested_cv_catboost_model_comparison.csv")

# Save final model artifacts (compatible with original pipeline)
final_model_artifacts = {
    'final_model': final_model,
    'scaler': scaler,
    'feature_columns': feature_columns,
    'model_name': best_model_nested,
    'hyperparameters': best_params_final,
    'nested_cv_performance': {
        'rmse_mean': best_rmse_nested,
        'rmse_std': best_rmse_std_nested,
        'mae_mean': best_mae_nested,
        'mae_std': best_mae_std_nested,
        'r2_mean': best_r2_nested,
        'r2_std': best_r2_std_nested
    },
    'test_performance': {
        'rmse': test_rmse,
        'mae': test_mae,
        'r2': test_r2,
        'mape': test_mape if not np.isnan(test_mape) else None
    },
    'training_info': {
        'cv_method': 'Nested Cross-Validation with CatBoost',
        'outer_folds': OUTER_CV_FOLDS,
        'inner_folds': INNER_CV_FOLDS,
        'optimization_metric': 'RMSE',
        'training_samples': len(X_train_full_scaled),
        'test_samples': len(X_test_scaled),
        'training_date': datetime.now().isoformat()
    }
}

joblib.dump(final_model_artifacts, data_path + '/nested_cv_catboost_final_model.pkl')
print("   âœ… Final model artifacts saved: nested_cv_catboost_final_model.pkl")

print("   ğŸ’¾ Saving enhanced datasets for reference...")
with open(data_path + '/nested_cv_catboost_data_info.txt', 'w') as f:
    f.write("Nested CV with CatBoost Data Information\n")
    f.write("=" * 50 + "\n")
    f.write(f"Training data (full): {X_train_full_scaled.shape}\n")
    f.write(f"Test data: {X_test_scaled.shape}\n")
    f.write(f"Features: {len(feature_columns)}\n")
    f.write(f"Target: {target_column}\n")
    f.write(f"Preprocessing: RobustScaler\n")
    f.write(f"Models evaluated: {', '.join(base_models.keys())}\n")
    f.write(f"Winner: {best_model_nested}\n")

print("   âœ… Data information saved: nested_cv_catboost_data_info.txt")


# %%
# SECTION 18: NESTED CV SUMMARY REPORT (WITH CATBOOST)
# =============================================================================
print("\nğŸ“‹ NESTED CV SUMMARY REPORT (WITH CATBOOST)")
print("=" * 60)

print("ğŸ¯ NESTED CROSS-VALIDATION PIPELINE SUMMARY")
print("=" * 60)

print(f"\nğŸ“Š METHODOLOGY:")
print(f"   â€¢ Outer CV: {OUTER_CV_FOLDS}-fold for unbiased performance estimation")
print(f"   â€¢ Inner CV: {INNER_CV_FOLDS}-fold for hyperparameter tuning")
print(f"   â€¢ Total model trainings: {OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS * len(base_models):,}")
print(f"   â€¢ Hyperparameter search: RandomizedSearchCV ({RANDOM_SEARCH_ITERATIONS} iterations per inner fold)")
print(f"   â€¢ Models evaluated: {', '.join(base_models.keys())}")

print(f"\nğŸ† RESULTS:")
print(f"   â€¢ Best Model: {best_model_nested}")
print(f"   â€¢ Unbiased RMSE Estimate: ${best_rmse_nested:.2f} Â± ${best_rmse_std_nested:.2f}")
print(f"   â€¢ 95% Confidence Interval: [${best_rmse_nested - 1.96*best_rmse_std_nested:.2f}, ${best_rmse_nested + 1.96*best_rmse_std_nested:.2f}]")
print(f"   â€¢ Test Set RMSE: ${test_rmse:.2f}")
print(f"   â€¢ Test Set RÂ²: {test_r2:.4f}")

print(f"\nğŸ“ˆ PERFORMANCE ASSESSMENT:")
if test_rmse <= 600:
    performance_level = "EXCELLENT"
    emoji = "ğŸ‰"
elif test_rmse <= 800:
    performance_level = "GOOD"
    emoji = "âœ…"
elif test_rmse <= 1000:
    performance_level = "ACCEPTABLE"
    emoji = "ğŸ‘"
else:
    performance_level = "NEEDS IMPROVEMENT"
    emoji = "âš ï¸"

print(f"   {emoji} Performance Level: {performance_level}")
print(f"   â€¢ RMSE = ${test_rmse:.2f} {'âœ…' if test_rmse <= 800 else 'âš ï¸'}")

print(f"\nğŸš€ CATBOOST IMPACT:")
if 'CatBoost' in nested_cv_results:
    catboost_rank = nested_comparison_df[nested_comparison_df['Model'] == 'CatBoost'].index[0] + 1
    print(f"   â€¢ CatBoost Ranking: #{catboost_rank} out of {len(base_models)} models")
    if catboost_rank == 1:
        print(f"   ğŸ¥‡ CatBoost WINNER! Excellent performance with categorical features")
    elif catboost_rank <= 2:
        print(f"   ğŸ¥ˆ CatBoost performed excellently, very competitive")
    else:
        print(f"   ğŸ“Š CatBoost provided valuable comparison perspective")

print(f"\nğŸ’¾ PRODUCTION READY:")
print(f"   â€¢ Final model: {best_model_nested}")
print(f"   â€¢ Frequency mappings: Saved for production use")
print(f"   â€¢ Expected RMSE: ${best_rmse_nested:.2f}")
print(f"   â€¢ Model artifacts: Saved for deployment")

print(f"\nâœ… NESTED CROSS-VALIDATION WITH CATBOOST COMPLETED SUCCESSFULLY!")
print("=" * 80)


# %%
# SECTION 19: PRODUCTION MODEL TRAINING (OPTIONAL - USING ALL DATA)
# =============================================================================
print("\nğŸš€ PRODUCTION MODEL TRAINING (USING ALL AVAILABLE DATA)")
print("=" * 70)
print("Training final production model using ALL available data (train + validation + test)...")
print("Using best hyperparameters from nested CV analysis.")

# Combine ALL data for final production model
X_all_data = pd.concat([X_train_full, X_test], ignore_index=True)
y_all_data = pd.concat([y_train_full, y_test], ignore_index=True)

print(f"\nğŸ“Š PRODUCTION TRAINING DATA:")
print(f"   â€¢ Total samples: {X_all_data.shape[0]:,}")
print(f"   â€¢ Features: {X_all_data.shape[1]}")
print(f"   â€¢ Target mean: ${y_all_data.mean():,.2f}")
print(f"   â€¢ Target std: ${y_all_data.std():,.2f}")

# Scale all data using the same scaler fitted on training data
X_all_data_scaled = pd.DataFrame(
    scaler.transform(X_all_data),
    columns=X_all_data.columns,
    index=X_all_data.index
)

# Train final production model
print(f"\nğŸ”§ Training final production {best_model_nested} model...")
production_model = base_models[best_model_nested].set_params(**best_params_final)
production_model.fit(X_all_data_scaled, y_all_data)

print(f"âœ… Production {best_model_nested} model trained successfully!")

# Save production model
production_model_artifacts = {
    'production_model': production_model,
    'scaler': scaler,
    'feature_columns': feature_columns,
    'model_name': best_model_nested,
    'hyperparameters': best_params_final,
    'training_data_size': len(X_all_data_scaled),
    'expected_performance': {
        'rmse_estimate': best_rmse_nested,
        'rmse_std': best_rmse_std_nested,
        'r2_estimate': best_r2_nested
    },
    'training_info': {
        'training_date': datetime.now().isoformat(),
        'data_used': 'All available data (train + validation + test)',
        'cv_method': 'Nested CV with CatBoost for hyperparameter selection'
    }
}

joblib.dump(production_model_artifacts, data_path + '/production_model_catboost_all_data.pkl')
print(f"ğŸ’¾ Production model saved: production_model_catboost_all_data.pkl")

print(f"\nğŸ¯ PRODUCTION DEPLOYMENT READY:")
print(f"   â€¢ Model: {best_model_nested}")
print(f"   â€¢ Training samples: {len(X_all_data_scaled):,}")
print(f"   â€¢ Expected RMSE: ${best_rmse_nested:.2f}")
print(f"   â€¢ Frequency mappings: Available for new predictions")
print(f"   â€¢ Artifacts saved: Ready for deployment")

# =============================================================================
# CALCULATE CONFIDENCE INTERVALS FOR PRODUCTION PREDICTIONS
# =============================================================================
print("\nğŸ”¢ CALCULATING CONFIDENCE INTERVALS FOR PREDICTIONS")
print("-" * 50)

# Make predictions on the full training dataset to calculate residuals
print("   ğŸ“Š Making predictions on training data for residual calculation...")
y_train_pred = production_model.predict(X_all_data_scaled)

# Calculate residuals (actual - predicted)
residuals = y_all_data - y_train_pred
print(f"   âœ… Residuals calculated: {len(residuals)} samples")

# Calculate confidence interval offsets using quantile method
# This avoids normal distribution assumptions and uses actual residual distribution
confidence_level = 0.90  # 90% confidence interval
lower_percentile = (1 - confidence_level) / 2 * 100  # 5th percentile
upper_percentile = (1 + confidence_level) / 2 * 100  # 95th percentile

ci_lower_offset = np.percentile(residuals, lower_percentile)
ci_upper_offset = np.percentile(residuals, upper_percentile)

print(f"   ğŸ“ˆ Confidence Level: {confidence_level*100:.0f}%")
print(f"   ğŸ“‰ Lower offset (5th percentile): ${ci_lower_offset:.2f}")
print(f"   ğŸ“ˆ Upper offset (95th percentile): ${ci_upper_offset:.2f}")
print(f"   ğŸ“Š Average CI width: ${ci_upper_offset - ci_lower_offset:.2f}")

# Store confidence interval information
confidence_intervals = {
    'confidence_level': confidence_level,
    'ci_lower_offset': ci_lower_offset,
    'ci_upper_offset': ci_upper_offset,
    'method': 'quantile_based',
    'training_coverage_note': f'Based on {len(residuals)} training samples',
    'residual_stats': {
        'mean': np.mean(residuals),
        'std': np.std(residuals),
        'min': np.min(residuals),
        'max': np.max(residuals)
    }
}

print(f"   âœ… Confidence interval calculation complete!")

# Update production model artifacts with confidence intervals
production_model_artifacts['confidence_intervals'] = confidence_intervals
production_model_artifacts['prediction_usage'] = {
    'point_prediction': 'Use production_model.predict(X)',
    'confidence_interval': f'Add/subtract offsets: [prediction + {ci_lower_offset:.2f}, prediction + {ci_upper_offset:.2f}]',
    'confidence_level': f'{confidence_level*100:.0f}%',
    'interpretation': 'Prediction Â± confidence interval gives range where true value likely falls'
}

# Re-save with confidence intervals
joblib.dump(production_model_artifacts, data_path + '/production_model_catboost_all_data.pkl')
print(f"ğŸ’¾ Production model updated with confidence intervals")

print(f"\nğŸ¯ PRODUCTION PREDICTION USAGE:")
print(f"   1. Point prediction: production_model.predict(X)")
print(f"   2. Lower bound: prediction + {ci_lower_offset:.2f}")
print(f"   3. Upper bound: prediction + {ci_upper_offset:.2f}")
print(f"   4. Confidence level: {confidence_level*100:.0f}%")
print(f"   5. Example: If prediction = $1000, range = [${1000 + ci_lower_offset:.0f}, ${1000 + ci_upper_offset:.0f}]")

print(f"\nğŸ‰ COMPLETE NESTED CV PIPELINE WITH CATBOOST FINISHED!")
print("=" * 80)
