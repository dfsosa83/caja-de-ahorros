# %%
# =============================================================================
# INCOME PREDICTION MODEL - NESTED CROSS-VALIDATION PIPELINE WITH CATBOOST
# =============================================================================
# Goal: Predict customer income using NESTED CV for unbiased performance estimates
# Based on: model_process_with_no_tranformations.txt preprocessing structure
#
# Key Features:
# - Nested CV Structure: Outer CV (5 folds) for evaluation, Inner CV (3 folds) for hyperparameter tuning
# - Robust Metrics: Primary focus on RMSE/MAE instead of R¬≤ for better income prediction assessment
# - Complete Pipeline: From preprocessed data to production-ready model
# - Comprehensive Analysis: Permutation importance, visualizations, model comparison
#
# Models: XGBoost, LightGBM, Random Forest, CatBoost
# =============================================================================

# %%
# SECTION 1: IMPORTS AND SETUP
# =============================================================================
# NOTE: Before running this notebook, install CatBoost:
# pip install catboost
# or
# conda install -c conda-forge catboost

import pandas as pd
import numpy as np
import warnings
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import (KFold, GroupKFold, cross_val_score,
                                   RandomizedSearchCV)
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.inspection import permutation_importance
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
from collections import Counter

warnings.filterwarnings('ignore')

print("================================================================================")
print("NESTED CROSS-VALIDATION INCOME PREDICTION MODEL WITH CATBOOST - STARTED")
print("================================================================================")
print("üìã Pipeline based on: model_process_with_no_tranformations.txt")
print("üéØ Primary Metrics: RMSE (Root Mean Square Error) and MAE (Mean Absolute Error)")
print("üîÑ Nested CV: Outer (5-fold) for evaluation, Inner (3-fold) for hyperparameter tuning")
print("üöÄ Models: Linear Regression (Baseline), XGBoost, LightGBM, Random Forest, CatBoost")

# %%
# SECTION 2: PREPARE FINAL DATASETS FOR NESTED CV MODELING
# =============================================================================
print("\nüéØ PREPARING FINAL DATASETS FOR NESTED CV")
print("-" * 60)

# EXPLICIT FEATURE SELECTION APPROACH (from original pipeline)
id_columns = ['cliente', 'identificador_unico']
target_column = 'ingresos_reportados'

# Use the selected features from the original pipeline
# NOTE: Adjust 'selected_features_final' to your actual feature list variable
feature_columns = selected_features_final

# Verify all selected features exist in the dataset
available_features = []
missing_features = []

for feature in feature_columns:
    if feature in train_df_enhanced.columns:
        available_features.append(feature)
    else:
        missing_features.append(feature)

if missing_features:
    print(f"‚ö†Ô∏è  Missing features (will be skipped): {missing_features}")

feature_columns = available_features

print(f"   üìä Selected feature columns: {len(feature_columns)}")
print(f"   üéØ Target column: {target_column}")

# Create feature matrices and targets (from original pipeline structure)
X_train = train_df_enhanced[feature_columns].copy()
y_train = train_df_enhanced[target_column].copy()

X_valid = valid_df_enhanced[feature_columns].copy()
y_valid = valid_df_enhanced[target_column].copy()

X_test = test_df_enhanced[feature_columns].copy()
y_test = test_df_enhanced[target_column].copy()

print(f"\nüìà DATASET SHAPES:")
print(f"   X_train: {X_train.shape}")
print(f"   X_valid: {X_valid.shape}")
print(f"   X_test: {X_test.shape}")

# Combine train and validation for nested CV (test set remains untouched)
X_train_full = pd.concat([X_train, X_valid], ignore_index=True)
y_train_full = pd.concat([y_train, y_valid], ignore_index=True)

print(f"   X_train_full (for nested CV): {X_train_full.shape}")
print(f"   X_test (held out): {X_test.shape}")

# Show selected features grouped by type
print(f"\nüìã SELECTED FEATURES ({len(feature_columns)} features):")
print("-" * 60)

# Group features by type for better readability (from original pipeline)
basic_features = []
age_features = []
freq_features = []
interaction_features = []
other_features = []

for feature in feature_columns:
    if feature.startswith('age_group_'):
        age_features.append(feature)
    elif feature.endswith('_freq'):
        freq_features.append(feature)
    elif '_x_' in feature or 'retired_x_' in feature or 'employer_x_' in feature or 'gender_x_' in feature:
        interaction_features.append(feature)
    elif feature in ['edad', 'letras_mensuales', 'monto_letra', 'saldo', 'is_retired']:
        basic_features.append(feature)
    else:
        other_features.append(feature)

print(f"üî¢ BASIC FEATURES ({len(basic_features)}): {basic_features}")
print(f"üë• AGE GROUP FEATURES ({len(age_features)}): {age_features}")
print(f"üìä FREQUENCY FEATURES ({len(freq_features)}): {freq_features}")
print(f"‚ö° INTERACTION FEATURES ({len(interaction_features)}): {interaction_features}")
print(f"üîß OTHER FEATURES ({len(other_features)}): {other_features}")

# Verify data quality
print(f"\n‚úÖ DATA QUALITY CHECKS:")
print(f"   Missing values in X_train_full: {X_train_full.isnull().sum().sum()}")
print(f"   Missing values in y_train_full: {y_train_full.isnull().sum()}")
print(f"   All features numeric: {all(X_train_full.dtypes.apply(lambda x: x in ['int64', 'float64']))}")

# Save feature list to file for reference
feature_list_df = pd.DataFrame({
    'feature_name': feature_columns,
    'feature_type': ['basic' if f in basic_features else
                    'age_group' if f in age_features else
                    'frequency' if f in freq_features else
                    'interaction' if f in interaction_features else
                    'other' for f in feature_columns]
})

feature_list_df.to_csv(data_path + '/nested_cv_catboost_feature_list.csv', index=False)
print(f"\nüíæ Feature list saved to: nested_cv_catboost_feature_list.csv")

# %%
# SECTION 2.5: EXTRACT AND SAVE FREQUENCY MAPPINGS FOR PRODUCTION
# =============================================================================
print("\nüíæ EXTRACTING FREQUENCY MAPPINGS FOR PRODUCTION DEPLOYMENT")
print("-" * 60)

# Extract frequency mappings from the training data for production use
# These mappings ensure consistent encoding when predicting single customers

frequency_mappings = {}

# Find frequency-encoded features in your selected features
freq_features_in_model = [f for f in feature_columns if f.endswith('_freq')]

print(f"   üìä Found {len(freq_features_in_model)} frequency-encoded features:")
for freq_feature in freq_features_in_model:
    print(f"      ‚Ä¢ {freq_feature}")

# Extract mappings from the enhanced training dataframes
# We'll use the full training data (train + validation) to get complete mappings
print(f"\nüîç Extracting frequency mappings from training data...")

for freq_feature in freq_features_in_model:
    if freq_feature in train_df_enhanced.columns:
        # Get the original categorical column name
        original_col = freq_feature.replace('_freq', '')
        
        if original_col in train_df_enhanced.columns:
            # Extract the frequency mapping from training data
            freq_mapping = train_df_enhanced[original_col].value_counts().to_dict()
            frequency_mappings[freq_feature] = freq_mapping
            
            print(f"   ‚úÖ {freq_feature}:")
            print(f"      Original column: {original_col}")
            print(f"      Categories: {len(freq_mapping)}")
            print(f"      Top 3: {list(freq_mapping.keys())[:3]}")
        else:
            print(f"   ‚ö†Ô∏è Original column '{original_col}' not found for {freq_feature}")
    else:
        print(f"   ‚ö†Ô∏è Frequency feature '{freq_feature}' not found in training data")

# Save frequency mappings for production use
import pickle
import json

# Save as pickle for Python production systems
frequency_mappings_path = data_path + '/production_frequency_mappings_catboost.pkl'
with open(frequency_mappings_path, 'wb') as f:
    pickle.dump(frequency_mappings, f)

# Save as JSON for cross-platform compatibility
frequency_mappings_json_path = data_path + '/production_frequency_mappings_catboost.json'
with open(frequency_mappings_json_path, 'w') as f:
    json.dump(frequency_mappings, f, indent=2)

print(f"\nüíæ FREQUENCY MAPPINGS SAVED:")
print(f"   üì¶ Pickle format: production_frequency_mappings_catboost.pkl")
print(f"   üìÑ JSON format: production_frequency_mappings_catboost.json")
print(f"   üéØ Total mappings: {len(frequency_mappings)}")

# Create a summary of the mappings for documentation
mappings_summary = {}
for freq_feature, mapping in frequency_mappings.items():
    mappings_summary[freq_feature] = {
        'total_categories': len(mapping),
        'top_5_categories': dict(list(mapping.items())[:5]),
        'min_frequency': min(mapping.values()),
        'max_frequency': max(mapping.values()),
        'others_frequency': mapping.get('Others', 'Not found')
    }

# Save summary for documentation
summary_path = data_path + '/frequency_mappings_summary_catboost.json'
with open(summary_path, 'w') as f:
    json.dump(mappings_summary, f, indent=2)

print(f"   üìã Summary saved: frequency_mappings_summary_catboost.json")

print(f"\nüéØ PRODUCTION USAGE:")
print(f"   1. Load mappings: frequency_mappings = pickle.load(open('production_frequency_mappings_catboost.pkl', 'rb'))")
print(f"   2. Apply to new customer: customer['ocupacion_consolidated_freq'] = frequency_mappings['ocupacion_consolidated_freq'][customer['ocupacion_consolidated']]")
print(f"   3. Handle new values: Map unknown categories to 'Others' frequency")

print(f"\n‚úÖ FREQUENCY MAPPINGS EXTRACTION COMPLETE!")

# %%
# SECTION 3: FEATURE SCALING
# =============================================================================
print("\n‚öñÔ∏è FEATURE SCALING")
print("-" * 50)

# Apply robust scaling to features (from original pipeline)
print("   ‚öñÔ∏è Applying RobustScaler...")
scaler = RobustScaler()

# Fit scaler on full training data (train + validation combined)
X_train_full_scaled = pd.DataFrame(
    scaler.fit_transform(X_train_full),
    columns=X_train_full.columns,
    index=X_train_full.index
)

# Transform test set using the same scaler
X_test_scaled = pd.DataFrame(
    scaler.transform(X_test),
    columns=X_test.columns,
    index=X_test.index
)

print("   ‚úÖ Feature scaling complete")
print(f"   üìä Scaled training data: {X_train_full_scaled.shape}")
print(f"   üìä Scaled test data: {X_test_scaled.shape}")

# %%
# SECTION 4: NESTED CROSS-VALIDATION SETUP
# =============================================================================
print("\nüîÑ NESTED CROSS-VALIDATION SETUP")
print("-" * 60)

# Define CV strategies with robust metrics focus
OUTER_CV_FOLDS = 5  # For unbiased performance estimation
INNER_CV_FOLDS = 3  # For hyperparameter tuning
RANDOM_SEARCH_ITERATIONS = 80  # Increased for better hyperparameter search

# Create CV objects
outer_cv = KFold(n_splits=OUTER_CV_FOLDS, shuffle=True, random_state=42)
inner_cv = KFold(n_splits=INNER_CV_FOLDS, shuffle=True, random_state=42)

print(f"   üîÑ Outer CV: {OUTER_CV_FOLDS} folds (for unbiased model evaluation)")
print(f"   üîÑ Inner CV: {INNER_CV_FOLDS} folds (for hyperparameter tuning)")
print(f"   üîç Random Search: {RANDOM_SEARCH_ITERATIONS} iterations per inner fold")
print(f"   üìä Total model trainings: {OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS} per model")
print(f"      ({OUTER_CV_FOLDS} outer √ó {INNER_CV_FOLDS} inner √ó {RANDOM_SEARCH_ITERATIONS} iterations)")


# %%
# SECTION 5: MODEL DEFINITIONS AND HYPERPARAMETER GRIDS WITH CATBOOST
# =============================================================================
print("\nü§ñ MODEL DEFINITIONS AND HYPERPARAMETER GRIDS WITH CATBOOST")
print("-" * 60)

# Base models (from simple to complex: Linear Regression ‚Üí Tree-based ‚Üí Gradient Boosting)
base_models = {
    'Linear Regression': LinearRegression(n_jobs=-1),
    'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1),
    'XGBoost': xgb.XGBRegressor(random_state=42, n_jobs=-1),
    'LightGBM': lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),
    'CatBoost': cb.CatBoostRegressor(random_state=42, verbose=False, thread_count=-1)
}

# Comprehensive hyperparameter grids for nested CV
# These grids are designed for income prediction optimization
param_grids = {
    'Linear Regression': {
        # Linear Regression has no hyperparameters to tune
        # We include an empty dict to maintain consistency with the nested CV framework
    },
    'Random Forest': {
        'n_estimators': [200, 300, 400],
        'max_depth': [10, 15, 20, None],
        'min_samples_split': [5, 10, 15],
        'min_samples_leaf': [2, 5, 10],
        'max_features': ['sqrt', 'log2', 0.8],
        'max_samples': [0.7, 0.8, 0.9]
    },
    'XGBoost': {
        'n_estimators': [300, 800, 1100],
        'max_depth': [6, 8, 10],
        'learning_rate': [0.005, 0.007, 0.01],
        'subsample': [0.8, 0.85, 0.9],
        'colsample_bytree': [0.8, 0.85, 0.9],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [0.5, 1.0, 2.0],
        'min_child_weight': [1, 3, 5]
    },
    'LightGBM': {
        'n_estimators': [300, 800, 1100],
        'max_depth': [6, 8, 10],
        'learning_rate': [0.005, 0.007, 0.01],
        'subsample': [0.7, 0.8, 0.85],
        'colsample_bytree': [0.7, 0.8, 0.85],
        'num_leaves': [30, 50, 80],
        'min_child_samples': [30, 40, 50],
        'reg_alpha': [0.5, 1.0, 1.5],
        'reg_lambda': [2.0, 3.0, 4.0]
        },
    'CatBoost': {
        'iterations': [300, 800, 1100],
        'depth': [6, 8, 10],
        'learning_rate': [0.005, 0.007, 0.01],
        'subsample': [0.8, 0.85, 0.9],
        'colsample_bylevel': [0.8, 0.85, 0.9],
        'l2_leaf_reg': [1, 3, 5],
        'border_count': [32, 64, 128],
        'bagging_temperature': [0, 0.5, 1.0]
    }
}

print(f"   ü§ñ Models defined: {list(base_models.keys())}")
print(f"   üîß Hyperparameter search space per model:")
for model_name, grid in param_grids.items():
    total_combinations = np.prod([len(values) for values in grid.values()])
    print(f"      {model_name}: {total_combinations:,} total combinations")

print(f"\nüí° Primary Optimization Metric: RMSE (Root Mean Square Error)")
print(f"   üìä RMSE penalizes large prediction errors more heavily")
print(f"   üí∞ Better suited for income prediction than R¬≤ alone")
print(f"   üìà Lower RMSE = Better model performance")
print(f"\nüöÄ CatBoost Advantages:")
print(f"   üéØ Excellent with categorical features (even after encoding)")
print(f"   üîß Built-in regularization and overfitting protection")
print(f"   ‚ö° Often competitive with XGBoost/LightGBM")
print(f"   üìä Robust hyperparameter defaults")


# %%
# SECTION 6: NESTED CROSS-VALIDATION IMPLEMENTATION
# =============================================================================
print("\nüéØ NESTED CROSS-VALIDATION IMPLEMENTATION")
print("=" * 70)

def nested_cross_validation(model, param_grid, X, y, outer_cv, inner_cv, model_name):
    """
    Perform nested cross-validation for unbiased model evaluation with robust metrics focus

    This implementation prioritizes RMSE and MAE as primary metrics for income prediction,
    while still tracking R¬≤ for comparison purposes.

    Args:
        model: Base model to evaluate
        param_grid: Hyperparameter grid for tuning
        X, y: Training data and target
        outer_cv, inner_cv: Cross-validation objects
        model_name: Name for reporting

    Returns:
        dict: Comprehensive nested CV results with robust metrics
    """
    print(f"\nüîÑ Starting Nested CV for {model_name}")
    print(f"   üìä Outer folds: {outer_cv.n_splits}, Inner folds: {inner_cv.n_splits}")
    print(f"   üéØ Primary metrics: RMSE (lower is better), MAE (lower is better)")

    # Storage for results - prioritizing robust metrics
    outer_scores_rmse = []
    outer_scores_mae = []
    outer_scores_r2 = []  # Still track R¬≤ for comparison
    best_params_per_fold = []
    fold_details = []

    # Outer CV loop - each iteration gives unbiased performance estimate
    for fold_idx, (train_idx, val_idx) in enumerate(outer_cv.split(X, y), 1):
        print(f"\n   üîÑ Outer Fold {fold_idx}/{outer_cv.n_splits}")

        # Split data for this outer fold
        X_train_outer = X.iloc[train_idx]
        X_val_outer = X.iloc[val_idx]
        y_train_outer = y.iloc[train_idx]
        y_val_outer = y.iloc[val_idx]

        print(f"      üìä Fold {fold_idx} sizes: Train={len(train_idx)}, Val={len(val_idx)}")

        # Inner CV: Hyperparameter tuning using RMSE as optimization metric
        print(f"      üîß Inner CV: Hyperparameter tuning (optimizing RMSE)...")

        # Handle Linear Regression (no hyperparameters to tune)
        if model_name == 'Linear Regression':
            # Linear Regression has no hyperparameters, so just fit the model
            best_model = model
            best_model.fit(X_train_outer, y_train_outer)
            best_params = {}  # No parameters to store
            best_params_per_fold.append(best_params)

            # Calculate RMSE on inner CV for consistency
            inner_cv_scores = cross_val_score(model, X_train_outer, y_train_outer,
                                            cv=inner_cv, scoring='neg_mean_squared_error', n_jobs=-1)
            best_inner_rmse = np.sqrt(-inner_cv_scores.mean())
            print(f"      ‚úÖ Linear Regression CV RMSE: ${best_inner_rmse:.2f} (no hyperparameters to tune)")
        else:
            # For other models, perform hyperparameter search
            random_search = RandomizedSearchCV(
                estimator=model,
                param_distributions=param_grid,
                n_iter=RANDOM_SEARCH_ITERATIONS,  # Use global variable
                cv=inner_cv,
                scoring='neg_mean_squared_error',  # Optimizing RMSE (MSE negated)
                n_jobs=-1,
                random_state=42,
                verbose=0
            )

            # Fit hyperparameter search on outer training data
            random_search.fit(X_train_outer, y_train_outer)

            # Get best model from inner CV
            best_model = random_search.best_estimator_
            best_params = random_search.best_params_
            best_params_per_fold.append(best_params)

            # Convert negative MSE back to RMSE for reporting
            best_inner_rmse = np.sqrt(-random_search.best_score_)
            print(f"      ‚úÖ Best inner CV RMSE: ${best_inner_rmse:.2f}")

        # Evaluate best model on outer validation fold
        y_pred_outer = best_model.predict(X_val_outer)

        # Calculate comprehensive metrics for this outer fold
        fold_rmse = np.sqrt(mean_squared_error(y_val_outer, y_pred_outer))
        fold_mae = mean_absolute_error(y_val_outer, y_pred_outer)
        fold_r2 = r2_score(y_val_outer, y_pred_outer)

        # Store scores
        outer_scores_rmse.append(fold_rmse)
        outer_scores_mae.append(fold_mae)
        outer_scores_r2.append(fold_r2)

        print(f"      üìä Outer fold performance:")
        print(f"         üéØ RMSE: ${fold_rmse:.2f}")
        print(f"         üéØ MAE:  ${fold_mae:.2f}")
        print(f"         üìà R¬≤:   {fold_r2:.4f}")

        # Calculate additional income-specific metrics
        # Mean Absolute Percentage Error (MAPE) - but handle low incomes carefully
        valid_mask = y_val_outer > 100  # Avoid division by very small numbers
        if valid_mask.sum() > 0:
            mape = np.mean(np.abs((y_val_outer[valid_mask] - y_pred_outer[valid_mask]) / y_val_outer[valid_mask])) * 100
            print(f"         üí∞ MAPE (>$100): {mape:.1f}%")
        else:
            mape = np.nan

        # Store detailed results for this fold
        fold_details.append({
            'fold': fold_idx,
            'rmse': fold_rmse,
            'mae': fold_mae,
            'r2': fold_r2,
            'mape': mape,
            'best_params': best_params,
            'inner_cv_rmse': best_inner_rmse,
            'train_size': len(train_idx),
            'val_size': len(val_idx),
            'y_true_mean': y_val_outer.mean(),
            'y_pred_mean': y_pred_outer.mean()
        })

    # Calculate final nested CV results with robust metrics emphasis
    nested_cv_results = {
        'model_name': model_name,
        # Primary metrics (RMSE/MAE)
        'outer_cv_rmse_mean': np.mean(outer_scores_rmse),
        'outer_cv_rmse_std': np.std(outer_scores_rmse),
        'outer_cv_mae_mean': np.mean(outer_scores_mae),
        'outer_cv_mae_std': np.std(outer_scores_mae),
        # Secondary metric (R¬≤)
        'outer_cv_r2_mean': np.mean(outer_scores_r2),
        'outer_cv_r2_std': np.std(outer_scores_r2),
        # Raw scores for analysis
        'outer_scores_rmse': outer_scores_rmse,
        'outer_scores_mae': outer_scores_mae,
        'outer_scores_r2': outer_scores_r2,
        # Hyperparameter analysis
        'best_params_per_fold': best_params_per_fold,
        'fold_details': fold_details,
        # Model selection criteria (lower is better for RMSE/MAE)
        'selection_metric': 'rmse',  # Primary metric for model selection
        'selection_score': np.mean(outer_scores_rmse)
    }

    print(f"\n   üèÜ {model_name} Nested CV Summary:")
    print(f"      üéØ RMSE: ${nested_cv_results['outer_cv_rmse_mean']:.2f} ¬± ${nested_cv_results['outer_cv_rmse_std']:.2f}")
    print(f"      üéØ MAE:  ${nested_cv_results['outer_cv_mae_mean']:.2f} ¬± ${nested_cv_results['outer_cv_mae_std']:.2f}")
    print(f"      üìà R¬≤:   {nested_cv_results['outer_cv_r2_mean']:.4f} ¬± {nested_cv_results['outer_cv_r2_std']:.4f}")

    return nested_cv_results

print("‚úÖ Nested CV function defined with robust metrics focus")

# %%
# SECTION 7: RUN NESTED CROSS-VALIDATION FOR ALL MODELS (INCLUDING CATBOOST)
# =============================================================================
print("\nüöÄ RUNNING NESTED CROSS-VALIDATION FOR ALL MODELS (BASELINE TO ADVANCED)")
print("=" * 70)
print("‚ö†Ô∏è  This will take 30-90 minutes depending on your hardware...")
print(f"   Each complex model will be trained {OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS} times")
print(f"   ({OUTER_CV_FOLDS} outer √ó {INNER_CV_FOLDS} inner √ó {RANDOM_SEARCH_ITERATIONS} iterations)")
print("üéØ Optimizing for RMSE (Root Mean Square Error) - best metric for income prediction")
print("üìä Model progression: Linear Regression (baseline) ‚Üí Tree-based ‚Üí Gradient Boosting")
print("üöÄ Including CatBoost for comprehensive model comparison!")

# Storage for all nested CV results
nested_cv_results = {}
total_start_time = datetime.now()

# Run nested CV for each model
for model_idx, (model_name, base_model) in enumerate(base_models.items(), 1):
    print(f"\n{'='*80}")
    print(f"NESTED CV {model_idx}/{len(base_models)}: {model_name.upper()}")
    print(f"{'='*80}")

    model_start_time = datetime.now()

    # Run nested cross-validation
    results = nested_cross_validation(
        model=base_model,
        param_grid=param_grids[model_name],
        X=X_train_full_scaled,  # Use full training data (train + validation)
        y=y_train_full,
        outer_cv=outer_cv,
        inner_cv=inner_cv,
        model_name=model_name
    )

    nested_cv_results[model_name] = results

    model_end_time = datetime.now()
    model_duration = (model_end_time - model_start_time).total_seconds() / 60

    print(f"\n   ‚è±Ô∏è {model_name} completed in {model_duration:.1f} minutes")

    # Show progress
    remaining_models = len(base_models) - model_idx
    if remaining_models > 0:
        estimated_remaining = model_duration * remaining_models
        print(f"   üìä Progress: {model_idx}/{len(base_models)} models complete")
        print(f"   ‚è∞ Estimated remaining time: {estimated_remaining:.1f} minutes")

total_end_time = datetime.now()
total_duration = (total_end_time - total_start_time).total_seconds() / 60

print(f"\nüéâ ALL NESTED CV COMPLETED (BASELINE TO ADVANCED MODELS)!")
print(f"‚è±Ô∏è Total execution time: {total_duration:.1f} minutes")

# %%
# SECTION 8: NESTED CV RESULTS ANALYSIS AND MODEL COMPARISON (BASELINE TO ADVANCED)
# =============================================================================
print("\nüìä NESTED CV RESULTS ANALYSIS AND MODEL COMPARISON (BASELINE TO ADVANCED)")
print("=" * 70)

# Create comprehensive comparison DataFrame with robust metrics focus
comparison_data = []
for model_name, results in nested_cv_results.items():
    comparison_data.append({
        'Model': model_name,
        # Primary metrics (lower is better)
        'Nested_CV_RMSE_Mean': results['outer_cv_rmse_mean'],
        'Nested_CV_RMSE_Std': results['outer_cv_rmse_std'],
        'Nested_CV_MAE_Mean': results['outer_cv_mae_mean'],
        'Nested_CV_MAE_Std': results['outer_cv_mae_std'],
        # Secondary metric
        'Nested_CV_R2_Mean': results['outer_cv_r2_mean'],
        'Nested_CV_R2_Std': results['outer_cv_r2_std'],
        # Selection score (RMSE for ranking)
        'Selection_Score': results['selection_score']
    })

nested_comparison_df = pd.DataFrame(comparison_data)
# Sort by RMSE (lower is better) - primary metric for income prediction
nested_comparison_df = nested_comparison_df.sort_values('Nested_CV_RMSE_Mean', ascending=True)

print("\nüèÜ NESTED CV PERFORMANCE COMPARISON (Sorted by RMSE - Lower is Better):")
print("=" * 90)
print(nested_comparison_df.round(4).to_string(index=False))

# Identify best model based on RMSE (robust metric for income prediction)
best_model_nested = nested_comparison_df.iloc[0]['Model']
best_rmse_nested = nested_comparison_df.iloc[0]['Nested_CV_RMSE_Mean']
best_rmse_std_nested = nested_comparison_df.iloc[0]['Nested_CV_RMSE_Std']
best_mae_nested = nested_comparison_df.iloc[0]['Nested_CV_MAE_Mean']
best_mae_std_nested = nested_comparison_df.iloc[0]['Nested_CV_MAE_Std']
best_r2_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Mean']
best_r2_std_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Std']

print(f"\nü•á BEST MODEL (Based on RMSE): {best_model_nested}")
print(f"   üéØ Unbiased RMSE: ${best_rmse_nested:.2f} ¬± ${best_rmse_std_nested:.2f}")
print(f"   üéØ Unbiased MAE:  ${best_mae_nested:.2f} ¬± ${best_mae_std_nested:.2f}")
print(f"   üìà Unbiased R¬≤:   {best_r2_nested:.4f} ¬± {best_r2_std_nested:.4f}")

# Calculate confidence intervals for RMSE (primary metric)
rmse_ci_lower = best_rmse_nested - 1.96 * best_rmse_std_nested
rmse_ci_upper = best_rmse_nested + 1.96 * best_rmse_std_nested
print(f"   üìä 95% Confidence Interval (RMSE): [${rmse_ci_lower:.2f}, ${rmse_ci_upper:.2f}]")

# Calculate confidence intervals for MAE and R¬≤ as well
mae_ci_lower = best_mae_nested - 1.96 * best_mae_std_nested
mae_ci_upper = best_mae_nested + 1.96 * best_mae_std_nested
print(f"   üìä 95% Confidence Interval (MAE): [${mae_ci_lower:.2f}, ${mae_ci_upper:.2f}]")

r2_ci_lower = best_r2_nested - 1.96 * best_r2_std_nested
r2_ci_upper = best_r2_nested + 1.96 * best_r2_std_nested
print(f"   üìä 95% Confidence Interval (R¬≤): [{r2_ci_lower:.4f}, {r2_ci_upper:.4f}]")

# Performance assessment based on RMSE
print(f"\nüìà PERFORMANCE ASSESSMENT:")
if best_rmse_nested <= 600:
    performance_level = "EXCELLENT"
    emoji = "üéâ"
elif best_rmse_nested <= 800:
    performance_level = "GOOD"
    emoji = "‚úÖ"
elif best_rmse_nested <= 1000:
    performance_level = "ACCEPTABLE"
    emoji = "üëç"
else:
    performance_level = "NEEDS IMPROVEMENT"
    emoji = "‚ö†Ô∏è"

print(f"   {emoji} Performance Level: {performance_level}")
print(f"   üí∞ RMSE = ${best_rmse_nested:.2f} (Average prediction error)")
print(f"   üí∞ MAE = ${best_mae_nested:.2f} (Median prediction error)")

# Model ranking summary
print(f"\nüìã MODEL RANKING (by RMSE):")
for idx, row in nested_comparison_df.iterrows():
    rank = nested_comparison_df.index.get_loc(idx) + 1
    model = row['Model']
    rmse = row['Nested_CV_RMSE_Mean']
    rmse_std = row['Nested_CV_RMSE_Std']
    print(f"   {rank}. {model:<15}: RMSE = ${rmse:.2f} ¬± ${rmse_std:.2f}")

print(f"\nüí° INTERPRETATION:")
print(f"   üéØ RMSE measures average prediction error in dollars")
print(f"   üéØ MAE measures typical prediction error (less sensitive to outliers)")
print(f"   üìà R¬≤ measures proportion of variance explained (0-1 scale)")
print(f"   ‚úÖ Lower RMSE/MAE = Better model for income prediction")
print(f"   üìä Linear Regression provides interpretable baseline performance")
print(f"   üöÄ CatBoost often excels with categorical features and provides robust results")

# %%
# SECTION 8.5: BASELINE COMPARISON ANALYSIS
# =============================================================================
print("\nüìà BASELINE COMPARISON ANALYSIS")
print("-" * 60)

if 'Linear Regression' in nested_cv_results:
    lr_results = nested_cv_results['Linear Regression']
    lr_rmse = lr_results['outer_cv_rmse_mean']
    lr_rmse_std = lr_results['outer_cv_rmse_std']
    lr_rank = nested_comparison_df[nested_comparison_df['Model'] == 'Linear Regression'].index[0] + 1

    print(f"üéØ LINEAR REGRESSION BASELINE PERFORMANCE:")
    print(f"   üìä RMSE: ${lr_rmse:.2f} ¬± ${lr_rmse_std:.2f}")
    print(f"   üèÜ Ranking: #{lr_rank} out of {len(base_models)} models")

    # Calculate improvement over baseline for each model
    print(f"\nüìä IMPROVEMENT OVER LINEAR REGRESSION BASELINE:")
    for idx, row in nested_comparison_df.iterrows():
        model = row['Model']
        rmse = row['Nested_CV_RMSE_Mean']

        if model != 'Linear Regression':
            improvement = ((lr_rmse - rmse) / lr_rmse) * 100
            improvement_dollars = lr_rmse - rmse

            if improvement > 0:
                print(f"   ‚úÖ {model:<15}: {improvement:+5.1f}% improvement (${improvement_dollars:+6.0f})")
            else:
                print(f"   ‚ùå {model:<15}: {improvement:+5.1f}% worse (${improvement_dollars:+6.0f})")

    # Business interpretation
    print(f"\nüí° BASELINE INSIGHTS:")
    if lr_rank == 1:
        print(f"   üö® LINEAR REGRESSION WINS! Complex models may be overfitting.")
        print(f"   üíº Consider using Linear Regression for production (simpler, interpretable)")
    elif lr_rank <= 3:
        print(f"   üëç Linear Regression performs competitively")
        print(f"   üíº Complex models provide modest but meaningful improvements")
    else:
        print(f"   ‚úÖ Complex models significantly outperform baseline")
        print(f"   üíº Investment in complex models is justified")

    print(f"\nüîç LINEAR REGRESSION ADVANTAGES:")
    print(f"   ‚úÖ Highly interpretable (feature coefficients)")
    print(f"   ‚úÖ Fast training and prediction")
    print(f"   ‚úÖ No hyperparameter tuning needed")
    print(f"   ‚úÖ Robust to overfitting")
    print(f"   ‚úÖ Easy to deploy and maintain")

else:
    print("   ‚ö†Ô∏è Linear Regression results not found.")

# %%
# SECTION 9: CATBOOST SPECIFIC ANALYSIS
# =============================================================================
print("\nüöÄ CATBOOST SPECIFIC ANALYSIS")
print("-" * 60)

if 'CatBoost' in nested_cv_results:
    catboost_results = nested_cv_results['CatBoost']
    catboost_rmse = catboost_results['outer_cv_rmse_mean']
    catboost_rmse_std = catboost_results['outer_cv_rmse_std']
    catboost_rank = nested_comparison_df[nested_comparison_df['Model'] == 'CatBoost'].index[0] + 1

    print(f"üéØ CATBOOST PERFORMANCE:")
    print(f"   üìä RMSE: ${catboost_rmse:.2f} ¬± ${catboost_rmse_std:.2f}")
    print(f"   üèÜ Ranking: #{catboost_rank} out of {len(base_models)} models")

    # Compare with other gradient boosting models
    if 'XGBoost' in nested_cv_results and 'LightGBM' in nested_cv_results:
        xgb_rmse = nested_cv_results['XGBoost']['outer_cv_rmse_mean']
        lgb_rmse = nested_cv_results['LightGBM']['outer_cv_rmse_mean']

        print(f"\nüìä GRADIENT BOOSTING COMPARISON:")
        print(f"   CatBoost: ${catboost_rmse:.2f}")
        print(f"   XGBoost:  ${xgb_rmse:.2f}")
        print(f"   LightGBM: ${lgb_rmse:.2f}")

        if catboost_rmse <= min(xgb_rmse, lgb_rmse):
            print(f"   ü•á CatBoost WINS among gradient boosting models!")
        elif catboost_rmse <= max(xgb_rmse, lgb_rmse):
            print(f"   ü•à CatBoost performs competitively with other gradient boosting models")
        else:
            print(f"   üìä CatBoost provides alternative perspective to XGBoost/LightGBM")

    print(f"\nüöÄ CATBOOST ADVANTAGES FOR THIS DATASET:")
    print(f"   üéØ Excellent handling of categorical features (even after frequency encoding)")
    print(f"   üîß Built-in overfitting protection and regularization")
    print(f"   ‚ö° Often requires less hyperparameter tuning than XGBoost/LightGBM")
    print(f"   üìä Robust performance across different data distributions")
    print(f"   üí∞ Well-suited for financial/income prediction tasks")

    # Hyperparameter stability analysis for CatBoost
    catboost_params = catboost_results['best_params_per_fold']
    print(f"\nüîß CATBOOST HYPERPARAMETER STABILITY:")

    # Check most common parameters
    param_stability = {}
    for param_name in ['iterations', 'depth', 'learning_rate']:
        if param_name in catboost_params[0]:
            values = [params.get(param_name) for params in catboost_params]
            unique_values = len(set(values))
            most_common = max(set(values), key=values.count)
            param_stability[param_name] = {
                'unique_values': unique_values,
                'most_common': most_common,
                'stability': 'High' if unique_values <= 2 else 'Moderate' if unique_values <= 3 else 'Low'
            }

    for param, info in param_stability.items():
        print(f"   {param}: {info['stability']} stability (most common: {info['most_common']})")

else:
    print("   ‚ö†Ô∏è CatBoost results not found. Make sure CatBoost was included in the model comparison.")

print(f"\n‚úÖ CATBOOST ANALYSIS COMPLETE!")

# %%
# FINAL SUMMARY: COMPREHENSIVE MODEL COMPARISON WITH CATBOOST
# =============================================================================
print("\nüéâ FINAL SUMMARY: COMPREHENSIVE MODEL COMPARISON WITH CATBOOST")
print("=" * 80)

print(f"üìä MODELS EVALUATED: {len(base_models)} (Baseline to Advanced)")
for i, model in enumerate(base_models.keys(), 1):
    if model == 'Linear Regression':
        print(f"   {i}. {model} (üìä BASELINE)")
    elif model in ['XGBoost', 'LightGBM', 'CatBoost']:
        print(f"   {i}. {model} (üöÄ ADVANCED)")
    else:
        print(f"   {i}. {model}")

print(f"\nüèÜ FINAL RANKINGS (by RMSE):")
for idx, row in nested_comparison_df.iterrows():
    rank = nested_comparison_df.index.get_loc(idx) + 1
    model = row['Model']
    rmse = row['Nested_CV_RMSE_Mean']
    rmse_std = row['Nested_CV_RMSE_Std']

    if rank == 1:
        emoji = "ü•á"
    elif rank == 2:
        emoji = "ü•à"
    elif rank == 3:
        emoji = "ü•â"
    else:
        emoji = f"{rank}."

    print(f"   {emoji} {model}: ${rmse:.2f} ¬± ${rmse_std:.2f}")

print(f"\nüéØ WINNER: {best_model_nested}")
print(f"   üí∞ Expected RMSE: ${best_rmse_nested:.2f}")
print(f"   üìä This represents the average prediction error in dollars")

print(f"\nüìä BASELINE TO ADVANCED PROGRESSION:")
print(f"   ‚úÖ Linear Regression: Interpretable baseline performance")
print(f"   ‚úÖ Random Forest: Tree-based ensemble approach")
print(f"   ‚úÖ XGBoost/LightGBM/CatBoost: Advanced gradient boosting")
print(f"   ‚úÖ Comprehensive comparison across algorithmic complexity")
print(f"   ‚úÖ Demonstrates value of complex models vs simple baseline")

print(f"\nüíæ NEXT STEPS:")
print(f"   1. Use the best model ({best_model_nested}) for final training")
print(f"   2. Apply saved frequency mappings for production predictions")
print(f"   3. Monitor model performance with expected RMSE: ${best_rmse_nested:.2f}")
print(f"   4. Consider ensemble approaches if multiple models perform similarly")

print(f"\nüéâ NESTED CV WITH CATBOOST PIPELINE COMPLETE!")


# %%
# SECTION 10: HYPERPARAMETER ANALYSIS AND STABILITY ASSESSMENT
# =============================================================================
print("\nüîß HYPERPARAMETER ANALYSIS AND STABILITY ASSESSMENT")
print("-" * 70)

# Analyze hyperparameter consistency across folds for model robustness
for model_name, results in nested_cv_results.items():
    print(f"\nüìã {model_name} - Hyperparameter Stability Analysis:")
    print("-" * 50)

    best_params_list = results['best_params_per_fold']

    # Get all unique parameter names
    all_param_names = set()
    for params in best_params_list:
        all_param_names.update(params.keys())

    # Analyze each parameter for consistency
    stable_params = 0
    total_params = len(all_param_names)

    for param_name in sorted(all_param_names):
        param_values = [params.get(param_name, 'N/A') for params in best_params_list]
        unique_values = list(set(param_values))

        if len(unique_values) == 1:
            consistency = "‚úÖ STABLE"
            stable_params += 1
        elif len(unique_values) == 2:
            consistency = "‚ö†Ô∏è MODERATE"
        else:
            consistency = "‚ùå UNSTABLE"

        # Show parameter values across folds
        value_counts = Counter(param_values)
        most_common = value_counts.most_common(1)[0]

        print(f"   {param_name:<25}: {consistency}")
        print(f"      Values: {param_values}")
        print(f"      Most frequent: {most_common[0]} ({most_common[1]}/{len(param_values)} folds)")

    # Calculate stability percentage
    stability_pct = (stable_params / total_params) * 100
    print(f"\n   üìä Hyperparameter Stability: {stable_params}/{total_params} stable ({stability_pct:.1f}%)")

    if stability_pct >= 80:
        print("   ‚úÖ HIGH STABILITY: Model hyperparameters are robust across data splits")
    elif stability_pct >= 60:
        print("   üëç MODERATE STABILITY: Some hyperparameter variation across splits")
    else:
        print("   ‚ö†Ô∏è LOW STABILITY: High hyperparameter sensitivity to data splits")



# %%
# SECTION 11: NESTED CV RESULTS ANALYSIS
# =============================================================================
print("\nüìä NESTED CV RESULTS ANALYSIS")
print("=" * 60)

# Create comparison DataFrame
comparison_data = []
for model_name, results in nested_cv_results.items():
    comparison_data.append({
        'Model': model_name,
        'Nested_CV_R2_Mean': results['outer_cv_r2_mean'],
        'Nested_CV_R2_Std': results['outer_cv_r2_std'],
        'Nested_CV_RMSE_Mean': results['outer_cv_rmse_mean'],
        'Nested_CV_RMSE_Std': results['outer_cv_rmse_std'],
        'Nested_CV_MAE_Mean': results['outer_cv_mae_mean'],
        'Nested_CV_MAE_Std': results['outer_cv_mae_std']
    })

nested_comparison_df = pd.DataFrame(comparison_data)
nested_comparison_df = nested_comparison_df.sort_values('Nested_CV_R2_Mean', ascending=False)

print("\nüèÜ NESTED CV PERFORMANCE COMPARISON:")
print("=" * 80)
print(nested_comparison_df.round(4).to_string(index=False))

# Identify best model from nested CV
best_model_nested = nested_comparison_df.iloc[0]['Model']
best_r2_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Mean']
best_r2_std_nested = nested_comparison_df.iloc[0]['Nested_CV_R2_Std']

print(f"\nü•á BEST MODEL (Nested CV): {best_model_nested}")
print(f"   üìä Unbiased R¬≤: {best_r2_nested:.4f} ¬± {best_r2_std_nested:.4f}")
print(f"   üìä 95% Confidence Interval: [{best_r2_nested - 1.96*best_r2_std_nested:.4f}, {best_r2_nested + 1.96*best_r2_std_nested:.4f}]")


# %%
# SECTION 12: FINAL MODEL TRAINING WITH AGGREGATED BEST HYPERPARAMETERS
# =============================================================================
print("\nüéØ FINAL MODEL TRAINING WITH AGGREGATED BEST HYPERPARAMETERS")
print("=" * 70)

def get_most_frequent_params(best_params_list):
    """
    Get the most frequently selected hyperparameters across CV folds
    This provides robust hyperparameter selection for the final model
    """
    # Get all parameter names
    all_param_names = set()
    for params in best_params_list:
        all_param_names.update(params.keys())

    # Find most frequent value for each parameter
    final_params = {}
    for param_name in all_param_names:
        param_values = [params.get(param_name) for params in best_params_list if param_name in params]
        if param_values:
            # Get most common value
            most_common = Counter(param_values).most_common(1)[0][0]
            final_params[param_name] = most_common

    return final_params

# Get best hyperparameters for the winning model
best_model_results = nested_cv_results[best_model_nested]
best_params_final = get_most_frequent_params(best_model_results['best_params_per_fold'])

print(f"üèÜ Training final {best_model_nested} model with aggregated best parameters:")
print("üìã Final hyperparameters (most frequent across CV folds):")
for param, value in sorted(best_params_final.items()):
    print(f"   {param:<25}: {value}")

# Create and train final model on full training data
print(f"\nüöÄ Training final {best_model_nested} model...")
final_model = base_models[best_model_nested].set_params(**best_params_final)
final_model.fit(X_train_full_scaled, y_train_full)

print(f"‚úÖ Final {best_model_nested} model trained on full training set")
print(f"   üìä Training data: {X_train_full_scaled.shape[0]:,} samples")
print(f"   üîß Features: {len(feature_columns)} variables")
print(f"   üéØ Expected RMSE: ${best_rmse_nested:.2f} ¬± ${best_rmse_std_nested:.2f}")


# %%
# SECTION 13: FINAL MODEL EVALUATION ON TEST SET
# =============================================================================
print("\nüéØ FINAL MODEL EVALUATION ON TEST SET")
print("-" * 60)

# Make predictions on test set (held-out data never seen during nested CV)
y_pred_test = final_model.predict(X_test_scaled)

# Calculate comprehensive test metrics
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
test_mae = mean_absolute_error(y_test, y_pred_test)
test_r2 = r2_score(y_test, y_pred_test)

# Calculate additional income-specific metrics
# MAPE for incomes > $100 (avoid division by very small numbers)
valid_mask = y_test > 100
if valid_mask.sum() > 0:
    test_mape = np.mean(np.abs((y_test[valid_mask] - y_pred_test[valid_mask]) / y_test[valid_mask])) * 100
else:
    test_mape = np.nan

print(f"üèÜ FINAL TEST PERFORMANCE ({best_model_nested}):")
print("=" * 60)
print(f"   üéØ Test RMSE: ${test_rmse:.2f}")
print(f"   üéØ Test MAE:  ${test_mae:.2f}")
print(f"   üìà Test R¬≤:   {test_r2:.4f}")
if not np.isnan(test_mape):
    print(f"   üí∞ Test MAPE (>$100): {test_mape:.1f}%")

# Compare with nested CV estimates (validation of nested CV effectiveness)
print(f"\nüìä NESTED CV vs TEST SET COMPARISON:")
print("-" * 50)
print(f"   Metric    | Nested CV Estimate | Test Set | Difference")
print(f"   ----------|-------------------|----------|----------")
print(f"   RMSE      | ${best_rmse_nested:.2f} ¬± ${best_rmse_std_nested:.2f}     | ${test_rmse:.2f}     | ${abs(test_rmse - best_rmse_nested):.2f}")
print(f"   MAE       | ${best_mae_nested:.2f} ¬± ${best_mae_std_nested:.2f}     | ${test_mae:.2f}     | ${abs(test_mae - best_mae_nested):.2f}")
print(f"   R¬≤        | {best_r2_nested:.4f} ¬± {best_r2_std_nested:.4f} | {test_r2:.4f}   | {abs(test_r2 - best_r2_nested):.4f}")

# Assess nested CV prediction accuracy
rmse_within_ci = abs(test_rmse - best_rmse_nested) <= 2 * best_rmse_std_nested
mae_within_ci = abs(test_mae - best_mae_nested) <= 2 * best_mae_std_nested
r2_within_ci = abs(test_r2 - best_r2_nested) <= 2 * best_r2_std_nested

print(f"\n‚úÖ NESTED CV VALIDATION:")
print(f"   RMSE within 95% CI: {'‚úÖ YES' if rmse_within_ci else '‚ö†Ô∏è NO'}")
print(f"   MAE within 95% CI:  {'‚úÖ YES' if mae_within_ci else '‚ö†Ô∏è NO'}")
print(f"   R¬≤ within 95% CI:   {'‚úÖ YES' if r2_within_ci else '‚ö†Ô∏è NO'}")

if rmse_within_ci and mae_within_ci:
    print("   üéâ EXCELLENT: Nested CV provided accurate performance estimates!")
elif rmse_within_ci or mae_within_ci:
    print("   üëç GOOD: Nested CV estimates reasonably accurate")
else:
    print("   ‚ö†Ô∏è WARNING: Test performance differs significantly from nested CV estimates")

# Target distribution comparison (from original pipeline)
print(f"\nüìà TARGET DISTRIBUTION COMPARISON:")
print(f"   Training Full - Mean: ${y_train_full.mean():,.2f}, Std: ${y_train_full.std():,.2f}")
print(f"   Test Set      - Mean: ${y_test.mean():,.2f}, Std: ${y_test.std():,.2f}")
print(f"   Predictions   - Mean: ${y_pred_test.mean():,.2f}, Std: ${y_pred_test.std():,.2f}")


# %%
# SECTION 14: FINAL MODEL TRAINING WITH BEST HYPERPARAMETERS
# =============================================================================
print("\nüéØ FINAL MODEL TRAINING WITH BEST HYPERPARAMETERS")
print("=" * 60)

def get_most_frequent_params(best_params_list):
    """
    Get the most frequently selected hyperparameters across CV folds
    """
    from collections import Counter

    # Get all parameter names
    all_param_names = set()
    for params in best_params_list:
        all_param_names.update(params.keys())

    # Find most frequent value for each parameter
    final_params = {}
    for param_name in all_param_names:
        param_values = [params.get(param_name) for params in best_params_list if param_name in params]
        if param_values:
            # Get most common value
            most_common = Counter(param_values).most_common(1)[0][0]
            final_params[param_name] = most_common

    return final_params

# Get best hyperparameters for the winning model
best_model_results = nested_cv_results[best_model_nested]
best_params_final = get_most_frequent_params(best_model_results['best_params_per_fold'])

print(f"üèÜ Training final {best_model_nested} model with aggregated best parameters:")
print("üìã Final hyperparameters (most frequent across CV folds):")
for param, value in best_params_final.items():
    print(f"   {param}: {value}")

# Create and train final model
final_model = base_models[best_model_nested].set_params(**best_params_final)
final_model.fit(X_train_full_scaled, y_train_full)

print(f"\n‚úÖ Final {best_model_nested} model trained on full training set")


# %%
# SECTION 15: PERMUTATION IMPORTANCE ANALYSIS
# =============================================================================
print("\nüîç PERMUTATION IMPORTANCE ANALYSIS")
print("-" * 60)

# Calculate permutation importance on test set (from original pipeline)
print("üîÑ Calculating permutation importance (this may take a few minutes)...")
perm_importance = permutation_importance(
    final_model,
    X_test_scaled,
    y_test,
    n_repeats=15,  # More repeats for robust estimates
    random_state=42,
    scoring='neg_mean_squared_error'  # Use RMSE-based scoring
)

# Fix the importance calculation - use the raw importance values
# The permutation importance already gives us the decrease in performance
feature_importance_mean = -perm_importance.importances_mean  # Convert from negative to positive
feature_importance_std = perm_importance.importances_std

# Create importance DataFrame
importance_df = pd.DataFrame({
    'feature': feature_columns,
    'importance_mean': feature_importance_mean,
    'importance_std': feature_importance_std
}).sort_values('importance_mean', ascending=False)

print(f"\nüìä TOP 15 MOST IMPORTANT FEATURES (by MSE increase when permuted):")
print("-" * 60)
for i, (_, row) in enumerate(importance_df.head(15).iterrows(), 1):
    print(f"   {i:2d}. {row['feature']:<30} {row['importance_mean']:>8.4f} ¬± {row['importance_std']:>6.4f}")

# Save importance results
importance_df.to_csv(data_path + '/nested_cv_catboost_permutation_importance.csv', index=False)
print(f"\nüíæ Permutation importance saved to: nested_cv_catboost_permutation_importance.csv")

# Create permutation importance visualization (from original pipeline)
print(f"\nüìà Creating permutation importance visualization...")

plt.figure(figsize=(12, 8))

# Get top 20 features for visualization
num_features_to_plot = min(20, len(feature_importance_mean))
top_features = importance_df.head(num_features_to_plot)

# Create horizontal bar plot
y_pos = np.arange(num_features_to_plot)
bars = plt.barh(y_pos, top_features['importance_mean'],
                xerr=top_features['importance_std'],
                capsize=3, alpha=0.7, color='steelblue')

plt.yticks(y_pos, top_features['feature'])
plt.xlabel('MSE Increase When Feature Permuted')
plt.title(f'Permutation Importance - Top {num_features_to_plot} Features\n({best_model_nested} Model)')
plt.grid(True, alpha=0.3, axis='x')

# Add value labels on bars
for i, (bar, mean_val) in enumerate(zip(bars, top_features['importance_mean'])):
    plt.text(bar.get_width() + max(top_features['importance_mean']) * 0.01,
             bar.get_y() + bar.get_height()/2,
             f'{mean_val:.4f}',
             va='center', fontsize=8)

plt.tight_layout()

# Save plot
plt.savefig(data_path + '/nested_cv_catboost_permutation_importance.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"‚úÖ Permutation importance visualization saved to: nested_cv_catboost_permutation_importance.png")

# Feature importance insights
print(f"\nüí° FEATURE IMPORTANCE INSIGHTS:")
top_5_features = importance_df.head(5)['feature'].tolist()
print(f"   üîù Top 5 features: {', '.join(top_5_features)}")

# Analyze feature types in top 10
top_10_features = importance_df.head(10)['feature'].tolist()
top_basic = [f for f in top_10_features if f in basic_features]
top_age = [f for f in top_10_features if f in age_features]
top_interaction = [f for f in top_10_features if f in interaction_features]

print(f"   üìä In top 10 - Basic: {len(top_basic)}, Age: {len(top_age)}, Interaction: {len(top_interaction)}")
print(f"   üí∞ Feature importance measured as MSE increase when feature is randomly shuffled")

# Additional analysis: Check for zero importance features
zero_importance = importance_df[importance_df['importance_mean'] <= 0]
if len(zero_importance) > 0:
    print(f"\n‚ö†Ô∏è  Features with zero or negative importance: {len(zero_importance)}")
    print("   These features may not be contributing to model performance")
else:
    print(f"\n‚úÖ All features show positive importance")

# Show importance statistics
print(f"\nüìä IMPORTANCE STATISTICS:")
print(f"   Mean importance: {importance_df['importance_mean'].mean():.4f}")
print(f"   Std importance: {importance_df['importance_mean'].std():.4f}")
print(f"   Max importance: {importance_df['importance_mean'].max():.4f}")
print(f"   Min importance: {importance_df['importance_mean'].min():.4f}")

# %%
# SECTION 16: COMPREHENSIVE VISUALIZATIONS
# =============================================================================
print("\nüìà CREATING COMPREHENSIVE NESTED CV VISUALIZATIONS")
print("-" * 60)

# Create comprehensive visualization dashboard
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('Nested Cross-Validation Results Dashboard (with CatBoost)', fontsize=16, fontweight='bold')

# 1. Model comparison by RMSE (primary metric)
ax1 = axes[0, 0]
models = nested_comparison_df['Model']
rmse_means = nested_comparison_df['Nested_CV_RMSE_Mean']
rmse_stds = nested_comparison_df['Nested_CV_RMSE_Std']

bars = ax1.bar(models, rmse_means, yerr=rmse_stds, capsize=5, alpha=0.7,
               color=['gold' if m == best_model_nested else 'lightblue' for m in models])
ax1.set_ylabel('RMSE ($)')
ax1.set_title('Model Comparison by RMSE (Lower is Better)')
ax1.grid(True, alpha=0.3)
ax1.tick_params(axis='x', rotation=45)

# Add value labels
for bar, mean, std in zip(bars, rmse_means, rmse_stds):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 10,
             f'${mean:.0f}¬±{std:.0f}', ha='center', va='bottom', fontsize=9)

# 2. RMSE scores across CV folds for best model
ax2 = axes[0, 1]
best_rmse_scores = nested_cv_results[best_model_nested]['outer_scores_rmse']
fold_numbers = range(1, len(best_rmse_scores) + 1)

ax2.plot(fold_numbers, best_rmse_scores, 'o-', linewidth=2, markersize=8, color='red')
ax2.axhline(y=np.mean(best_rmse_scores), color='red', linestyle='--', alpha=0.7,
            label=f'Mean: ${np.mean(best_rmse_scores):.0f}')
ax2.fill_between(fold_numbers,
                 np.mean(best_rmse_scores) - np.std(best_rmse_scores),
                 np.mean(best_rmse_scores) + np.std(best_rmse_scores),
                 alpha=0.2, color='red')
ax2.set_xlabel('CV Fold')
ax2.set_ylabel('RMSE ($)')
ax2.set_title(f'{best_model_nested} - RMSE Across CV Folds')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. MAE comparison
ax3 = axes[0, 2]
mae_means = nested_comparison_df['Nested_CV_MAE_Mean']
mae_stds = nested_comparison_df['Nested_CV_MAE_Std']

bars = ax3.bar(models, mae_means, yerr=mae_stds, capsize=5, alpha=0.7,
               color=['coral' if m == best_model_nested else 'lightgreen' for m in models])
ax3.set_ylabel('MAE ($)')
ax3.set_title('Model Comparison by MAE (Lower is Better)')
ax3.grid(True, alpha=0.3)
ax3.tick_params(axis='x', rotation=45)

# 4. Nested CV vs Test Set comparison
ax4 = axes[1, 0]
metrics = ['RMSE', 'MAE', 'R¬≤']
nested_values = [best_rmse_nested, best_mae_nested, best_r2_nested]
test_values = [test_rmse, test_mae, test_r2]

x = np.arange(len(metrics))
width = 0.35

bars1 = ax4.bar(x - width/2, nested_values, width, label='Nested CV', alpha=0.7, color='skyblue')
bars2 = ax4.bar(x + width/2, test_values, width, label='Test Set', alpha=0.7, color='orange')

ax4.set_ylabel('Value')
ax4.set_title('Nested CV vs Test Set Performance')
ax4.set_xticks(x)
ax4.set_xticklabels(metrics)
ax4.legend()
ax4.grid(True, alpha=0.3)

# 5. Prediction vs Actual scatter plot
ax5 = axes[1, 1]
ax5.scatter(y_test, y_pred_test, alpha=0.6, s=20)
min_val = min(y_test.min(), y_pred_test.min())
max_val = max(y_test.max(), y_pred_test.max())
ax5.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
ax5.set_xlabel('Actual Income ($)')
ax5.set_ylabel('Predicted Income ($)')
ax5.set_title(f'Predictions vs Actual ({best_model_nested})')
ax5.legend()
ax5.grid(True, alpha=0.3)

# 6. Residuals plot
ax6 = axes[1, 2]
residuals = y_test - y_pred_test
ax6.scatter(y_pred_test, residuals, alpha=0.6, s=20)
ax6.axhline(y=0, color='r', linestyle='--', linewidth=2)
ax6.set_xlabel('Predicted Income ($)')
ax6.set_ylabel('Residuals ($)')
ax6.set_title('Residuals Plot')
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(data_path + '/nested_cv_catboost_comprehensive_results.png', dpi=300, bbox_inches='tight')
plt.show()

print("   ‚úÖ Comprehensive visualizations saved to: nested_cv_catboost_comprehensive_results.png")


# %%
# SECTION 17: SAVE NESTED CV RESULTS AND ARTIFACTS
# =============================================================================
print("\nüíæ SAVING NESTED CV RESULTS AND ARTIFACTS (WITH CATBOOST)")
print("-" * 60)

# Create comprehensive nested CV summary
nested_cv_summary = {
    'methodology': {
        'approach': 'Nested Cross-Validation with CatBoost',
        'outer_folds': OUTER_CV_FOLDS,
        'inner_folds': INNER_CV_FOLDS,
        'random_search_iterations': RANDOM_SEARCH_ITERATIONS,
        'primary_metric': 'RMSE',
        'models_evaluated': list(base_models.keys()),
        'total_model_trainings_per_model': OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS
    },
    'nested_cv_results': nested_cv_results,
    'best_model_name': best_model_nested,
    'best_hyperparameters': best_params_final,
    'performance_estimates': {
        'nested_cv_rmse_mean': best_rmse_nested,
        'nested_cv_rmse_std': best_rmse_std_nested,
        'nested_cv_mae_mean': best_mae_nested,
        'nested_cv_mae_std': best_mae_std_nested,
        'nested_cv_r2_mean': best_r2_nested,
        'nested_cv_r2_std': best_r2_std_nested
    },
    'test_performance': {
        'test_rmse': test_rmse,
        'test_mae': test_mae,
        'test_r2': test_r2,
        'test_mape': test_mape if not np.isnan(test_mape) else None
    },
    'model_comparison': nested_comparison_df.to_dict('records'),
    'feature_info': {
        'total_features': len(feature_columns),
        'feature_types': {
            'basic': len(basic_features),
            'age_group': len(age_features),
            'frequency': len(freq_features),
            'interaction': len(interaction_features),
            'other': len(other_features)
        }
    },
    'execution_info': {
        'total_duration_minutes': total_duration,
        'execution_date': datetime.now().isoformat()
    }
}

# Save comprehensive results to JSON
with open(data_path + '/nested_cv_catboost_comprehensive_results.json', 'w') as f:
    # Convert numpy types for JSON serialization
    def convert_numpy(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return obj

    json.dump(nested_cv_summary, f, indent=2, default=convert_numpy)

print("   ‚úÖ Comprehensive results saved: nested_cv_catboost_comprehensive_results.json")

# Save comparison table
nested_comparison_df.to_csv(data_path + '/nested_cv_catboost_model_comparison.csv', index=False)
print("   ‚úÖ Model comparison saved: nested_cv_catboost_model_comparison.csv")

# Save final model artifacts (compatible with original pipeline)
final_model_artifacts = {
    'final_model': final_model,
    'scaler': scaler,
    'feature_columns': feature_columns,
    'model_name': best_model_nested,
    'hyperparameters': best_params_final,
    'nested_cv_performance': {
        'rmse_mean': best_rmse_nested,
        'rmse_std': best_rmse_std_nested,
        'mae_mean': best_mae_nested,
        'mae_std': best_mae_std_nested,
        'r2_mean': best_r2_nested,
        'r2_std': best_r2_std_nested
    },
    'test_performance': {
        'rmse': test_rmse,
        'mae': test_mae,
        'r2': test_r2,
        'mape': test_mape if not np.isnan(test_mape) else None
    },
    'training_info': {
        'cv_method': 'Nested Cross-Validation with CatBoost',
        'outer_folds': OUTER_CV_FOLDS,
        'inner_folds': INNER_CV_FOLDS,
        'optimization_metric': 'RMSE',
        'training_samples': len(X_train_full_scaled),
        'test_samples': len(X_test_scaled),
        'training_date': datetime.now().isoformat()
    }
}

joblib.dump(final_model_artifacts, data_path + '/nested_cv_catboost_final_model.pkl')
print("   ‚úÖ Final model artifacts saved: nested_cv_catboost_final_model.pkl")

print("   üíæ Saving enhanced datasets for reference...")
with open(data_path + '/nested_cv_catboost_data_info.txt', 'w') as f:
    f.write("Nested CV with CatBoost Data Information\n")
    f.write("=" * 50 + "\n")
    f.write(f"Training data (full): {X_train_full_scaled.shape}\n")
    f.write(f"Test data: {X_test_scaled.shape}\n")
    f.write(f"Features: {len(feature_columns)}\n")
    f.write(f"Target: {target_column}\n")
    f.write(f"Preprocessing: RobustScaler\n")
    f.write(f"Models evaluated: {', '.join(base_models.keys())}\n")
    f.write(f"Winner: {best_model_nested}\n")

print("   ‚úÖ Data information saved: nested_cv_catboost_data_info.txt")


# %%
# SECTION 18: NESTED CV SUMMARY REPORT (WITH CATBOOST)
# =============================================================================
print("\nüìã NESTED CV SUMMARY REPORT (WITH CATBOOST)")
print("=" * 60)

print("üéØ NESTED CROSS-VALIDATION PIPELINE SUMMARY")
print("=" * 60)

print(f"\nüìä METHODOLOGY:")
print(f"   ‚Ä¢ Outer CV: {OUTER_CV_FOLDS}-fold for unbiased performance estimation")
print(f"   ‚Ä¢ Inner CV: {INNER_CV_FOLDS}-fold for hyperparameter tuning")
print(f"   ‚Ä¢ Total model trainings: {OUTER_CV_FOLDS * INNER_CV_FOLDS * RANDOM_SEARCH_ITERATIONS * len(base_models):,}")
print(f"   ‚Ä¢ Hyperparameter search: RandomizedSearchCV ({RANDOM_SEARCH_ITERATIONS} iterations per inner fold)")
print(f"   ‚Ä¢ Models evaluated: {', '.join(base_models.keys())}")

print(f"\nüèÜ RESULTS:")
print(f"   ‚Ä¢ Best Model: {best_model_nested}")
print(f"   ‚Ä¢ Unbiased RMSE Estimate: ${best_rmse_nested:.2f} ¬± ${best_rmse_std_nested:.2f}")
print(f"   ‚Ä¢ 95% Confidence Interval: [${best_rmse_nested - 1.96*best_rmse_std_nested:.2f}, ${best_rmse_nested + 1.96*best_rmse_std_nested:.2f}]")
print(f"   ‚Ä¢ Test Set RMSE: ${test_rmse:.2f}")
print(f"   ‚Ä¢ Test Set R¬≤: {test_r2:.4f}")

print(f"\nüìà PERFORMANCE ASSESSMENT:")
if test_rmse <= 600:
    performance_level = "EXCELLENT"
    emoji = "üéâ"
elif test_rmse <= 800:
    performance_level = "GOOD"
    emoji = "‚úÖ"
elif test_rmse <= 1000:
    performance_level = "ACCEPTABLE"
    emoji = "üëç"
else:
    performance_level = "NEEDS IMPROVEMENT"
    emoji = "‚ö†Ô∏è"

print(f"   {emoji} Performance Level: {performance_level}")
print(f"   ‚Ä¢ RMSE = ${test_rmse:.2f} {'‚úÖ' if test_rmse <= 800 else '‚ö†Ô∏è'}")

print(f"\nüöÄ CATBOOST IMPACT:")
if 'CatBoost' in nested_cv_results:
    catboost_rank = nested_comparison_df[nested_comparison_df['Model'] == 'CatBoost'].index[0] + 1
    print(f"   ‚Ä¢ CatBoost Ranking: #{catboost_rank} out of {len(base_models)} models")
    if catboost_rank == 1:
        print(f"   ü•á CatBoost WINNER! Excellent performance with categorical features")
    elif catboost_rank <= 2:
        print(f"   ü•à CatBoost performed excellently, very competitive")
    else:
        print(f"   üìä CatBoost provided valuable comparison perspective")

print(f"\nüíæ PRODUCTION READY:")
print(f"   ‚Ä¢ Final model: {best_model_nested}")
print(f"   ‚Ä¢ Frequency mappings: Saved for production use")
print(f"   ‚Ä¢ Expected RMSE: ${best_rmse_nested:.2f}")
print(f"   ‚Ä¢ Model artifacts: Saved for deployment")

print(f"\n‚úÖ NESTED CROSS-VALIDATION WITH CATBOOST COMPLETED SUCCESSFULLY!")
print("=" * 80)


# %%
# SECTION 19: PRODUCTION MODEL TRAINING (OPTIONAL - USING ALL DATA)
# =============================================================================
print("\nüöÄ PRODUCTION MODEL TRAINING (USING ALL AVAILABLE DATA)")
print("=" * 70)
print("Training final production model using ALL available data (train + validation + test)...")
print("Using best hyperparameters from nested CV analysis.")

# Combine ALL data for final production model
X_all_data = pd.concat([X_train_full, X_test], ignore_index=True)
y_all_data = pd.concat([y_train_full, y_test], ignore_index=True)

print(f"\nüìä PRODUCTION TRAINING DATA:")
print(f"   ‚Ä¢ Total samples: {X_all_data.shape[0]:,}")
print(f"   ‚Ä¢ Features: {X_all_data.shape[1]}")
print(f"   ‚Ä¢ Target mean: ${y_all_data.mean():,.2f}")
print(f"   ‚Ä¢ Target std: ${y_all_data.std():,.2f}")

# Scale all data using the same scaler fitted on training data
X_all_data_scaled = pd.DataFrame(
    scaler.transform(X_all_data),
    columns=X_all_data.columns,
    index=X_all_data.index
)

# Train final production model
print(f"\nüîß Training final production {best_model_nested} model...")
production_model = base_models[best_model_nested].set_params(**best_params_final)
production_model.fit(X_all_data_scaled, y_all_data)

print(f"‚úÖ Production {best_model_nested} model trained successfully!")

# Save production model
production_model_artifacts = {
    'production_model': production_model,
    'scaler': scaler,
    'feature_columns': feature_columns,
    'model_name': best_model_nested,
    'hyperparameters': best_params_final,
    'training_data_size': len(X_all_data_scaled),
    'expected_performance': {
        'rmse_estimate': best_rmse_nested,
        'rmse_std': best_rmse_std_nested,
        'r2_estimate': best_r2_nested
    },
    'training_info': {
        'training_date': datetime.now().isoformat(),
        'data_used': 'All available data (train + validation + test)',
        'cv_method': 'Nested CV with CatBoost for hyperparameter selection'
    }
}

joblib.dump(production_model_artifacts, data_path + '/production_model_catboost_all_data.pkl')
print(f"üíæ Production model saved: production_model_catboost_all_data.pkl")

print(f"\nüéØ PRODUCTION DEPLOYMENT READY:")
print(f"   ‚Ä¢ Model: {best_model_nested}")
print(f"   ‚Ä¢ Training samples: {len(X_all_data_scaled):,}")
print(f"   ‚Ä¢ Expected RMSE: ${best_rmse_nested:.2f}")
print(f"   ‚Ä¢ Frequency mappings: Available for new predictions")
print(f"   ‚Ä¢ Artifacts saved: Ready for deployment")

# =============================================================================
# CALCULATE CONFIDENCE INTERVALS FOR PRODUCTION PREDICTIONS
# =============================================================================
print("\nüî¢ CALCULATING CONFIDENCE INTERVALS FOR PREDICTIONS")
print("-" * 50)

# Make predictions on the full training dataset to calculate residuals
print("   üìä Making predictions on training data for residual calculation...")
y_train_pred = production_model.predict(X_all_data_scaled)

# Calculate residuals (actual - predicted)
residuals = y_all_data - y_train_pred
print(f"   ‚úÖ Residuals calculated: {len(residuals)} samples")

# Calculate confidence interval offsets using quantile method
# This avoids normal distribution assumptions and uses actual residual distribution
confidence_level = 0.90  # 90% confidence interval
lower_percentile = (1 - confidence_level) / 2 * 100  # 5th percentile
upper_percentile = (1 + confidence_level) / 2 * 100  # 95th percentile

ci_lower_offset = np.percentile(residuals, lower_percentile)
ci_upper_offset = np.percentile(residuals, upper_percentile)

print(f"   üìà Confidence Level: {confidence_level*100:.0f}%")
print(f"   üìâ Lower offset (5th percentile): ${ci_lower_offset:.2f}")
print(f"   üìà Upper offset (95th percentile): ${ci_upper_offset:.2f}")
print(f"   üìä Average CI width: ${ci_upper_offset - ci_lower_offset:.2f}")

# Store confidence interval information
confidence_intervals = {
    'confidence_level': confidence_level,
    'ci_lower_offset': ci_lower_offset,
    'ci_upper_offset': ci_upper_offset,
    'method': 'quantile_based',
    'training_coverage_note': f'Based on {len(residuals)} training samples',
    'residual_stats': {
        'mean': np.mean(residuals),
        'std': np.std(residuals),
        'min': np.min(residuals),
        'max': np.max(residuals)
    }
}

print(f"   ‚úÖ Confidence interval calculation complete!")

# Update production model artifacts with confidence intervals
production_model_artifacts['confidence_intervals'] = confidence_intervals
production_model_artifacts['prediction_usage'] = {
    'point_prediction': 'Use production_model.predict(X)',
    'confidence_interval': f'Add/subtract offsets: [prediction + {ci_lower_offset:.2f}, prediction + {ci_upper_offset:.2f}]',
    'confidence_level': f'{confidence_level*100:.0f}%',
    'interpretation': 'Prediction ¬± confidence interval gives range where true value likely falls'
}

# Re-save with confidence intervals
joblib.dump(production_model_artifacts, data_path + '/production_model_catboost_all_data.pkl')
print(f"üíæ Production model updated with confidence intervals")

print(f"\nüéØ PRODUCTION PREDICTION USAGE:")
print(f"   1. Point prediction: production_model.predict(X)")
print(f"   2. Lower bound: prediction + {ci_lower_offset:.2f}")
print(f"   3. Upper bound: prediction + {ci_upper_offset:.2f}")
print(f"   4. Confidence level: {confidence_level*100:.0f}%")
print(f"   5. Example: If prediction = $1000, range = [${1000 + ci_lower_offset:.0f}, ${1000 + ci_upper_offset:.0f}]")

print(f"\nüéâ COMPLETE NESTED CV PIPELINE WITH CATBOOST FINISHED!")
print("=" * 80)
