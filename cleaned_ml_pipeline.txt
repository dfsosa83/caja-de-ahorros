# %%
# =============================================================================
# INCOME PREDICTION MODEL - CLEAN ML PIPELINE
# =============================================================================
# Goal: Predict customer income (ingresos_reportados) using enhanced features
# Current Performance: RÂ² â‰ˆ 0.31 (Target: Improve to 0.35+)
# Models: XGBoost, LightGBM, Random Forest with GroupKFold CV
# =============================================================================

# %%
# SECTION 1: IMPORTS AND SETUP
# =============================================================================
import pandas as pd
import numpy as np
import warnings
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import GroupKFold, cross_val_score, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)

print("=" * 80)
print("INCOME PREDICTION MODEL - CLEAN PIPELINE STARTED")
print("=" * 80)

# %%
# SECTION 2: DATA LOADING AND INITIAL SETUP
# =============================================================================
print("\nğŸ”„ LOADING PROCESSED DATA")
print("-" * 50)

# Define data path
data_path = r'C:\Users\david\OneDrive\Documents\augment-projects\caja-de-ahorros\data\processed'

# Load the main dataset
df_original = pd.read_csv(data_path + '/df_clientes_clean.csv')

print(f"âœ… Original dataset loaded: {df_original.shape}")
print(f"   Columns: {df_original.shape[1]}")
print(f"   Records: {df_original.shape[0]:,}")

# %%
# SECTION 3: FEATURE SELECTION AND DATASET PREPARATION
# =============================================================================
print("\nğŸ¯ PREPARING FEATURE SET")
print("-" * 50)

# Define columns to keep for modeling
columns_to_keep = [
    # ID columns
    'cliente', 'identificador_unico',
    
    # Target variable
    'ingresos_reportados',
    
    # Core features (already processed)
    'edad', 'letras_mensuales', 'monto_letra', 'saldo',
    'fechaingresoempleo', 'fecha_inicio', 'fecha_vencimiento',
    'ocupacion_consolidated', 'ciudad_consolidated',
    'nombreempleadorcliente_consolidated', 'cargoempleocliente_consolidated',
    'sexo_consolidated', 'estado_civil_consolidated', 'pais_consolidated',
    'missing_fechaingresoempleo', 'missing_nombreempleadorcliente', 'missing_cargoempleocliente',
    'is_retired', 'age_group'
]

# Filter to existing columns
existing_columns = [col for col in columns_to_keep if col in df_original.columns]
df_working = df_original[existing_columns].copy()

print(f"âœ… Working dataset prepared: {df_working.shape}")
print(f"   Features to process: {len(existing_columns) - 3}")  # Exclude IDs and target

# %%
# SECTION 4: DATA PREPROCESSING PIPELINE
# =============================================================================
print("\nğŸ”§ DATA PREPROCESSING PIPELINE")
print("-" * 50)

def preprocess_data(df):
    """
    Complete data preprocessing pipeline
    """
    df_processed = df.copy()
    
    # 4.1: Handle Missing Values
    print("   ğŸ“‹ Handling missing values...")
    
    # Numerical missing values
    if 'monto_letra' in df_processed.columns:
        df_processed['monto_letra_missing'] = df_processed['monto_letra'].isnull().astype(int)
        median_monto = df_processed['monto_letra'].median()
        df_processed['monto_letra'] = df_processed['monto_letra'].fillna(median_monto)
        print(f"      âœ… monto_letra: filled {df_processed['monto_letra_missing'].sum()} missing values")
    
    # 4.2: Convert Date Features
    print("   ğŸ“… Converting date features...")
    reference_date = pd.Timestamp('2020-01-01')
    date_columns = ['fechaingresoempleo', 'fecha_inicio', 'fecha_vencimiento']
    
    for date_col in date_columns:
        if date_col in df_processed.columns:
            # Convert to datetime
            df_processed[date_col] = pd.to_datetime(df_processed[date_col], errors='coerce')
            
            # Convert to days since reference
            df_processed[f'{date_col}_days'] = (df_processed[date_col] - reference_date).dt.days
            
            # Handle missing values
            missing_count = df_processed[f'{date_col}_days'].isnull().sum()
            if missing_count > 0:
                df_processed[f'{date_col}_missing'] = df_processed[f'{date_col}_days'].isnull().astype(int)
                median_days = df_processed[f'{date_col}_days'].median()
                df_processed[f'{date_col}_days'] = df_processed[f'{date_col}_days'].fillna(median_days)
                print(f"      âœ… {date_col}: converted to days, filled {missing_count} missing")
            
            # Drop original date column
            df_processed.drop(columns=[date_col], inplace=True)
    
    # 4.3: Encode Categorical Features
    print("   ğŸ·ï¸  Encoding categorical features...")
    
    # Low cardinality - One-hot encoding
    low_cardinality = ['sexo_consolidated', 'estado_civil_consolidated', 'pais_consolidated', 'age_group']
    existing_low_card = [col for col in low_cardinality if col in df_processed.columns]
    
    for col in existing_low_card:
        df_processed[col] = df_processed[col].fillna('Unknown')
        dummies = pd.get_dummies(df_processed[col], prefix=col, drop_first=True)
        df_processed = pd.concat([df_processed, dummies], axis=1)
        df_processed.drop(columns=[col], inplace=True)
        print(f"      âœ… {col}: created {len(dummies.columns)} dummy variables")
    
    # High cardinality - Frequency encoding
    high_cardinality = ['ocupacion_consolidated', 'ciudad_consolidated',
                       'nombreempleadorcliente_consolidated', 'cargoempleocliente_consolidated']
    existing_high_card = [col for col in high_cardinality if col in df_processed.columns]
    
    for col in existing_high_card:
        df_processed[col] = df_processed[col].fillna('Unknown')
        freq_map = df_processed[col].value_counts().to_dict()
        new_col_name = f'{col}_freq'
        df_processed[new_col_name] = df_processed[col].map(freq_map)
        df_processed.drop(columns=[col], inplace=True)
        print(f"      âœ… {col}: frequency encoded (range: {df_processed[new_col_name].min()}-{df_processed[new_col_name].max()})")
    
    return df_processed

# Apply preprocessing
df_processed = preprocess_data(df_working)
print(f"\nâœ… Preprocessing complete: {df_processed.shape}")

# %%
# SECTION 5: FEATURE ENGINEERING FUNCTION
# =============================================================================
print("\nâš™ï¸ FEATURE ENGINEERING SETUP")
print("-" * 50)

def create_interaction_features(df):
    """
    Create interaction features for enhanced model performance
    """
    df_enhanced = df.copy()
    
    print(f"   ğŸ”§ Creating interaction features for dataset: {df.shape}")
    
    # Get age group columns
    age_columns = [col for col in df.columns if col.startswith('age_group_')]
    
    # 1. Age-Occupation Interactions
    if 'ocupacion_consolidated_freq' in df.columns and age_columns:
        for age_col in age_columns:
            interaction_name = f"{age_col}_x_occupation"
            df_enhanced[interaction_name] = (df_enhanced[age_col] * df_enhanced['ocupacion_consolidated_freq'])
        print(f"      âœ… Created {len(age_columns)} age-occupation interactions")
    
    # 2. Location-Occupation Interaction
    if ('ciudad_consolidated_freq' in df.columns and 'ocupacion_consolidated_freq' in df.columns):
        df_enhanced['location_x_occupation'] = (df_enhanced['ciudad_consolidated_freq'] * 
                                              df_enhanced['ocupacion_consolidated_freq'])
        print(f"      âœ… Created location-occupation interaction")
    
    # 3. Retirement-Age Interactions
    if 'is_retired' in df.columns and age_columns:
        for age_col in age_columns:
            interaction_name = f"retired_x_{age_col}"
            df_enhanced[interaction_name] = (df_enhanced['is_retired'] * df_enhanced[age_col])
        print(f"      âœ… Created {len(age_columns)} retirement-age interactions")
    
    # 4. Employer-Position Interaction
    if ('nombreempleadorcliente_consolidated_freq' in df.columns and 
        'cargoempleocliente_consolidated_freq' in df.columns):
        df_enhanced['employer_x_position'] = (df_enhanced['nombreempleadorcliente_consolidated_freq'] * 
                                            df_enhanced['cargoempleocliente_consolidated_freq'])
        print(f"      âœ… Created employer-position interaction")
    
    # 5. Gender-Occupation Interaction
    if ('sexo_consolidated_Masculino' in df.columns and 'ocupacion_consolidated_freq' in df.columns):
        df_enhanced['gender_x_occupation'] = (df_enhanced['sexo_consolidated_Masculino'] * 
                                            df_enhanced['ocupacion_consolidated_freq'])
        print(f"      âœ… Created gender-occupation interaction")
    
    # 6. High-Value Combinations
    if 'ocupacion_consolidated_freq' in df.columns:
        occupation_75th = df_enhanced['ocupacion_consolidated_freq'].quantile(0.75)
        df_enhanced['high_freq_occupation'] = (df_enhanced['ocupacion_consolidated_freq'] >= occupation_75th).astype(int)
    
    if 'ciudad_consolidated_freq' in df.columns:
        city_75th = df_enhanced['ciudad_consolidated_freq'].quantile(0.75)
        df_enhanced['high_freq_city'] = (df_enhanced['ciudad_consolidated_freq'] >= city_75th).astype(int)
    
    if ('high_freq_occupation' in df_enhanced.columns and 'high_freq_city' in df_enhanced.columns):
        df_enhanced['high_value_location_job'] = (df_enhanced['high_freq_occupation'] * 
                                                df_enhanced['high_freq_city'])
        print(f"      âœ… Created high-value feature combinations")
    
    new_features = [col for col in df_enhanced.columns if col not in df.columns]
    print(f"   âœ¨ Feature engineering complete: +{len(new_features)} new features")
    
    return df_enhanced

print("âœ… Feature engineering function ready")

# %%
# SECTION 6: TRAIN/VALIDATION/TEST SPLIT (NO ID OVERLAP)
# =============================================================================
print("\nğŸ“Š CREATING TRAIN/VALIDATION/TEST SPLITS")
print("-" * 50)

from sklearn.model_selection import train_test_split

# Define split ratios
TRAIN_RATIO, VALID_RATIO, TEST_RATIO = 0.70, 0.15, 0.15

# Split by customer IDs to prevent data leakage
unique_customers = df_processed['identificador_unico'].unique()
print(f"   Total customers: {len(unique_customers):,}")

# First split: separate test customers
train_valid_customers, test_customers = train_test_split(
    unique_customers, test_size=TEST_RATIO, random_state=42, shuffle=True)

# Second split: separate train and validation customers
train_customers, valid_customers = train_test_split(
    train_valid_customers, test_size=VALID_RATIO/(TRAIN_RATIO + VALID_RATIO),
    random_state=42, shuffle=True)

# Create datasets based on customer splits
train_df = df_processed[df_processed['identificador_unico'].isin(train_customers)].copy()
valid_df = df_processed[df_processed['identificador_unico'].isin(valid_customers)].copy()
test_df = df_processed[df_processed['identificador_unico'].isin(test_customers)].copy()

print(f"   âœ… Train: {len(train_df):,} records ({len(train_df)/len(df_processed):.1%})")
print(f"   âœ… Valid: {len(valid_df):,} records ({len(valid_df)/len(df_processed):.1%})")
print(f"   âœ… Test: {len(test_df):,} records ({len(test_df)/len(df_processed):.1%})")

# Verify no customer overlap
train_ids = set(train_df['identificador_unico'].unique())
valid_ids = set(valid_df['identificador_unico'].unique())
test_ids = set(test_df['identificador_unico'].unique())

overlaps = [
    len(train_ids.intersection(valid_ids)),
    len(train_ids.intersection(test_ids)),
    len(valid_ids.intersection(test_ids))
]

if sum(overlaps) == 0:
    print("   âœ… No customer ID overlap - data leakage prevented")
else:
    print("   âŒ Customer overlap detected!")

# %%
# SECTION 7: OUTLIER CLEANING (TRAINING-BASED PARAMETERS)
# =============================================================================
print("\nğŸ§¹ OUTLIER CLEANING")
print("-" * 50)

def clean_target_outliers(df_list, target_col='ingresos_reportados'):
    """
    Clean outliers using training set parameters to prevent data leakage
    """
    # Get outlier parameters from training set only
    train_target = df_list[0][target_col]
    lower_cap = train_target.quantile(0.01)  # 1st percentile
    upper_cap = train_target.quantile(0.99)  # 99th percentile

    print(f"   ğŸ“ Outlier bounds from training: ${lower_cap:,.2f} to ${upper_cap:,.2f}")

    cleaned_dfs = []
    set_names = ['Training', 'Validation', 'Test']

    for i, (df, set_name) in enumerate(zip(df_list, set_names)):
        df_clean = df.copy()
        original_target = df_clean[target_col].copy()

        # Apply winsorization using training parameters
        df_clean[target_col] = original_target.clip(lower=lower_cap, upper=upper_cap)

        # Count changes
        n_capped = (original_target != df_clean[target_col]).sum()
        print(f"   âœ… {set_name}: {n_capped:,} values capped ({n_capped/len(df)*100:.1f}%)")

        # Add outlier flags
        df_clean['was_winsorized'] = (original_target != df_clean[target_col]).astype(int)

        cleaned_dfs.append(df_clean)

    return cleaned_dfs

# Apply outlier cleaning
train_df_clean, valid_df_clean, test_df_clean = clean_target_outliers([train_df, valid_df, test_df])

print("   âœ… Outlier cleaning complete - consistent across all sets")

# %%
# SECTION 8: FEATURE ENGINEERING APPLICATION
# =============================================================================
print("\nâš™ï¸ APPLYING FEATURE ENGINEERING")
print("-" * 50)

# Apply feature engineering to all datasets
train_df_enhanced = create_interaction_features(train_df_clean)
valid_df_enhanced = create_interaction_features(valid_df_clean)
test_df_enhanced = create_interaction_features(test_df_clean)

print(f"\nğŸ¯ ENHANCED DATASET SHAPES:")
print(f"   Training: {train_df_enhanced.shape}")
print(f"   Validation: {valid_df_enhanced.shape}")
print(f"   Test: {test_df_enhanced.shape}")

# %%
# SECTION 9: PREPARE FINAL DATASETS FOR MODELING
# =============================================================================
print("\nğŸ¯ PREPARING FINAL DATASETS")
print("-" * 50)

# Define columns
id_columns = ['cliente', 'identificador_unico']
target_column = 'ingresos_reportados'
feature_columns = [col for col in train_df_enhanced.columns
                  if col not in id_columns + [target_column]]

print(f"   ğŸ“Š Feature columns: {len(feature_columns)}")
print(f"   ğŸ¯ Target column: {target_column}")

# Create feature matrices and targets
X_train = train_df_enhanced[feature_columns].copy()
y_train = train_df_enhanced[target_column].copy()

X_valid = valid_df_enhanced[feature_columns].copy()
y_valid = valid_df_enhanced[target_column].copy()

X_test = test_df_enhanced[feature_columns].copy()
y_test = test_df_enhanced[target_column].copy()

print(f"\nğŸ“ˆ FINAL DATASET SHAPES:")
print(f"   X_train: {X_train.shape}")
print(f"   X_valid: {X_valid.shape}")
print(f"   X_test: {X_test.shape}")

# Verify data quality
print(f"\nâœ… DATA QUALITY CHECKS:")
print(f"   Missing values in X_train: {X_train.isnull().sum().sum()}")
print(f"   Missing values in y_train: {y_train.isnull().sum()}")
print(f"   All features numeric: {all(X_train.dtypes.apply(lambda x: x in ['int64', 'float64']))}")

# %%
# SECTION 10: GROUPKFOLD SETUP FOR IMBALANCED FEATURES
# =============================================================================
print("\nğŸ”„ GROUPKFOLD SETUP FOR IMBALANCED FEATURES")
print("-" * 50)

def create_age_groups(df):
    """
    Create groups based on imbalanced age features for GroupKFold
    """
    imbalanced_age_features = ['age_group_26-35', 'age_group_36-45', 'age_group_65+']
    groups = []

    for idx, row in df.iterrows():
        group_assigned = False
        for i, age_feature in enumerate(imbalanced_age_features):
            if age_feature in df.columns and row[age_feature] == 1:
                groups.append(i + 1)  # Groups 1, 2, 3
                group_assigned = True
                break

        if not group_assigned:
            groups.append(0)  # Default group

    return np.array(groups)

# Create groups for training set
groups_train = create_age_groups(X_train)
group_counts = pd.Series(groups_train).value_counts().sort_index()

print(f"   ğŸ“Š Group distribution:")
for group, count in group_counts.items():
    print(f"      Group {group}: {count:,} samples ({count/len(groups_train)*100:.1f}%)")

# Setup GroupKFold
gkf = GroupKFold(n_splits=4)
print(f"   âœ… GroupKFold configured: 4 splits")

print("\nğŸš€ READY FOR MODEL TRAINING!")
print("=" * 80)

# %%
# SECTION 11: MODEL TRAINING AND COMPARISON
# =============================================================================
print("\nğŸ¤– MODEL TRAINING AND COMPARISON")
print("-" * 50)

# Apply robust scaling to features
print("   âš–ï¸ Applying RobustScaler...")
scaler = RobustScaler()
X_train_scaled = pd.DataFrame(
    scaler.fit_transform(X_train),
    columns=X_train.columns,
    index=X_train.index
)
X_valid_scaled = pd.DataFrame(
    scaler.transform(X_valid),
    columns=X_valid.columns,
    index=X_valid.index
)
X_test_scaled = pd.DataFrame(
    scaler.transform(X_test),
    columns=X_test.columns,
    index=X_test.index
)
print("   âœ… Feature scaling complete")

# Model configurations
models = {
    'XGBoost': xgb.XGBRegressor(
        n_estimators=300,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    ),
    'LightGBM': lgb.LGBMRegressor(
        n_estimators=300,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        verbose=-1
    ),
    'Random Forest': RandomForestRegressor(
        n_estimators=200,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )
}

# %%
# SECTION 12: CROSS-VALIDATION AND MODEL EVALUATION
# =============================================================================
print("\nğŸ“Š CROSS-VALIDATION EVALUATION")
print("-" * 50)

def evaluate_model_cv(model, X, y, groups, model_name):
    """
    Evaluate model using GroupKFold cross-validation
    """
    print(f"\n   ğŸ”„ Evaluating {model_name}...")

    # Cross-validation scores
    cv_scores = cross_val_score(
        model, X, y,
        groups=groups,
        cv=gkf,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )

    # Convert to RMSE
    cv_rmse = np.sqrt(-cv_scores)

    # Calculate RÂ² using cross-validation
    cv_r2_scores = cross_val_score(
        model, X, y,
        groups=groups,
        cv=gkf,
        scoring='r2',
        n_jobs=-1
    )

    print(f"      ğŸ“ˆ CV RMSE: {cv_rmse.mean():.2f} Â± {cv_rmse.std():.2f}")
    print(f"      ğŸ“ˆ CV RÂ²: {cv_r2_scores.mean():.4f} Â± {cv_r2_scores.std():.4f}")

    return {
        'cv_rmse_mean': cv_rmse.mean(),
        'cv_rmse_std': cv_rmse.std(),
        'cv_r2_mean': cv_r2_scores.mean(),
        'cv_r2_std': cv_r2_scores.std()
    }

# Evaluate all models
cv_results = {}
for model_name, model in models.items():
    cv_results[model_name] = evaluate_model_cv(model, X_train_scaled, y_train, groups_train, model_name)

# %%
# SECTION 13: FINAL MODEL TRAINING AND VALIDATION
# =============================================================================
print("\nğŸ¯ FINAL MODEL TRAINING")
print("-" * 50)

def train_and_evaluate_final(model, X_train, y_train, X_valid, y_valid, model_name):
    """
    Train final model and evaluate on validation set
    """
    print(f"\n   ğŸš€ Training final {model_name}...")

    # Train model
    model.fit(X_train, y_train)

    # Predictions
    y_pred_train = model.predict(X_train)
    y_pred_valid = model.predict(X_valid)

    # Metrics
    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    valid_rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid))

    train_r2 = r2_score(y_train, y_pred_train)
    valid_r2 = r2_score(y_valid, y_pred_valid)

    train_mae = mean_absolute_error(y_train, y_pred_train)
    valid_mae = mean_absolute_error(y_valid, y_pred_valid)

    print(f"      ğŸ“Š Training   - RMSE: {train_rmse:.2f}, RÂ²: {train_r2:.4f}, MAE: {train_mae:.2f}")
    print(f"      ğŸ“Š Validation - RMSE: {valid_rmse:.2f}, RÂ²: {valid_r2:.4f}, MAE: {valid_mae:.2f}")

    return {
        'model': model,
        'train_rmse': train_rmse, 'valid_rmse': valid_rmse,
        'train_r2': train_r2, 'valid_r2': valid_r2,
        'train_mae': train_mae, 'valid_mae': valid_mae,
        'y_pred_valid': y_pred_valid
    }

# Train all final models
final_results = {}
for model_name, model in models.items():
    final_results[model_name] = train_and_evaluate_final(
        model, X_train_scaled, y_train, X_valid_scaled, y_valid, model_name
    )

# %%
# SECTION 14: MODEL COMPARISON AND SELECTION
# =============================================================================
print("\nğŸ† MODEL COMPARISON SUMMARY")
print("-" * 50)

# Create comparison DataFrame
comparison_data = []
for model_name in models.keys():
    cv_res = cv_results[model_name]
    final_res = final_results[model_name]

    comparison_data.append({
        'Model': model_name,
        'CV_R2_Mean': cv_res['cv_r2_mean'],
        'CV_R2_Std': cv_res['cv_r2_std'],
        'CV_RMSE_Mean': cv_res['cv_rmse_mean'],
        'Valid_R2': final_res['valid_r2'],
        'Valid_RMSE': final_res['valid_rmse'],
        'Valid_MAE': final_res['valid_mae']
    })

comparison_df = pd.DataFrame(comparison_data)
comparison_df = comparison_df.sort_values('Valid_R2', ascending=False)

print("\nğŸ“Š PERFORMANCE COMPARISON:")
print(comparison_df.round(4).to_string(index=False))

# Select best model
best_model_name = comparison_df.iloc[0]['Model']
best_model_results = final_results[best_model_name]

print(f"\nğŸ¥‡ BEST MODEL: {best_model_name}")
print(f"   ğŸ“ˆ Validation RÂ²: {best_model_results['valid_r2']:.4f}")
print(f"   ğŸ“ˆ Validation RMSE: {best_model_results['valid_rmse']:.2f}")
print(f"   ğŸ“ˆ Validation MAE: {best_model_results['valid_mae']:.2f}")

# %%
# SECTION 15: FINAL TEST EVALUATION
# =============================================================================
print("\nğŸ¯ FINAL TEST EVALUATION")
print("-" * 50)

# Evaluate best model on test set
best_model = best_model_results['model']
y_pred_test = best_model.predict(X_test_scaled)

# Test metrics
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
test_r2 = r2_score(y_test, y_pred_test)
test_mae = mean_absolute_error(y_test, y_pred_test)

print(f"ğŸ† FINAL TEST PERFORMANCE ({best_model_name}):")
print(f"   ğŸ“Š Test RMSE: {test_rmse:.2f}")
print(f"   ğŸ“Š Test RÂ²: {test_r2:.4f}")
print(f"   ğŸ“Š Test MAE: {test_mae:.2f}")

# Target statistics comparison
print(f"\nğŸ“ˆ TARGET DISTRIBUTION COMPARISON:")
print(f"   Training   - Mean: ${y_train.mean():,.2f}, Std: ${y_train.std():,.2f}")
print(f"   Validation - Mean: ${y_valid.mean():,.2f}, Std: ${y_valid.std():,.2f}")
print(f"   Test       - Mean: ${y_test.mean():,.2f}, Std: ${y_test.std():,.2f}")
print(f"   Predictions- Mean: ${y_pred_test.mean():,.2f}, Std: ${y_pred_test.std():,.2f}")

print("\n" + "=" * 80)
print("ğŸ‰ INCOME PREDICTION MODEL PIPELINE COMPLETE!")
print(f"ğŸ¯ FINAL PERFORMANCE: RÂ² = {test_r2:.4f}")
print("=" * 80)

# %%
# SECTION 16: SAVE RESULTS AND ARTIFACTS
# =============================================================================
print("\nğŸ’¾ SAVING RESULTS")
print("-" * 50)

# Save model artifacts
import joblib

# Save best model and scaler
model_artifacts = {
    'best_model': best_model,
    'scaler': scaler,
    'feature_columns': feature_columns,
    'model_name': best_model_name,
    'test_performance': {
        'r2': test_r2,
        'rmse': test_rmse,
        'mae': test_mae
    }
}

# Save to file
joblib.dump(model_artifacts, data_path + '/income_prediction_model.pkl')
print(f"   âœ… Model artifacts saved to: income_prediction_model.pkl")

# Save enhanced datasets
train_df_enhanced.to_csv(data_path + '/train_enhanced.csv', index=False)
valid_df_enhanced.to_csv(data_path + '/valid_enhanced.csv', index=False)
test_df_enhanced.to_csv(data_path + '/test_enhanced.csv', index=False)
print(f"   âœ… Enhanced datasets saved")

# Save comparison results
comparison_df.to_csv(data_path + '/model_comparison.csv', index=False)
print(f"   âœ… Model comparison saved")

print("\nğŸ¯ NEXT STEPS FOR IMPROVEMENT:")
print("   1. Hyperparameter tuning with RandomizedSearchCV")
print("   2. Ensemble methods (combine top models)")
print("   3. Additional feature engineering")
print("   4. Advanced outlier detection methods")
print(f"   5. Target: Improve RÂ² from {test_r2:.4f} to 0.35+")

print("\nâœ… PIPELINE READY FOR OPTIMIZATION!")
print("=" * 80)
