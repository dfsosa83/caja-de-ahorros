{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48177524",
   "metadata": {},
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# INCOME PREDICTION MODEL - CLEAN ML PIPELINE\n",
    "# =============================================================================\n",
    "# Goal: Predict customer income (ingresos_reportados) using enhanced features\n",
    "# Current Performance: R¬≤ ‚âà 0.31 (Target: Improve to 0.35+)\n",
    "# Models: XGBoost, LightGBM, Random Forest with GroupKFold CV\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7be12e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INCOME PREDICTION MODEL - CLEAN PIPELINE STARTED\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# SECTION 1: IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INCOME PREDICTION MODEL - CLEAN PIPELINE STARTED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27822274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ LOADING PROCESSED DATA\n",
      "--------------------------------------------------\n",
      "‚úÖ Original dataset loaded: (15000, 32)\n",
      "   Columns: 32\n",
      "   Records: 15,000\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# SECTION 2: DATA LOADING AND INITIAL SETUP\n",
    "# =============================================================================\n",
    "print(\"\\nüîÑ LOADING PROCESSED DATA\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Define data path\n",
    "data_path = r'C:\\Users\\david\\OneDrive\\Documents\\augment-projects\\caja-de-ahorros\\data\\processed'\n",
    "\n",
    "# Load the main dataset\n",
    "df_original = pd.read_csv(data_path + '/df_clientes_clean.csv')\n",
    "\n",
    "print(f\"‚úÖ Original dataset loaded: {df_original.shape}\")\n",
    "print(f\"   Columns: {df_original.shape[1]}\")\n",
    "print(f\"   Records: {df_original.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 3: FEATURE SELECTION AND DATASET PREPARATION\n",
    "# =============================================================================\n",
    "print(\"\\nüéØ PREPARING FEATURE SET\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Define columns to keep for modeling\n",
    "columns_to_keep = [\n",
    "    # ID columns\n",
    "    'cliente', 'identificador_unico',\n",
    "    \n",
    "    # Target variable\n",
    "    'ingresos_reportados',\n",
    "    \n",
    "    # Core features (already processed)\n",
    "    'edad', 'letras_mensuales', 'monto_letra', 'saldo',\n",
    "    'fechaingresoempleo', 'fecha_inicio', 'fecha_vencimiento',\n",
    "    'ocupacion_consolidated', 'ciudad_consolidated',\n",
    "    'nombreempleadorcliente_consolidated', 'cargoempleocliente_consolidated',\n",
    "    'sexo_consolidated', 'estado_civil_consolidated', 'pais_consolidated',\n",
    "    'missing_fechaingresoempleo', 'missing_nombreempleadorcliente', 'missing_cargoempleocliente',\n",
    "    'is_retired', 'age_group'\n",
    "]\n",
    "\n",
    "# Filter to existing columns\n",
    "existing_columns = [col for col in columns_to_keep if col in df_original.columns]\n",
    "df_working = df_original[existing_columns].copy()\n",
    "\n",
    "print(f\"‚úÖ Working dataset prepared: {df_working.shape}\")\n",
    "print(f\"   Features to process: {len(existing_columns) - 3}\")  # Exclude IDs and target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca49f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 4: DATA PREPROCESSING PIPELINE\n",
    "# =============================================================================\n",
    "print(\"\\nüîß DATA PREPROCESSING PIPELINE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Complete data preprocessing pipeline\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 4.1: Handle Missing Values\n",
    "    print(\"   üìã Handling missing values...\")\n",
    "    \n",
    "    # Numerical missing values\n",
    "    if 'monto_letra' in df_processed.columns:\n",
    "        df_processed['monto_letra_missing'] = df_processed['monto_letra'].isnull().astype(int)\n",
    "        median_monto = df_processed['monto_letra'].median()\n",
    "        df_processed['monto_letra'] = df_processed['monto_letra'].fillna(median_monto)\n",
    "        print(f\"      ‚úÖ monto_letra: filled {df_processed['monto_letra_missing'].sum()} missing values\")\n",
    "    \n",
    "    # 4.2: Convert Date Features\n",
    "    print(\"   üìÖ Converting date features...\")\n",
    "    reference_date = pd.Timestamp('2020-01-01')\n",
    "    date_columns = ['fechaingresoempleo', 'fecha_inicio', 'fecha_vencimiento']\n",
    "    \n",
    "    for date_col in date_columns:\n",
    "        if date_col in df_processed.columns:\n",
    "            # Convert to datetime\n",
    "            df_processed[date_col] = pd.to_datetime(df_processed[date_col], errors='coerce')\n",
    "            \n",
    "            # Convert to days since reference\n",
    "            df_processed[f'{date_col}_days'] = (df_processed[date_col] - reference_date).dt.days\n",
    "            \n",
    "            # Handle missing values\n",
    "            missing_count = df_processed[f'{date_col}_days'].isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                df_processed[f'{date_col}_missing'] = df_processed[f'{date_col}_days'].isnull().astype(int)\n",
    "                median_days = df_processed[f'{date_col}_days'].median()\n",
    "                df_processed[f'{date_col}_days'] = df_processed[f'{date_col}_days'].fillna(median_days)\n",
    "                print(f\"      ‚úÖ {date_col}: converted to days, filled {missing_count} missing\")\n",
    "            \n",
    "            # Drop original date column\n",
    "            df_processed.drop(columns=[date_col], inplace=True)\n",
    "    \n",
    "    # 4.3: Encode Categorical Features\n",
    "    print(\"   üè∑Ô∏è  Encoding categorical features...\")\n",
    "    \n",
    "    # Low cardinality - One-hot encoding\n",
    "    low_cardinality = ['sexo_consolidated', 'estado_civil_consolidated', 'pais_consolidated', 'age_group']\n",
    "    existing_low_card = [col for col in low_cardinality if col in df_processed.columns]\n",
    "    \n",
    "    for col in existing_low_card:\n",
    "        df_processed[col] = df_processed[col].fillna('Unknown')\n",
    "        dummies = pd.get_dummies(df_processed[col], prefix=col, drop_first=True)\n",
    "        df_processed = pd.concat([df_processed, dummies], axis=1)\n",
    "        df_processed.drop(columns=[col], inplace=True)\n",
    "        print(f\"      ‚úÖ {col}: created {len(dummies.columns)} dummy variables\")\n",
    "    \n",
    "    # High cardinality - Frequency encoding\n",
    "    high_cardinality = ['ocupacion_consolidated', 'ciudad_consolidated',\n",
    "                       'nombreempleadorcliente_consolidated', 'cargoempleocliente_consolidated']\n",
    "    existing_high_card = [col for col in high_cardinality if col in df_processed.columns]\n",
    "    \n",
    "    for col in existing_high_card:\n",
    "        df_processed[col] = df_processed[col].fillna('Unknown')\n",
    "        freq_map = df_processed[col].value_counts().to_dict()\n",
    "        new_col_name = f'{col}_freq'\n",
    "        df_processed[new_col_name] = df_processed[col].map(freq_map)\n",
    "        df_processed.drop(columns=[col], inplace=True)\n",
    "        print(f\"      ‚úÖ {col}: frequency encoded (range: {df_processed[new_col_name].min()}-{df_processed[new_col_name].max()})\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Apply preprocessing\n",
    "df_processed = preprocess_data(df_working)\n",
    "print(f\"\\n‚úÖ Preprocessing complete: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 5: FEATURE ENGINEERING FUNCTION\n",
    "# =============================================================================\n",
    "print(\"\\n‚öôÔ∏è FEATURE ENGINEERING SETUP\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Create interaction features for enhanced model performance\n",
    "    \"\"\"\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    print(f\"   üîß Creating interaction features for dataset: {df.shape}\")\n",
    "    \n",
    "    # Get age group columns\n",
    "    age_columns = [col for col in df.columns if col.startswith('age_group_')]\n",
    "    \n",
    "    # 1. Age-Occupation Interactions\n",
    "    if 'ocupacion_consolidated_freq' in df.columns and age_columns:\n",
    "        for age_col in age_columns:\n",
    "            interaction_name = f\"{age_col}_x_occupation\"\n",
    "            df_enhanced[interaction_name] = (df_enhanced[age_col] * df_enhanced['ocupacion_consolidated_freq'])\n",
    "        print(f\"      ‚úÖ Created {len(age_columns)} age-occupation interactions\")\n",
    "    \n",
    "    # 2. Location-Occupation Interaction\n",
    "    if ('ciudad_consolidated_freq' in df.columns and 'ocupacion_consolidated_freq' in df.columns):\n",
    "        df_enhanced['location_x_occupation'] = (df_enhanced['ciudad_consolidated_freq'] * \n",
    "                                              df_enhanced['ocupacion_consolidated_freq'])\n",
    "        print(f\"      ‚úÖ Created location-occupation interaction\")\n",
    "    \n",
    "    # 3. Retirement-Age Interactions\n",
    "    if 'is_retired' in df.columns and age_columns:\n",
    "        for age_col in age_columns:\n",
    "            interaction_name = f\"retired_x_{age_col}\"\n",
    "            df_enhanced[interaction_name] = (df_enhanced['is_retired'] * df_enhanced[age_col])\n",
    "        print(f\"      ‚úÖ Created {len(age_columns)} retirement-age interactions\")\n",
    "    \n",
    "    # 4. Employer-Position Interaction\n",
    "    if ('nombreempleadorcliente_consolidated_freq' in df.columns and \n",
    "        'cargoempleocliente_consolidated_freq' in df.columns):\n",
    "        df_enhanced['employer_x_position'] = (df_enhanced['nombreempleadorcliente_consolidated_freq'] * \n",
    "                                            df_enhanced['cargoempleocliente_consolidated_freq'])\n",
    "        print(f\"      ‚úÖ Created employer-position interaction\")\n",
    "    \n",
    "    # 5. Gender-Occupation Interaction\n",
    "    if ('sexo_consolidated_Masculino' in df.columns and 'ocupacion_consolidated_freq' in df.columns):\n",
    "        df_enhanced['gender_x_occupation'] = (df_enhanced['sexo_consolidated_Masculino'] * \n",
    "                                            df_enhanced['ocupacion_consolidated_freq'])\n",
    "        print(f\"      ‚úÖ Created gender-occupation interaction\")\n",
    "    \n",
    "    # 6. High-Value Combinations\n",
    "    if 'ocupacion_consolidated_freq' in df.columns:\n",
    "        occupation_75th = df_enhanced['ocupacion_consolidated_freq'].quantile(0.75)\n",
    "        df_enhanced['high_freq_occupation'] = (df_enhanced['ocupacion_consolidated_freq'] >= occupation_75th).astype(int)\n",
    "    \n",
    "    if 'ciudad_consolidated_freq' in df.columns:\n",
    "        city_75th = df_enhanced['ciudad_consolidated_freq'].quantile(0.75)\n",
    "        df_enhanced['high_freq_city'] = (df_enhanced['ciudad_consolidated_freq'] >= city_75th).astype(int)\n",
    "    \n",
    "    if ('high_freq_occupation' in df_enhanced.columns and 'high_freq_city' in df_enhanced.columns):\n",
    "        df_enhanced['high_value_location_job'] = (df_enhanced['high_freq_occupation'] * \n",
    "                                                df_enhanced['high_freq_city'])\n",
    "        print(f\"      ‚úÖ Created high-value feature combinations\")\n",
    "    \n",
    "    # 7. FINANCIAL BEHAVIOR FEATURES\n",
    "    # ============================================================================\n",
    "    print(\"   üí∞ Creating financial behavior features...\")\n",
    "    \n",
    "    # Debt-to-income ratio (if monto_letra exists)\n",
    "    if 'monto_letra' in df.columns and 'edad' in df.columns:\n",
    "        # Payment burden relative to age\n",
    "        df_enhanced['payment_per_age'] = df_enhanced['monto_letra'] / (df_enhanced['edad'] + 1)\n",
    "        \n",
    "        # High payment burden indicator\n",
    "        payment_75th = df_enhanced['monto_letra'].quantile(0.75)\n",
    "        df_enhanced['high_payment_burden'] = (df_enhanced['monto_letra'] >= payment_75th).astype(int)\n",
    "        print(f\"      ‚úÖ Created payment burden features\")\n",
    "    \n",
    "    # Account balance features\n",
    "    if 'saldo' in df.columns:\n",
    "        # Balance categories\n",
    "        saldo_25th = df_enhanced['saldo'].quantile(0.25)\n",
    "        saldo_75th = df_enhanced['saldo'].quantile(0.75)\n",
    "        \n",
    "        df_enhanced['low_balance'] = (df_enhanced['saldo'] <= saldo_25th).astype(int)\n",
    "        df_enhanced['high_balance'] = (df_enhanced['saldo'] >= saldo_75th).astype(int)\n",
    "        df_enhanced['medium_balance'] = ((df_enhanced['saldo'] > saldo_25th) & \n",
    "                                       (df_enhanced['saldo'] < saldo_75th)).astype(int)\n",
    "        print(f\"      ‚úÖ Created balance category features\")\n",
    "    \n",
    "    # Payment frequency features\n",
    "    if 'letras_mensuales' in df.columns:\n",
    "        # Payment frequency categories\n",
    "        df_enhanced['low_frequency_payments'] = (df_enhanced['letras_mensuales'] <= 1).astype(int)\n",
    "        df_enhanced['medium_frequency_payments'] = ((df_enhanced['letras_mensuales'] > 1) & \n",
    "                                                  (df_enhanced['letras_mensuales'] <= 3)).astype(int)\n",
    "        df_enhanced['high_frequency_payments'] = (df_enhanced['letras_mensuales'] > 3).astype(int)\n",
    "        print(f\"      ‚úÖ Created payment frequency features\")\n",
    "    \n",
    "    # 8. EMPLOYMENT STABILITY FEATURES\n",
    "    # ============================================================================\n",
    "    print(\"   üè¢ Creating employment stability features...\")\n",
    "    \n",
    "    # Employment tenure (if date features exist)\n",
    "    date_cols = [col for col in df.columns if col.endswith('_days')]\n",
    "    if any('fechaingresoempleo' in col for col in date_cols):\n",
    "        employment_col = [col for col in date_cols if 'fechaingresoempleo' in col][0]\n",
    "        \n",
    "        # Employment tenure categories (assuming more recent dates = higher values)\n",
    "        tenure_25th = df_enhanced[employment_col].quantile(0.25)\n",
    "        tenure_75th = df_enhanced[employment_col].quantile(0.75)\n",
    "        \n",
    "        df_enhanced['short_tenure'] = (df_enhanced[employment_col] <= tenure_25th).astype(int)\n",
    "        df_enhanced['long_tenure'] = (df_enhanced[employment_col] >= tenure_75th).astype(int)\n",
    "        df_enhanced['medium_tenure'] = ((df_enhanced[employment_col] > tenure_25th) & \n",
    "                                      (df_enhanced[employment_col] < tenure_75th)).astype(int)\n",
    "        print(f\"      ‚úÖ Created employment tenure features\")\n",
    "    \n",
    "    # Employer size categories\n",
    "    if 'nombreempleadorcliente_consolidated_freq' in df.columns:\n",
    "        employer_median = df_enhanced['nombreempleadorcliente_consolidated_freq'].median()\n",
    "        df_enhanced['large_employer'] = (df_enhanced['nombreempleadorcliente_consolidated_freq'] >= employer_median).astype(int)\n",
    "        df_enhanced['small_employer'] = (df_enhanced['nombreempleadorcliente_consolidated_freq'] < employer_median).astype(int)\n",
    "        print(f\"      ‚úÖ Created employer size features\")\n",
    "    \n",
    "    # 9. DEMOGRAPHIC COMBINATIONS\n",
    "    # ============================================================================\n",
    "    print(\"   üë• Creating demographic combination features...\")\n",
    "    \n",
    "    # Gender-Age interactions\n",
    "    if 'sexo_consolidated_Masculino' in df.columns and age_columns:\n",
    "        for age_col in age_columns:\n",
    "            df_enhanced[f\"male_x_{age_col}\"] = (df_enhanced['sexo_consolidated_Masculino'] * \n",
    "                                              df_enhanced[age_col])\n",
    "        print(f\"      ‚úÖ Created {len(age_columns)} gender-age interactions\")\n",
    "    \n",
    "    # Marital status interactions\n",
    "    marital_cols = [col for col in df.columns if col.startswith('estado_civil_')]\n",
    "    if marital_cols and age_columns:\n",
    "        for marital_col in marital_cols:\n",
    "            for age_col in age_columns:\n",
    "                interaction_name = f\"{marital_col}_x_{age_col}\"\n",
    "                df_enhanced[interaction_name] = (df_enhanced[marital_col] * df_enhanced[age_col])\n",
    "        print(f\"      ‚úÖ Created {len(marital_cols) * len(age_columns)} marital-age interactions\")\n",
    "    \n",
    "    # 10. LOCATION-BASED FEATURES\n",
    "    # ============================================================================\n",
    "    print(\"   üåç Creating location-based features...\")\n",
    "    \n",
    "    # City size categories\n",
    "    if 'ciudad_consolidated_freq' in df.columns:\n",
    "        city_median = df_enhanced['ciudad_consolidated_freq'].median()\n",
    "        city_75th = df_enhanced['ciudad_consolidated_freq'].quantile(0.75)\n",
    "        \n",
    "        df_enhanced['major_city'] = (df_enhanced['ciudad_consolidated_freq'] >= city_75th).astype(int)\n",
    "        df_enhanced['medium_city'] = ((df_enhanced['ciudad_consolidated_freq'] >= city_median) & \n",
    "                                    (df_enhanced['ciudad_consolidated_freq'] < city_75th)).astype(int)\n",
    "        df_enhanced['small_city'] = (df_enhanced['ciudad_consolidated_freq'] < city_median).astype(int)\n",
    "        print(f\"      ‚úÖ Created city size category features\")\n",
    "    \n",
    "    # Location-Gender interactions\n",
    "    if ('ciudad_consolidated_freq' in df.columns and 'sexo_consolidated_Masculino' in df.columns):\n",
    "        df_enhanced['male_x_city_freq'] = (df_enhanced['sexo_consolidated_Masculino'] * \n",
    "                                         df_enhanced['ciudad_consolidated_freq'])\n",
    "        print(f\"      ‚úÖ Created location-gender interaction\")\n",
    "    \n",
    "    # 11. RISK PROFILE FEATURES\n",
    "    # ============================================================================\n",
    "    print(\"   ‚ö†Ô∏è Creating risk profile features...\")\n",
    "    \n",
    "    # Age-based risk categories\n",
    "    if 'edad' in df.columns:\n",
    "        df_enhanced['young_adult'] = ((df_enhanced['edad'] >= 18) & (df_enhanced['edad'] <= 30)).astype(int)\n",
    "        df_enhanced['prime_age'] = ((df_enhanced['edad'] > 30) & (df_enhanced['edad'] <= 50)).astype(int)\n",
    "        df_enhanced['senior'] = (df_enhanced['edad'] > 50).astype(int)\n",
    "        print(f\"      ‚úÖ Created age-based risk categories\")\n",
    "    \n",
    "    # Combined risk indicators\n",
    "    risk_features = []\n",
    "    if 'is_retired' in df.columns:\n",
    "        risk_features.append('is_retired')\n",
    "    if 'high_payment_burden' in df_enhanced.columns:\n",
    "        risk_features.append('high_payment_burden')\n",
    "    if 'low_balance' in df_enhanced.columns:\n",
    "        risk_features.append('low_balance')\n",
    "    if 'short_tenure' in df_enhanced.columns:\n",
    "        risk_features.append('short_tenure')\n",
    "    \n",
    "    if len(risk_features) >= 2:\n",
    "        # Create risk score (sum of risk indicators)\n",
    "        df_enhanced['risk_score'] = df_enhanced[risk_features].sum(axis=1)\n",
    "        df_enhanced['high_risk_profile'] = (df_enhanced['risk_score'] >= 2).astype(int)\n",
    "        print(f\"      ‚úÖ Created combined risk profile features\")\n",
    "    \n",
    "    # 12. TEMPORAL FEATURES\n",
    "    # ============================================================================\n",
    "    print(\"   üìÖ Creating temporal features...\")\n",
    "    \n",
    "    # Contract duration (if start and end dates exist)\n",
    "    start_date_cols = [col for col in df.columns if 'fecha_inicio' in col and col.endswith('_days')]\n",
    "    end_date_cols = [col for col in df.columns if 'fecha_vencimiento' in col and col.endswith('_days')]\n",
    "    \n",
    "    if start_date_cols and end_date_cols:\n",
    "        start_col = start_date_cols[0]\n",
    "        end_col = end_date_cols[0]\n",
    "        \n",
    "        df_enhanced['contract_duration'] = df_enhanced[end_col] - df_enhanced[start_col]\n",
    "        \n",
    "        # Duration categories\n",
    "        duration_median = df_enhanced['contract_duration'].median()\n",
    "        df_enhanced['short_contract'] = (df_enhanced['contract_duration'] <= duration_median).astype(int)\n",
    "        df_enhanced['long_contract'] = (df_enhanced['contract_duration'] > duration_median).astype(int)\n",
    "        print(f\"      ‚úÖ Created contract duration features\")\n",
    "    \n",
    "    # 13. INTERACTION COMPLEXITY FEATURES\n",
    "    # ============================================================================\n",
    "    print(\"   üîó Creating complex interaction features...\")\n",
    "    \n",
    "    # Three-way interactions (most predictive combinations)\n",
    "    if ('ocupacion_consolidated_freq' in df.columns and \n",
    "        'ciudad_consolidated_freq' in df.columns and \n",
    "        'edad' in df.columns):\n",
    "        \n",
    "        # Normalize features for interaction\n",
    "        occ_norm = df_enhanced['ocupacion_consolidated_freq'] / df_enhanced['ocupacion_consolidated_freq'].max()\n",
    "        city_norm = df_enhanced['ciudad_consolidated_freq'] / df_enhanced['ciudad_consolidated_freq'].max()\n",
    "        age_norm = df_enhanced['edad'] / df_enhanced['edad'].max()\n",
    "        \n",
    "        df_enhanced['occupation_city_age_interaction'] = occ_norm * city_norm * age_norm\n",
    "        print(f\"      ‚úÖ Created three-way interaction feature\")\n",
    "    \n",
    "    # Professional profile combinations\n",
    "    if ('ocupacion_consolidated_freq' in df.columns and \n",
    "        'nombreempleadorcliente_consolidated_freq' in df.columns and\n",
    "        'cargoempleocliente_consolidated_freq' in df.columns):\n",
    "        \n",
    "        # Professional stability score\n",
    "        occ_norm = df_enhanced['ocupacion_consolidated_freq'] / df_enhanced['ocupacion_consolidated_freq'].max()\n",
    "        emp_norm = df_enhanced['nombreempleadorcliente_consolidated_freq'] / df_enhanced['nombreempleadorcliente_consolidated_freq'].max()\n",
    "        pos_norm = df_enhanced['cargoempleocliente_consolidated_freq'] / df_enhanced['cargoempleocliente_consolidated_freq'].max()\n",
    "        \n",
    "        df_enhanced['professional_stability_score'] = (occ_norm + emp_norm + pos_norm) / 3\n",
    "        df_enhanced['high_professional_stability'] = (df_enhanced['professional_stability_score'] >= 0.7).astype(int)\n",
    "        print(f\"      ‚úÖ Created professional stability features\")\n",
    "    \n",
    "    new_features = [col for col in df_enhanced.columns if col not in df.columns]\n",
    "    print(f\"   ‚ú® Feature engineering complete: +{len(new_features)} new features\")\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "print(\"‚úÖ Feature engineering function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 6: TRAIN/VALIDATION/TEST SPLIT (NO ID OVERLAP)\n",
    "# =============================================================================\n",
    "print(\"\\nüìä CREATING TRAIN/VALIDATION/TEST SPLITS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define split ratios\n",
    "TRAIN_RATIO, VALID_RATIO, TEST_RATIO = 0.70, 0.15, 0.15\n",
    "\n",
    "# Split by customer IDs to prevent data leakage\n",
    "unique_customers = df_processed['identificador_unico'].unique()\n",
    "print(f\"   Total customers: {len(unique_customers):,}\")\n",
    "\n",
    "# First split: separate test customers\n",
    "train_valid_customers, test_customers = train_test_split(\n",
    "    unique_customers, test_size=TEST_RATIO, random_state=42, shuffle=True)\n",
    "\n",
    "# Second split: separate train and validation customers\n",
    "train_customers, valid_customers = train_test_split(\n",
    "    train_valid_customers, test_size=VALID_RATIO/(TRAIN_RATIO + VALID_RATIO),\n",
    "    random_state=42, shuffle=True)\n",
    "\n",
    "# Create datasets based on customer splits\n",
    "train_df = df_processed[df_processed['identificador_unico'].isin(train_customers)].copy()\n",
    "valid_df = df_processed[df_processed['identificador_unico'].isin(valid_customers)].copy()\n",
    "test_df = df_processed[df_processed['identificador_unico'].isin(test_customers)].copy()\n",
    "\n",
    "print(f\"   ‚úÖ Train: {len(train_df):,} records ({len(train_df)/len(df_processed):.1%})\")\n",
    "print(f\"   ‚úÖ Valid: {len(valid_df):,} records ({len(valid_df)/len(df_processed):.1%})\")\n",
    "print(f\"   ‚úÖ Test: {len(test_df):,} records ({len(test_df)/len(df_processed):.1%})\")\n",
    "\n",
    "# Verify no customer overlap\n",
    "train_ids = set(train_df['identificador_unico'].unique())\n",
    "valid_ids = set(valid_df['identificador_unico'].unique())\n",
    "test_ids = set(test_df['identificador_unico'].unique())\n",
    "\n",
    "overlaps = [\n",
    "    len(train_ids.intersection(valid_ids)),\n",
    "    len(train_ids.intersection(test_ids)),\n",
    "    len(valid_ids.intersection(test_ids))\n",
    "]\n",
    "\n",
    "if sum(overlaps) == 0:\n",
    "    print(\"   ‚úÖ No customer ID overlap - data leakage prevented\")\n",
    "else:\n",
    "    print(\"   ‚ùå Customer overlap detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ef6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 7: OUTLIER CLEANING (TRAINING-BASED PARAMETERS)\n",
    "# =============================================================================\n",
    "print(\"\\nüßπ OUTLIER CLEANING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def clean_target_outliers(df_list, target_col='ingresos_reportados'):\n",
    "    \"\"\"\n",
    "    Clean outliers using training set parameters to prevent data leakage\n",
    "    \"\"\"\n",
    "    # Get outlier parameters from training set only\n",
    "    train_target = df_list[0][target_col]\n",
    "    lower_cap = train_target.quantile(0.01)  # 1st percentile\n",
    "    upper_cap = train_target.quantile(0.99)  # 99th percentile\n",
    "\n",
    "    print(f\"   üìè Outlier bounds from training: ${lower_cap:,.2f} to ${upper_cap:,.2f}\")\n",
    "\n",
    "    cleaned_dfs = []\n",
    "    set_names = ['Training', 'Validation', 'Test']\n",
    "\n",
    "    for i, (df, set_name) in enumerate(zip(df_list, set_names)):\n",
    "        df_clean = df.copy()\n",
    "        original_target = df_clean[target_col].copy()\n",
    "\n",
    "        # Apply winsorization using training parameters\n",
    "        df_clean[target_col] = original_target.clip(lower=lower_cap, upper=upper_cap)\n",
    "\n",
    "        # Count changes\n",
    "        n_capped = (original_target != df_clean[target_col]).sum()\n",
    "        print(f\"   ‚úÖ {set_name}: {n_capped:,} values capped ({n_capped/len(df)*100:.1f}%)\")\n",
    "\n",
    "        # Add outlier flags\n",
    "        df_clean['was_winsorized'] = (original_target != df_clean[target_col]).astype(int)\n",
    "\n",
    "        cleaned_dfs.append(df_clean)\n",
    "\n",
    "    return cleaned_dfs\n",
    "\n",
    "# Apply outlier cleaning\n",
    "train_df_clean, valid_df_clean, test_df_clean = clean_target_outliers([train_df, valid_df, test_df])\n",
    "\n",
    "print(\"   ‚úÖ Outlier cleaning complete - consistent across all sets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 8: FEATURE ENGINEERING APPLICATION\n",
    "# =============================================================================\n",
    "print(\"\\n‚öôÔ∏è APPLYING FEATURE ENGINEERING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Apply feature engineering to all datasets\n",
    "train_df_enhanced = create_interaction_features(train_df_clean)\n",
    "valid_df_enhanced = create_interaction_features(valid_df_clean)\n",
    "test_df_enhanced = create_interaction_features(test_df_clean)\n",
    "\n",
    "print(f\"\\nüéØ ENHANCED DATASET SHAPES:\")\n",
    "print(f\"   Training: {train_df_enhanced.shape}\")\n",
    "print(f\"   Validation: {valid_df_enhanced.shape}\")\n",
    "print(f\"   Test: {test_df_enhanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 9: PREPARE FINAL DATASETS FOR MODELING\n",
    "# =============================================================================\n",
    "print(\"\\nüéØ PREPARING FINAL DATASETS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# RECOMMENDED APPROACH\n",
    "id_columns = ['cliente', 'identificador_unico']\n",
    "target_column = 'ingresos_reportados'\n",
    "\n",
    "# List features you want to drop\n",
    "features_to_drop = [\n",
    "    'outlier_iqr',\n",
    "    'outlier_domain', \n",
    "    'was_winsorized',\n",
    "    # Add any other features you want to exclude\n",
    "]\n",
    "\n",
    "# Create final feature list\n",
    "columns_to_exclude = id_columns + [target_column] + features_to_drop\n",
    "feature_columns = [col for col in train_df_enhanced.columns\n",
    "                  if col not in columns_to_exclude]\n",
    "\n",
    "\n",
    "\n",
    "#feature_columns = [col for col in train_df_enhanced.columns\n",
    " #                 if col not in id_columns + [target_column]]\n",
    "\n",
    "print(f\"   üìä Feature columns: {len(feature_columns)}\")\n",
    "print(f\"   üéØ Target column: {target_column}\")\n",
    "\n",
    "# Create feature matrices and targets\n",
    "X_train = train_df_enhanced[feature_columns].copy()\n",
    "y_train = train_df_enhanced[target_column].copy()\n",
    "\n",
    "X_valid = valid_df_enhanced[feature_columns].copy()\n",
    "y_valid = valid_df_enhanced[target_column].copy()\n",
    "\n",
    "X_test = test_df_enhanced[feature_columns].copy()\n",
    "y_test = test_df_enhanced[target_column].copy()\n",
    "\n",
    "print(f\"\\nüìà FINAL DATASET SHAPES:\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   X_valid: {X_valid.shape}\")\n",
    "print(f\"   X_test: {X_test.shape}\")\n",
    "\n",
    "# Verify data quality\n",
    "print(f\"\\n‚úÖ DATA QUALITY CHECKS:\")\n",
    "print(f\"   Missing values in X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"   Missing values in y_train: {y_train.isnull().sum()}\")\n",
    "print(f\"   All features numeric: {all(X_train.dtypes.apply(lambda x: x in ['int64', 'float64']))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# INSPECT FINAL FEATURE NAMES\n",
    "# =============================================================================\n",
    "print(f\"\\nüìã FINAL FEATURE LIST ({len(feature_columns)} features):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Group features by type for better readability\n",
    "basic_features = []\n",
    "age_features = []\n",
    "freq_features = []\n",
    "interaction_features = []\n",
    "other_features = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature.startswith('age_group_'):\n",
    "        age_features.append(feature)\n",
    "    elif feature.endswith('_freq'):\n",
    "        freq_features.append(feature)\n",
    "    elif '_x_' in feature or 'retired_x_' in feature or 'employer_x_' in feature or 'gender_x_' in feature:\n",
    "        interaction_features.append(feature)\n",
    "    elif feature in ['edad', 'letras_mensuales', 'monto_letra', 'saldo', 'is_retired']:\n",
    "        basic_features.append(feature)\n",
    "    else:\n",
    "        other_features.append(feature)\n",
    "\n",
    "print(f\"üî¢ BASIC FEATURES ({len(basic_features)}):\")\n",
    "for feature in basic_features:\n",
    "    print(f\"   - {feature}\")\n",
    "\n",
    "print(f\"\\nüë• AGE GROUP FEATURES ({len(age_features)}):\")\n",
    "for feature in age_features:\n",
    "    print(f\"   - {feature}\")\n",
    "\n",
    "print(f\"\\nüìä FREQUENCY FEATURES ({len(freq_features)}):\")\n",
    "for feature in freq_features:\n",
    "    print(f\"   - {feature}\")\n",
    "\n",
    "print(f\"\\n‚ö° INTERACTION FEATURES ({len(interaction_features)}):\")\n",
    "for feature in interaction_features:\n",
    "    print(f\"   - {feature}\")\n",
    "\n",
    "print(f\"\\nüîß OTHER FEATURES ({len(other_features)}):\")\n",
    "for feature in other_features:\n",
    "    print(f\"   - {feature}\")\n",
    "\n",
    "# Save feature list to file for reference\n",
    "feature_list_df = pd.DataFrame({\n",
    "    'feature_name': feature_columns,\n",
    "    'feature_type': ['basic' if f in basic_features else\n",
    "                    'age_group' if f in age_features else\n",
    "                    'frequency' if f in freq_features else\n",
    "                    'interaction' if f in interaction_features else\n",
    "                    'other' for f in feature_columns]\n",
    "})\n",
    "\n",
    "feature_list_df.to_csv(data_path + '/final_feature_list.csv', index=False)\n",
    "print(f\"\\nüíæ Feature list saved to: final_feature_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 10: GROUPKFOLD SETUP FOR IMBALANCED FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\nüîÑ GROUPKFOLD SETUP FOR IMBALANCED FEATURES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def create_age_groups(df):\n",
    "    \"\"\"\n",
    "    Create groups based on imbalanced age features for GroupKFold\n",
    "    \"\"\"\n",
    "    imbalanced_age_features = ['age_group_26-35', 'age_group_36-45', 'age_group_65+']\n",
    "    groups = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        group_assigned = False\n",
    "        for i, age_feature in enumerate(imbalanced_age_features):\n",
    "            if age_feature in df.columns and row[age_feature] == 1:\n",
    "                groups.append(i + 1)  # Groups 1, 2, 3\n",
    "                group_assigned = True\n",
    "                break\n",
    "\n",
    "        if not group_assigned:\n",
    "            groups.append(0)  # Default group\n",
    "\n",
    "    return np.array(groups)\n",
    "\n",
    "# Create groups for training set\n",
    "groups_train = create_age_groups(X_train)\n",
    "group_counts = pd.Series(groups_train).value_counts().sort_index()\n",
    "\n",
    "print(f\"   üìä Group distribution:\")\n",
    "for group, count in group_counts.items():\n",
    "    print(f\"      Group {group}: {count:,} samples ({count/len(groups_train)*100:.1f}%)\")\n",
    "\n",
    "# Setup GroupKFold\n",
    "gkf = GroupKFold(n_splits=4)\n",
    "print(f\"   ‚úÖ GroupKFold configured: 4 splits\")\n",
    "\n",
    "print(\"\\nüöÄ READY FOR MODEL TRAINING!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 11: MODEL TRAINING AND COMPARISON\n",
    "# =============================================================================\n",
    "print(\"\\nü§ñ MODEL TRAINING AND COMPARISON\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Apply robust scaling to features\n",
    "print(\"   ‚öñÔ∏è Applying RobustScaler...\")\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_valid_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_valid),\n",
    "    columns=X_valid.columns,\n",
    "    index=X_valid.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "print(\"   ‚úÖ Feature scaling complete\")\n",
    "\n",
    "# IMMEDIATE ACTION: Replace your model configs with this\n",
    "models = {\n",
    "    'XGBoost': xgb.XGBRegressor(\n",
    "        n_estimators=500, max_depth=8, learning_rate=0.05,\n",
    "        subsample=0.85, colsample_bytree=0.85, reg_alpha=0.1, reg_lambda=1.0,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'LightGBM': lgb.LGBMRegressor(\n",
    "        n_estimators=500, max_depth=8, learning_rate=0.05,\n",
    "        subsample=0.85, colsample_bytree=0.85, num_leaves=100,\n",
    "        min_child_samples=20, reg_alpha=0.1, reg_lambda=1.0,\n",
    "        random_state=42, n_jobs=-1, verbose=-1\n",
    "    ),\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=300, max_depth=15, min_samples_split=10,\n",
    "        min_samples_leaf=5, max_features='sqrt',\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a772b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 11.5: HYPERPARAMETER TUNING (OPTIONAL)\n",
    "# =============================================================================\n",
    "print(\"\\nüîß HYPERPARAMETER TUNING SETUP\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# HYPERPARAMETER TUNING GRIDS\n",
    "# =============================================================================\n",
    "\n",
    "# XGBoost tuning grid\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [300, 500, 700],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'learning_rate': [0.03, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.85, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.85, 0.9],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0.5, 1.0, 2.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# LightGBM tuning grid\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': [300, 500, 700],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'learning_rate': [0.03, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.85, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.85, 0.9],\n",
    "    'num_leaves': [50, 100, 150],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "# Random Forest tuning grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "    'min_samples_leaf': [2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2', 0.8],\n",
    "    'max_samples': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning function\n",
    "def tune_hyperparameters(model, param_grid, X_train, y_train, groups, model_name):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning with GroupKFold\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîß Tuning {model_name} hyperparameters...\")\n",
    "    \n",
    "    # Setup GroupKFold\n",
    "    gkf = GroupKFold(n_splits=4)\n",
    "    \n",
    "    # RandomizedSearchCV for efficiency\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=50,              # Number of parameter combinations to try\n",
    "        cv=gkf,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the search\n",
    "    random_search.fit(X_train, y_train, groups=groups)\n",
    "    \n",
    "    print(f\"   ‚úÖ Best {model_name} score: {-random_search.best_score_:.4f}\")\n",
    "    print(f\"   ‚úÖ Best {model_name} params: {random_search.best_params_}\")\n",
    "    \n",
    "    return random_search.best_estimator_, random_search.best_params_\n",
    "\n",
    "print(\"‚úÖ Hyperparameter tuning setup complete\")\n",
    "\n",
    "# %%\n",
    "# SECTION 11.6: APPLY HYPERPARAMETER TUNING (OPTIONAL - COMMENT OUT IF SKIPPING)\n",
    "# =============================================================================\n",
    "print(\"\\nüöÄ APPLYING HYPERPARAMETER TUNING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Set this to True if you want to run hyperparameter tuning (takes longer)\n",
    "RUN_HYPERPARAMETER_TUNING = True  # Change to True to enable\n",
    "\n",
    "if RUN_HYPERPARAMETER_TUNING:\n",
    "    print(\"üîß Running hyperparameter tuning (this may take 10-30 minutes)...\")\n",
    "    \n",
    "    # Create base models for tuning\n",
    "    base_models = {\n",
    "        'XGBoost': xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
    "        'LightGBM': lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "        'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    }\n",
    "    \n",
    "    # Parameter grids\n",
    "    param_grids = {\n",
    "        'XGBoost': xgb_param_grid,\n",
    "        'LightGBM': lgb_param_grid,\n",
    "        'Random Forest': rf_param_grid\n",
    "    }\n",
    "    \n",
    "    # Tune each model\n",
    "    tuned_models = {}\n",
    "    tuned_params = {}\n",
    "    \n",
    "    for model_name, base_model in base_models.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TUNING {model_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        tuned_model, best_params = tune_hyperparameters(\n",
    "            base_model,\n",
    "            param_grids[model_name],\n",
    "            X_train_scaled, y_train, groups_train,\n",
    "            model_name\n",
    "        )\n",
    "        \n",
    "        tuned_models[model_name] = tuned_model\n",
    "        tuned_params[model_name] = best_params\n",
    "    \n",
    "    # Replace original models with tuned models\n",
    "    models = tuned_models\n",
    "    print(f\"\\n‚úÖ Hyperparameter tuning complete! Using tuned models.\")\n",
    "    \n",
    "    # Save tuned parameters for reference\n",
    "    import json\n",
    "    with open(data_path + '/best_hyperparameters.json', 'w') as f:\n",
    "        # Convert numpy types to native Python types for JSON serialization\n",
    "        json_params = {}\n",
    "        for model_name, params in tuned_params.items():\n",
    "            json_params[model_name] = {k: int(v) if isinstance(v, np.integer) else \n",
    "                                     float(v) if isinstance(v, np.floating) else v \n",
    "                                     for k, v in params.items()}\n",
    "        json.dump(json_params, f, indent=2)\n",
    "    print(f\"   üíæ Best parameters saved to: best_hyperparameters.json\")\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping hyperparameter tuning (using default parameters)\")\n",
    "    print(\"   üí° Set RUN_HYPERPARAMETER_TUNING = True to enable tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a420fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 12: CROSS-VALIDATION AND MODEL EVALUATION\n",
    "# =============================================================================\n",
    "print(\"\\nüìä CROSS-VALIDATION EVALUATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def evaluate_model_cv(model, X, y, groups, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate model using GroupKFold cross-validation\n",
    "    \"\"\"\n",
    "    print(f\"\\n   üîÑ Evaluating {model_name}...\")\n",
    "\n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X, y,\n",
    "        groups=groups,\n",
    "        cv=gkf,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Convert to RMSE\n",
    "    cv_rmse = np.sqrt(-cv_scores)\n",
    "\n",
    "    # Calculate R¬≤ using cross-validation\n",
    "    cv_r2_scores = cross_val_score(\n",
    "        model, X, y,\n",
    "        groups=groups,\n",
    "        cv=gkf,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print(f\"      üìà CV RMSE: {cv_rmse.mean():.2f} ¬± {cv_rmse.std():.2f}\")\n",
    "    print(f\"      üìà CV R¬≤: {cv_r2_scores.mean():.4f} ¬± {cv_r2_scores.std():.4f}\")\n",
    "\n",
    "    return {\n",
    "        'cv_rmse_mean': cv_rmse.mean(),\n",
    "        'cv_rmse_std': cv_rmse.std(),\n",
    "        'cv_r2_mean': cv_r2_scores.mean(),\n",
    "        'cv_r2_std': cv_r2_scores.std()\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "cv_results = {}\n",
    "for model_name, model in models.items():\n",
    "    cv_results[model_name] = evaluate_model_cv(model, X_train_scaled, y_train, groups_train, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382927a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 13: FINAL MODEL TRAINING AND VALIDATION\n",
    "# =============================================================================\n",
    "print(\"\\nüéØ FINAL MODEL TRAINING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def train_and_evaluate_final(model, X_train, y_train, X_valid, y_valid, model_name):\n",
    "    \"\"\"\n",
    "    Train final model and evaluate on validation set\n",
    "    \"\"\"\n",
    "    print(f\"\\n   üöÄ Training final {model_name}...\")\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_valid = model.predict(X_valid)\n",
    "\n",
    "    # Metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    valid_rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid))\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    valid_r2 = r2_score(y_valid, y_pred_valid)\n",
    "\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    valid_mae = mean_absolute_error(y_valid, y_pred_valid)\n",
    "\n",
    "    print(f\"      üìä Training   - RMSE: {train_rmse:.2f}, R¬≤: {train_r2:.4f}, MAE: {train_mae:.2f}\")\n",
    "    print(f\"      üìä Validation - RMSE: {valid_rmse:.2f}, R¬≤: {valid_r2:.4f}, MAE: {valid_mae:.2f}\")\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_rmse': train_rmse, 'valid_rmse': valid_rmse,\n",
    "        'train_r2': train_r2, 'valid_r2': valid_r2,\n",
    "        'train_mae': train_mae, 'valid_mae': valid_mae,\n",
    "        'y_pred_valid': y_pred_valid\n",
    "    }\n",
    "\n",
    "# Train all final models\n",
    "final_results = {}\n",
    "for model_name, model in models.items():\n",
    "    final_results[model_name] = train_and_evaluate_final(\n",
    "        model, X_train_scaled, y_train, X_valid_scaled, y_valid, model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 14: MODEL COMPARISON AND SELECTION\n",
    "# =============================================================================\n",
    "print(\"\\nüèÜ MODEL COMPARISON SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name in models.keys():\n",
    "    cv_res = cv_results[model_name]\n",
    "    final_res = final_results[model_name]\n",
    "\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'CV_R2_Mean': cv_res['cv_r2_mean'],\n",
    "        'CV_R2_Std': cv_res['cv_r2_std'],\n",
    "        'CV_RMSE_Mean': cv_res['cv_rmse_mean'],\n",
    "        'Valid_R2': final_res['valid_r2'],\n",
    "        'Valid_RMSE': final_res['valid_rmse'],\n",
    "        'Valid_MAE': final_res['valid_mae']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Valid_R2', ascending=False)\n",
    "\n",
    "print(\"\\nüìä PERFORMANCE COMPARISON:\")\n",
    "print(comparison_df.round(4).to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model_results = final_results[best_model_name]\n",
    "\n",
    "print(f\"\\nü•á BEST MODEL: {best_model_name}\")\n",
    "print(f\"   üìà Validation R¬≤: {best_model_results['valid_r2']:.4f}\")\n",
    "print(f\"   üìà Validation RMSE: {best_model_results['valid_rmse']:.2f}\")\n",
    "print(f\"   üìà Validation MAE: {best_model_results['valid_mae']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f0be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 15: FINAL TEST EVALUATION\n",
    "# =============================================================================\n",
    "print(\"\\nüéØ FINAL TEST EVALUATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Evaluate best model on test set\n",
    "best_model = best_model_results['model']\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Test metrics\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"üèÜ FINAL TEST PERFORMANCE ({best_model_name}):\")\n",
    "print(f\"   üìä Test RMSE: {test_rmse:.2f}\")\n",
    "print(f\"   üìä Test R¬≤: {test_r2:.4f}\")\n",
    "print(f\"   üìä Test MAE: {test_mae:.2f}\")\n",
    "\n",
    "# Target statistics comparison\n",
    "print(f\"\\nüìà TARGET DISTRIBUTION COMPARISON:\")\n",
    "print(f\"   Training   - Mean: ${y_train.mean():,.2f}, Std: ${y_train.std():,.2f}\")\n",
    "print(f\"   Validation - Mean: ${y_valid.mean():,.2f}, Std: ${y_valid.std():,.2f}\")\n",
    "print(f\"   Test       - Mean: ${y_test.mean():,.2f}, Std: ${y_test.std():,.2f}\")\n",
    "print(f\"   Predictions- Mean: ${y_pred_test.mean():,.2f}, Std: ${y_pred_test.std():,.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ INCOME PREDICTION MODEL PIPELINE COMPLETE!\")\n",
    "print(f\"üéØ FINAL PERFORMANCE: R¬≤ = {test_r2:.4f}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# SECTION 16: SAVE RESULTS AND ARTIFACTS\n",
    "# =============================================================================\n",
    "print(\"\\nüíæ SAVING RESULTS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Save model artifacts\n",
    "import joblib\n",
    "\n",
    "# Save best model and scaler\n",
    "model_artifacts = {\n",
    "    'best_model': best_model,\n",
    "    'scaler': scaler,\n",
    "    'feature_columns': feature_columns,\n",
    "    'model_name': best_model_name,\n",
    "    'test_performance': {\n",
    "        'r2': test_r2,\n",
    "        'rmse': test_rmse,\n",
    "        'mae': test_mae\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "joblib.dump(model_artifacts, data_path + '/income_prediction_model.pkl')\n",
    "print(f\"   ‚úÖ Model artifacts saved to: income_prediction_model.pkl\")\n",
    "\n",
    "# Save enhanced datasets\n",
    "train_df_enhanced.to_csv(data_path + '/train_enhanced.csv', index=False)\n",
    "valid_df_enhanced.to_csv(data_path + '/valid_enhanced.csv', index=False)\n",
    "test_df_enhanced.to_csv(data_path + '/test_enhanced.csv', index=False)\n",
    "print(f\"   ‚úÖ Enhanced datasets saved\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_df.to_csv(data_path + '/model_comparison.csv', index=False)\n",
    "print(f\"   ‚úÖ Model comparison saved\")\n",
    "\n",
    "print(\"\\nüéØ NEXT STEPS FOR IMPROVEMENT:\")\n",
    "print(\"   1. Hyperparameter tuning with RandomizedSearchCV\")\n",
    "print(\"   2. Ensemble methods (combine top models)\")\n",
    "print(\"   3. Additional feature engineering\")\n",
    "print(\"   4. Advanced outlier detection methods\")\n",
    "print(f\"   5. Target: Improve R¬≤ from {test_r2:.4f} to 0.35+\")\n",
    "\n",
    "print(\"\\n‚úÖ PIPELINE READY FOR OPTIMIZATION!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a52be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best model is already available in your session\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Model object: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a49495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c21c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance\n",
    "perm_importance = permutation_importance(best_model, X_test_scaled, y_test, n_repeats=15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort features by importance\n",
    "feature_importance = perm_importance.importances_mean\n",
    "sorted_idx = feature_importance.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5aa0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from your dataset (replace these with your actual feature names)\n",
    "feature_names = X_test_scaled.columns  # or your list of feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41803352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Get the top 15 (or fewer) important features\n",
    "num_features_to_plot = min(20, len(feature_importance))\n",
    "top_indices = sorted_idx[-num_features_to_plot:]\n",
    "\n",
    "# Plot only the top features\n",
    "plt.barh(range(num_features_to_plot), feature_importance[top_indices],color='red')\n",
    "plt.yticks(range(num_features_to_plot), [feature_names[i] for i in top_indices])\n",
    "plt.xlabel('Mean Decrease in Accuracy')\n",
    "plt.title('Permutation Importance (Top 20 Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the names of the top features\n",
    "print(\"Top 20 Feature Names:\")\n",
    "for i in reversed(top_indices):\n",
    "    print(feature_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d578c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_lg_class = best_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lg_test =X_test_scaled.copy()\n",
    "test_lg_test['target'] = y_test\n",
    "test_lg_test [\"set_type\"]='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded01592",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lg_test[\"pred_income\"] = y_test_pred_lg_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb76f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "\n",
    "mae = mean_absolute_error(test_lg_test.target, test_lg_test.pred_income)\n",
    "mse = mean_squared_error(test_lg_test.target, test_lg_test.pred_income)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(test_lg_test.target, test_lg_test.pred_income)\n",
    "evs = explained_variance_score(test_lg_test.target, test_lg_test.pred_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f83db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAPE\n",
    "mape = np.mean(np.abs((test_lg_test.target - test_lg_test.pred_income) / test_lg_test.target)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"R-squared (R¬≤) Score: {r2:.4f}\")\n",
    "print(f\"Explained Variance Score: {evs:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f295c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model_name = best_model\n",
    "X_test_final = X_test_scaled\n",
    "y_train_cleaned = y_train\n",
    "y_test_cleaned = y_test\n",
    "best_model_name = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Training Target vs Test Predictions\n",
    "# ============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUALIZING TRAINING TARGET vs TEST PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# First, get predictions on test set using the best model\n",
    "best_model_obj = best_model  # Use best_model_name instead of best_model\n",
    "test_predictions = best_model_obj.predict(X_test_final)\n",
    "\n",
    "print(f\"Using best model: {best_model_name}\")\n",
    "print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Training target shape: {y_train_cleaned.shape}\")\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle(f'Training Target vs Test Predictions - {best_model_name}', fontsize=16)\n",
    "\n",
    "# 1. Distribution Comparison (Histograms)\n",
    "axes[0,0].hist(y_train_cleaned, bins=50, alpha=0.7, label='Training Target', color='blue', density=True)\n",
    "axes[0,0].hist(test_predictions, bins=50, alpha=0.7, label='Test Predictions', color='red', density=True)\n",
    "axes[0,0].set_xlabel('Income (ingresos_reportados)')\n",
    "axes[0,0].set_ylabel('Density')\n",
    "axes[0,0].set_title('Distribution Comparison')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box Plot Comparison\n",
    "box_data = [y_train_cleaned, test_predictions]\n",
    "box_labels = ['Training Target', 'Test Predictions']\n",
    "axes[0,1].boxplot(box_data, labels=box_labels)\n",
    "axes[0,1].set_ylabel('Income (ingresos_reportados)')\n",
    "axes[0,1].set_title('Box Plot Comparison')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-Q Plot (Quantile-Quantile)\n",
    "train_quantiles = np.percentile(y_train_cleaned, np.linspace(0, 100, 100))\n",
    "pred_quantiles = np.percentile(test_predictions, np.linspace(0, 100, 100))\n",
    "\n",
    "axes[0,2].scatter(train_quantiles, pred_quantiles, alpha=0.6)\n",
    "axes[0,2].plot([min(train_quantiles), max(train_quantiles)], \n",
    "               [min(train_quantiles), max(train_quantiles)], 'r--', lw=2)\n",
    "axes[0,2].set_xlabel('Training Target Quantiles')\n",
    "axes[0,2].set_ylabel('Test Predictions Quantiles')\n",
    "axes[0,2].set_title('Q-Q Plot')\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Actual vs Predicted Scatter Plot (Test Set)\n",
    "axes[1,0].scatter(y_test_cleaned, test_predictions, alpha=0.6, s=20)\n",
    "axes[1,0].plot([y_test_cleaned.min(), y_test_cleaned.max()], \n",
    "               [y_test_cleaned.min(), y_test_cleaned.max()], 'r--', lw=2)\n",
    "axes[1,0].set_xlabel('Actual Test Values')\n",
    "axes[1,0].set_ylabel('Predicted Test Values')\n",
    "axes[1,0].set_title('Actual vs Predicted (Test Set)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate R¬≤ for the plot\n",
    "test_r2 = r2_score(y_test_cleaned, test_predictions)\n",
    "axes[1,0].text(0.05, 0.95, f'R¬≤ = {test_r2:.3f}', transform=axes[1,0].transAxes, \n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 5. Residuals Plot\n",
    "residuals = y_test_cleaned - test_predictions\n",
    "axes[1,1].scatter(test_predictions, residuals, alpha=0.6, s=20)\n",
    "axes[1,1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1,1].set_xlabel('Predicted Values')\n",
    "axes[1,1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "axes[1,1].set_title('Residuals Plot')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Cumulative Distribution Functions\n",
    "train_sorted = np.sort(y_train_cleaned)\n",
    "pred_sorted = np.sort(test_predictions)\n",
    "train_cdf = np.arange(1, len(train_sorted) + 1) / len(train_sorted)\n",
    "pred_cdf = np.arange(1, len(pred_sorted) + 1) / len(pred_sorted)\n",
    "\n",
    "axes[1,2].plot(train_sorted, train_cdf, label='Training Target', linewidth=2)\n",
    "axes[1,2].plot(pred_sorted, pred_cdf, label='Test Predictions', linewidth=2)\n",
    "axes[1,2].set_xlabel('Income (ingresos_reportados)')\n",
    "axes[1,2].set_ylabel('Cumulative Probability')\n",
    "axes[1,2].set_title('Cumulative Distribution Functions')\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical Comparison\n",
    "print(f\"\\n--- STATISTICAL COMPARISON ---\")\n",
    "print(f\"Training Target Statistics:\")\n",
    "print(f\"  - Mean: ${y_train_cleaned.mean():,.2f}\")\n",
    "print(f\"  - Median: ${y_train_cleaned.median():,.2f}\")\n",
    "print(f\"  - Std: ${y_train_cleaned.std():,.2f}\")\n",
    "print(f\"  - Min: ${y_train_cleaned.min():,.2f}\")\n",
    "print(f\"  - Max: ${y_train_cleaned.max():,.2f}\")\n",
    "\n",
    "print(f\"\\nTest Predictions Statistics:\")\n",
    "print(f\"  - Mean: ${test_predictions.mean():,.2f}\")\n",
    "print(f\"  - Median: ${np.median(test_predictions):,.2f}\")\n",
    "print(f\"  - Std: ${test_predictions.std():,.2f}\")\n",
    "print(f\"  - Min: ${test_predictions.min():,.2f}\")\n",
    "print(f\"  - Max: ${test_predictions.max():,.2f}\")\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_cleaned, test_predictions))\n",
    "test_mae = np.mean(np.abs(y_test_cleaned - test_predictions))\n",
    "print(f\"  - RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  - R¬≤: {test_r2:.4f}\")\n",
    "print(f\"  - MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Distribution similarity tests\n",
    "from scipy.stats import ks_2samp\n",
    "ks_statistic, ks_pvalue = ks_2samp(y_train_cleaned, test_predictions)\n",
    "print(f\"\\nKolmogorov-Smirnov Test:\")\n",
    "print(f\"  - KS Statistic: {ks_statistic:.4f}\")\n",
    "print(f\"  - P-value: {ks_pvalue:.4f}\")\n",
    "print(f\"  - Interpretation: {'Distributions are similar' if ks_pvalue > 0.05 else 'Distributions are different'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ec36a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "income-estimator-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
