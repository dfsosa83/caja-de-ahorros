# CELL 1: Import Libraries and Setup
# ============================================================================
import pandas as pd
import numpy as np
import warnings
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)

print("=" * 60)
print("DATA PREPROCESSING PIPELINE - STARTED")
print("=" * 60)

# Code to create a new dataset excluding already worked features
import pandas as pd

# Load the original dataset
data_path = r'C:\Users\david\OneDrive\Documents\augment-projects\caja-de-ahorros\data\processed'
df_original = pd.read_csv(data_path + '/df_clientes_clean.csv')

print(f"Original dataset shape: {df_original.shape}")
print(f"Original columns: {list(df_original.columns)}")

# Define the features that have already been worked on (to exclude)
already_worked_features = [
    'segmento',
    'edad', 
    'sexo',
    'ciudad',
    'pais',
    'ocupacion',
    'estado_civil',
    'nombreempleadorcliente',
    'cargoempleocliente',
    'productos_activos'
]

print(f"\nFeatures to exclude (already worked): {already_worked_features}")

# Define columns to keep
# Keep ID columns, target variable, and features that haven't been worked on yet
columns_to_keep = [
    # ID columns
    'cliente',
    'identificador_unico',
    
    # Target variable
    'ingresos_reportados',
    
    # Features NOT yet worked on
    'fechaingresoempleo',
    'letras_mensuales',
    'monto_letra', 
    'saldo',
    'fecha_inicio',
    'fecha_vencimiento',
    'ocupacion_consolidated',
    'ciudad_consolidated',
    'nombreempleadorcliente_consolidated',
    'cargoempleocliente_consolidated',
    'sexo_consolidated',
    'estado_civil_consolidated',
    'pais_consolidated',
    'missing_fechaingresoempleo',
    'missing_nombreempleadorcliente',
    'missing_cargoempleocliente',
    'is_retired',
    'age_group',
    'payment_to_income_ratio'
]

print(f"\nColumns to keep: {columns_to_keep}")

# Create new dataset with only unworked features
df_unworked = df_original[columns_to_keep].copy()

print(f"\nNew dataset shape: {df_unworked.shape}")
print(f"Features excluded: {len(already_worked_features)}")
print(f"Features remaining: {df_unworked.shape[1] - 3}")  # Subtract ID columns and target

# Display first few rows
print(f"\nFirst 5 rows of new dataset:")
print(df_unworked.head())

# Save the new dataset
output_filename = 'df_unworked_features.csv'
df_unworked.to_csv(data_path + '/' + output_filename, index=False)
print(f"\n✅ New dataset saved as: {output_filename}")

# Summary
print(f"\n📊 SUMMARY:")
print(f"  - Original features: {df_original.shape[1]}")
print(f"  - Excluded features: {len(already_worked_features)}")
print(f"  - Remaining features: {len(columns_to_keep) - 3}")  # Subtract ID + target
print(f"  - ID columns: 2 (cliente, identificador_unico)")
print(f"  - Target variable: 1 (ingresos_reportados)")
print(f"  - Total columns in new dataset: {df_unworked.shape[1]}")

df=df_original.copy()

# STEP 1: Handle Missing Values
# ============================================================================
print("=" * 60)
print("STEP 1: HANDLING MISSING VALUES")
print("=" * 60)

# Check current missing values
print("Missing values before processing:")
missing_summary = df.isnull().sum()
missing_features = missing_summary[missing_summary > 0]
for col, count in missing_features.items():
    pct = (count / len(df)) * 100
    print(f"  - {col}: {count} ({pct:.1f}%)")

# Handle numerical missing values - monto_letra
print(f"\nProcessing monto_letra missing values...")
df['monto_letra_missing'] = df['monto_letra'].isnull().astype(int)
median_monto = df['monto_letra'].median()
df['monto_letra'] = df['monto_letra'].fillna(median_monto)
print(f"  ✅ Created missing indicator: monto_letra_missing")
print(f"  ✅ Filled missing values with median: {median_monto:.2f}")

print(f"\nMissing values after Step 1:")
remaining_missing = df.isnull().sum().sum()
print(f"  - Total missing values: {remaining_missing}")

# STEP 2: Convert Date Features to Numerical
# ============================================================================
print("\n" + "=" * 60)
print("STEP 2: CONVERTING DATE FEATURES")
print("=" * 60)

# Define reference date (choose a meaningful date for your context)
reference_date = pd.Timestamp('2020-01-01')
print(f"Reference date: {reference_date.date()}")

# List of date columns to process
date_columns = ['fechaingresoempleo', 'fecha_inicio', 'fecha_vencimiento']

for date_col in date_columns:
    if date_col in df.columns:
        print(f"\nProcessing {date_col}...")
        
        # Check missing values before conversion
        missing_before = df[date_col].isnull().sum()
        print(f"  - Missing values before: {missing_before}")
        
        # Convert to datetime
        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
        
        # Check if conversion created more missing values
        missing_after_conversion = df[date_col].isnull().sum()
        print(f"  - Missing values after conversion: {missing_after_conversion}")
        
        # Convert to days since reference date
        df[f'{date_col}_days'] = (df[date_col] - reference_date).dt.days
        
        # Handle missing values in the new numerical column
        if missing_after_conversion > 0:
            # Create missing indicator
            df[f'{date_col}_missing'] = df[f'{date_col}_days'].isnull().astype(int)
            
            # Fill missing with median of available values
            median_days = df[f'{date_col}_days'].median()
            df[f'{date_col}_days'] = df[f'{date_col}_days'].fillna(median_days)
            
            print(f"  ✅ Created missing indicator: {date_col}_missing")
            print(f"  ✅ Filled missing days with median: {median_days:.0f}")
        
        # Drop original date column
        df.drop(columns=[date_col], inplace=True)
        print(f"  ✅ Converted to: {date_col}_days")

print(f"\nDate conversion completed!")
print(f"New numerical date columns created: {[col for col in df.columns if col.endswith('_days')]}")

# STEP 3: Encode Categorical Features
# ============================================================================
print("\n" + "=" * 60)
print("STEP 3: ENCODING CATEGORICAL FEATURES")
print("=" * 60)

# Define categorical feature groups
low_cardinality = ['sexo_consolidated', 'estado_civil_consolidated', 
                   'pais_consolidated', 'age_group']

high_cardinality = ['ocupacion_consolidated', 'ciudad_consolidated',
                    'nombreempleadorcliente_consolidated', 'cargoempleocliente_consolidated']

# Check which features actually exist in the dataframe
existing_low_card = [col for col in low_cardinality if col in df.columns]
existing_high_card = [col for col in high_cardinality if col in df.columns]

print(f"Low cardinality features to encode: {existing_low_card}")
print(f"High cardinality features to encode: {existing_high_card}")

# ONE-HOT ENCODING (Low Cardinality)
print(f"\n--- One-Hot Encoding (Low Cardinality) ---")
for col in existing_low_card:
    print(f"\nProcessing {col}...")
    
    # Check unique values
    unique_vals = df[col].unique()
    print(f"  - Unique values ({len(unique_vals)}): {list(unique_vals)}")
    
    # Fill any missing values
    df[col] = df[col].fillna('Unknown')
    
    # Create dummy variables (drop_first=True to avoid multicollinearity)
    dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)
    
    # Add dummy columns to dataframe
    df = pd.concat([df, dummies], axis=1)
    
    # Drop original column
    df.drop(columns=[col], inplace=True)
    
    print(f"  ✅ Created {len(dummies.columns)} dummy variables")
    print(f"  ✅ New columns: {list(dummies.columns)}")

# FREQUENCY ENCODING (High Cardinality)
print(f"\n--- Frequency Encoding (High Cardinality) ---")
for col in existing_high_card:
    print(f"\nProcessing {col}...")
    
    # Check unique values
    unique_count = df[col].nunique()
    print(f"  - Unique values: {unique_count}")
    
    # Fill any missing values
    df[col] = df[col].fillna('Unknown')
    
    # Create frequency encoding
    freq_map = df[col].value_counts().to_dict()
    new_col_name = f'{col}_freq'
    df[new_col_name] = df[col].map(freq_map)
    
    # Drop original column
    df.drop(columns=[col], inplace=True)
    
    print(f"  ✅ Created frequency encoded column: {new_col_name}")
    print(f"  ✅ Frequency range: {df[new_col_name].min()} to {df[new_col_name].max()}")

print(f"\nCategorical encoding completed!")

# STEP 4: Feature Scaling
# ============================================================================
print("\n" + "=" * 60)
print("STEP 4: FEATURE SCALING")
print("=" * 60)

# Identify numerical features that need scaling
numerical_features = []

# Add basic numerical features
basic_numerical = ['letras_mensuales', 'saldo', 'monto_letra']
for col in basic_numerical:
    if col in df.columns:
        numerical_features.append(col)

# Add converted date features
date_numerical = [col for col in df.columns if col.endswith('_days')]
numerical_features.extend(date_numerical)

# Add frequency encoded features
freq_features = [col for col in df.columns if col.endswith('_freq')]
numerical_features.extend(freq_features)

print(f"Numerical features to scale: {numerical_features}")

# Apply StandardScaler
from sklearn.preprocessing import StandardScaler

if numerical_features:
    print(f"\nApplying StandardScaler to {len(numerical_features)} features...")
    
    scaler = StandardScaler()
    df[numerical_features] = scaler.fit_transform(df[numerical_features])
    
    print(f"  ✅ Scaling completed!")
    
    # Show scaling results
    print(f"\nScaling verification (should be ~0 mean, ~1 std):")
    for col in numerical_features[:5]:  # Show first 5 as example
        mean_val = df[col].mean()
        std_val = df[col].std()
        print(f"  - {col}: mean={mean_val:.3f}, std={std_val:.3f}")
    
    if len(numerical_features) > 5:
        print(f"  ... and {len(numerical_features)-5} more features")
else:
    print("No numerical features found to scale.")

# STEP 5: Remove Data Leakage Features
# ============================================================================
print("\n" + "=" * 60)
print("STEP 5: REMOVING DATA LEAKAGE FEATURES")
print("=" * 60)

# Features that may cause data leakage
leakage_features = ['payment_to_income_ratio']

print(f"Checking for potential data leakage features...")
for feature in leakage_features:
    if feature in df.columns:
        df.drop(columns=[feature], inplace=True)
        print(f"  ✅ Removed: {feature} (potential data leakage)")
    else:
        print(f"  ℹ️  Feature not found: {feature}")

print(f"\nData leakage check completed!")

# STEP 6: Final Validation
# ============================================================================
print("\n" + "=" * 60)
print("STEP 6: FINAL VALIDATION")
print("=" * 60)

# Exclude ID columns and target for feature analysis
id_columns = ['cliente', 'identificador_unico']
target_column = 'ingresos_reportados'
exclude_columns = id_columns + [target_column]

feature_columns = [col for col in df.columns if col not in exclude_columns]

print(f"📊 FINAL DATASET SUMMARY:")
print(f"  - Total rows: {df.shape[0]:,}")
print(f"  - Total columns: {df.shape[1]}")
print(f"  - Feature columns: {len(feature_columns)}")
print(f"  - ID columns: {len(id_columns)}")
print(f"  - Target column: 1")

print(f"\n✅ DATA QUALITY CHECKS:")
print(f"  - Missing values in features: {df[feature_columns].isnull().sum().sum()}")
print(f"  - Missing values in target: {df[target_column].isnull().sum()}")
print(f"  - All features numeric: {all(df[col].dtype in ['int64', 'float64'] for col in feature_columns)}")

print(f"\n📋 FEATURE BREAKDOWN:")
# Count different types of features
binary_features = [col for col in feature_columns if df[col].nunique() == 2]
continuous_features = [col for col in feature_columns if col not in binary_features]

print(f"  - Binary features: {len(binary_features)}")
print(f"  - Continuous features: {len(continuous_features)}")

print(f"\n🎯 READY FOR ML MODELING:")
print(f"  X = df.drop(columns={exclude_columns})")
print(f"  y = df['{target_column}']")

print(f"\nFirst 10 feature names:")
for i, col in enumerate(feature_columns[:10]):
    print(f"  {i+1:2d}. {col}")

if len(feature_columns) > 10:
    print(f"  ... and {len(feature_columns)-10} more features")

# Convert Boolean/String Boolean Values to Numerical
# ============================================================================
print("=" * 60)
print("CONVERTING BOOLEAN VALUES TO NUMERICAL")
print("=" * 60)

# Check for columns with boolean-like string values
boolean_columns = []
for col in df.columns:
    unique_vals = df[col].unique()
    # Check if column contains only 'True', 'False', and possibly NaN
    if set(str(val).lower() for val in unique_vals if pd.notna(val)).issubset({'true', 'false'}):
        boolean_columns.append(col)

print(f"Found columns with boolean string values: {boolean_columns}")

# Convert boolean strings to numerical
for col in boolean_columns:
    print(f"\nProcessing {col}...")
    print(f"  - Original values: {df[col].unique()}")
    
    # Convert string booleans to actual booleans, then to integers
    df[col] = df[col].astype(str).str.lower()  # Normalize case
    df[col] = df[col].map({'true': 1, 'false': 0})
    
    print(f"  - New values: {df[col].unique()}")
    print(f"  ✅ Converted to numerical: 0 (False) and 1 (True)")

# Alternative approach - check for any remaining object columns that might be boolean
print(f"\n--- Checking for other potential boolean columns ---")
object_columns = df.select_dtypes(include=['object']).columns
exclude_cols = ['cliente', 'identificador_unico', 'ingresos_reportados']  # Skip ID and target
object_columns = [col for col in object_columns if col not in exclude_cols]

for col in object_columns:
    unique_vals = df[col].unique()
    if len(unique_vals) <= 2:  # Potential binary column
        print(f"\nPotential binary column: {col}")
        print(f"  - Values: {unique_vals}")
        
        # If it's clearly boolean, convert it
        if set(str(val).lower() for val in unique_vals if pd.notna(val)).issubset({'true', 'false', '1', '0', 'yes', 'no'}):
            print(f"  - Converting to numerical...")
            # Create a mapping
            if 'true' in str(unique_vals).lower() or 'false' in str(unique_vals).lower():
                df[col] = df[col].astype(str).str.lower().map({'true': 1, 'false': 0})
            elif 'yes' in str(unique_vals).lower() or 'no' in str(unique_vals).lower():
                df[col] = df[col].astype(str).str.lower().map({'yes': 1, 'no': 0})
            elif set(str(val) for val in unique_vals if pd.notna(val)).issubset({'1', '0'}):
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            print(f"  ✅ Converted! New values: {df[col].unique()}")

print(f"\n✅ Boolean conversion completed!")

# Final check - ensure all feature columns are numerical
feature_columns = [col for col in df.columns if col not in ['cliente', 'identificador_unico', 'ingresos_reportados']]
non_numeric = []

for col in feature_columns:
    if df[col].dtype not in ['int64', 'float64']:
        non_numeric.append(col)

if non_numeric:
    print(f"\n⚠️  Still have non-numeric columns: {non_numeric}")
    print("These may need manual review...")
else:
    print(f"\n✅ All feature columns are now numerical!")

df_to_split = df.drop(columns=['segmento','sexo','ciudad','pais','ocupacion','estado_civil','nombreempleadorcliente','cargoempleocliente','productos_activos']).copy()

# =============================================================================
# FEATURE ENGINEERING FUNCTION
# =============================================================================

def create_interaction_features(df):
    """
    Create interaction features for income prediction model.
    Can be applied to train_df, X_valid, X_test datasets.
    
    Parameters:
    -----------
    df : pandas.DataFrame
        Dataset containing the base features
        
    Returns:
    --------
    pandas.DataFrame
        Dataset with additional interaction features
    """
    
    # Create a copy to avoid modifying original
    df_enhanced = df.copy()
    
    print(f"🔧 Creating interaction features for dataset with shape: {df.shape}")
    
    # 1. AGE-OCCUPATION INTERACTION
    # ============================================================================
    print("   📊 Creating age-occupation interactions...")
    
    # Get age group columns
    age_columns = [col for col in df.columns if col.startswith('age_group_')]
    
    if 'ocupacion_consolidated_freq' in df.columns and age_columns:
        for age_col in age_columns:
            # Create interaction: age_group * occupation_frequency
            interaction_name = f"{age_col}_x_occupation"
            df_enhanced[interaction_name] = (df_enhanced[age_col] * 
                                           df_enhanced['ocupacion_consolidated_freq'])
            
        print(f"      ✅ Created {len(age_columns)} age-occupation interactions")
    else:
        print("      ⚠️ Missing age groups or occupation frequency columns")
    
    # 2. LOCATION-OCCUPATION INTERACTION  
    # ============================================================================
    print("   🌍 Creating location-occupation interactions...")
    
    if ('ciudad_consolidated_freq' in df.columns and 
        'ocupacion_consolidated_freq' in df.columns):
        
        # City * Occupation frequency
        df_enhanced['location_x_occupation'] = (df_enhanced['ciudad_consolidated_freq'] * 
                                              df_enhanced['ocupacion_consolidated_freq'])
        print("      ✅ Created location-occupation interaction")
    else:
        print("      ⚠️ Missing city or occupation frequency columns")
    
    # 3. RETIREMENT-AGE INTERACTION
    # ============================================================================
    print("   👴 Creating retirement-age interactions...")
    
    if 'is_retired' in df.columns and age_columns:
        for age_col in age_columns:
            # Create interaction: retirement_status * age_group
            interaction_name = f"retired_x_{age_col}"
            df_enhanced[interaction_name] = (df_enhanced['is_retired'] * 
                                           df_enhanced[age_col])
            
        print(f"      ✅ Created {len(age_columns)} retirement-age interactions")
    else:
        print("      ⚠️ Missing retirement status or age group columns")
    
    # 4. EMPLOYER-POSITION INTERACTION
    # ============================================================================
    print("   🏢 Creating employer-position interactions...")
    
    if ('nombreempleadorcliente_consolidated_freq' in df.columns and 
        'cargoempleocliente_consolidated_freq' in df.columns):
        
        # Employer size * Job position level
        df_enhanced['employer_x_position'] = (df_enhanced['nombreempleadorcliente_consolidated_freq'] * 
                                            df_enhanced['cargoempleocliente_consolidated_freq'])
        print("      ✅ Created employer-position interaction")
    else:
        print("      ⚠️ Missing employer or position frequency columns")
    
    # 5. GENDER-OCCUPATION INTERACTION
    # ============================================================================
    print("   👥 Creating gender-occupation interactions...")
    
    if ('sexo_consolidated_Masculino' in df.columns and 
        'ocupacion_consolidated_freq' in df.columns):
        
        # Gender * Occupation frequency
        df_enhanced['gender_x_occupation'] = (df_enhanced['sexo_consolidated_Masculino'] * 
                                            df_enhanced['ocupacion_consolidated_freq'])
        print("      ✅ Created gender-occupation interaction")
    else:
        print("      ⚠️ Missing gender or occupation frequency columns")
    
    # 6. HIGH-VALUE COMBINATIONS
    # ============================================================================
    print("   💼 Creating high-value feature combinations...")
    
    # High-frequency occupation indicator (top 25%)
    if 'ocupacion_consolidated_freq' in df.columns:
        occupation_75th = df_enhanced['ocupacion_consolidated_freq'].quantile(0.75)
        df_enhanced['high_freq_occupation'] = (df_enhanced['ocupacion_consolidated_freq'] >= occupation_75th).astype(int)
        print("      ✅ Created high-frequency occupation indicator")
    
    # High-frequency city indicator (top 25%)
    if 'ciudad_consolidated_freq' in df.columns:
        city_75th = df_enhanced['ciudad_consolidated_freq'].quantile(0.75)
        df_enhanced['high_freq_city'] = (df_enhanced['ciudad_consolidated_freq'] >= city_75th).astype(int)
        print("      ✅ Created high-frequency city indicator")
    
    # Combined high-value indicator
    if ('high_freq_occupation' in df_enhanced.columns and 
        'high_freq_city' in df_enhanced.columns):
        df_enhanced['high_value_location_job'] = (df_enhanced['high_freq_occupation'] * 
                                                df_enhanced['high_freq_city'])
        print("      ✅ Created high-value location-job combination")
    
    # Summary
    new_features = [col for col in df_enhanced.columns if col not in df.columns]
    print(f"\n✨ FEATURE ENGINEERING COMPLETE!")
    print(f"   📈 Original features: {df.shape[1]}")
    print(f"   📈 New features added: {len(new_features)}")
    print(f"   📈 Total features: {df_enhanced.shape[1]}")
    print(f"   📋 New feature names: {new_features}")
    
    return df_enhanced

# Train/Validation/Test Split with No ID Overlap
# ============================================================================
print("=" * 60)
print("CREATING TRAIN/VALIDATION/TEST SPLITS (NO ID OVERLAP)")
print("=" * 60)

import numpy as np
from sklearn.model_selection import train_test_split

# Define split ratios
TRAIN_RATIO = 0.70    # 70% for training
VALID_RATIO = 0.15    # 15% for validation  
TEST_RATIO = 0.15     # 15% for testing

print(f"Split ratios: Train={TRAIN_RATIO:.0%}, Valid={VALID_RATIO:.0%}, Test={TEST_RATIO:.0%}")

# Step 1: Get unique customer IDs
unique_customers = df_to_split['identificador_unico'].unique()
n_customers = len(unique_customers)

print(f"\nDataset overview:")
print(f"  - Total records: {len(df_to_split):,}")
print(f"  - Unique customers: {n_customers:,}")
print(f"  - Avg records per customer: {len(df_to_split)/n_customers:.1f}")

# Step 2: Split customer IDs (not records) into train/valid/test
print(f"\nSplitting customer IDs...")

# First split: separate test customers
train_valid_customers, test_customers = train_test_split(
    unique_customers, 
    test_size=TEST_RATIO, 
    random_state=42,
    shuffle=True
)

# Second split: separate train and validation customers
train_customers, valid_customers = train_test_split(
    train_valid_customers,
    test_size=VALID_RATIO/(TRAIN_RATIO + VALID_RATIO),  # Adjust ratio for remaining data
    random_state=42,
    shuffle=True
)

print(f"Customer ID splits:")
print(f"  - Train customers: {len(train_customers):,}")
print(f"  - Valid customers: {len(valid_customers):,}")
print(f"  - Test customers: {len(test_customers):,}")

# Step 3: Create datasets based on customer ID splits
print(f"\nCreating datasets...")

# Train set: all records from train customers
train_df = df_to_split[df_to_split['identificador_unico'].isin(train_customers)].copy()

# Validation set: all records from validation customers  
valid_df = df_to_split[df_to_split['identificador_unico'].isin(valid_customers)].copy()

# Test set: all records from test customers
test_df = df_to_split[df_to_split['identificador_unico'].isin(test_customers)].copy()

print(f"Dataset splits:")
print(f"  - Train records: {len(train_df):,} ({len(train_df)/len(df_to_split):.1%})")
print(f"  - Valid records: {len(valid_df):,} ({len(valid_df)/len(df_to_split):.1%})")
print(f"  - Test records: {len(test_df):,} ({len(test_df)/len(df_to_split):.1%})")

# Step 4: Verify no customer ID overlap
print(f"\n✅ VERIFICATION - No Customer ID Overlap:")
train_ids = set(train_df['identificador_unico'].unique())
valid_ids = set(valid_df['identificador_unico'].unique())
test_ids = set(test_df['identificador_unico'].unique())

train_valid_overlap = train_ids.intersection(valid_ids)
train_test_overlap = train_ids.intersection(test_ids)
valid_test_overlap = valid_ids.intersection(test_ids)

print(f"  - Train ∩ Valid: {len(train_valid_overlap)} customers")
print(f"  - Train ∩ Test: {len(train_test_overlap)} customers")
print(f"  - Valid ∩ Test: {len(valid_test_overlap)} customers")

if len(train_valid_overlap) == 0 and len(train_test_overlap) == 0 and len(valid_test_overlap) == 0:
    print(f"  ✅ SUCCESS: No customer overlap between sets!")
else:
    print(f"  ❌ ERROR: Customer overlap detected!")

# Step 5: Prepare features and target for each set
print(f"\n--- Preparing X and y for each set ---")

# Define feature columns (exclude ID columns and target)
id_columns = ['cliente', 'identificador_unico']
target_column = 'ingresos_reportados'
feature_columns = [col for col in df_to_split.columns if col not in id_columns + [target_column]]

print(f"Feature columns: {len(feature_columns)}")
print(f"Target column: {target_column}")

# Create X and y for each set
X_train = train_df[feature_columns].copy()
y_train = train_df[target_column].copy()

X_valid = valid_df[feature_columns].copy()
y_valid = valid_df[target_column].copy()

X_test = test_df[feature_columns].copy()
y_test = test_df[target_column].copy()

print(f"\nFeature matrices (X):")
print(f"  - X_train: {X_train.shape}")
print(f"  - X_valid: {X_valid.shape}")
print(f"  - X_test: {X_test.shape}")

print(f"\nTarget vectors (y):")
print(f"  - y_train: {y_train.shape}")
print(f"  - y_valid: {y_valid.shape}")
print(f"  - y_test: {y_test.shape}")

# Step 6: Final validation checks
print(f"\n✅ FINAL VALIDATION:")
print(f"  - All features numeric in train: {all(X_train.dtypes.apply(lambda x: x in ['int64', 'float64']))}")
print(f"  - All features numeric in valid: {all(X_valid.dtypes.apply(lambda x: x in ['int64', 'float64']))}")
print(f"  - All features numeric in test: {all(X_test.dtypes.apply(lambda x: x in ['int64', 'float64']))}")

print(f"  - Missing values in X_train: {X_train.isnull().sum().sum()}")
print(f"  - Missing values in X_valid: {X_valid.isnull().sum().sum()}")
print(f"  - Missing values in X_test: {X_test.isnull().sum().sum()}")

print(f"  - Missing values in y_train: {y_train.isnull().sum()}")
print(f"  - Missing values in y_valid: {y_valid.isnull().sum()}")
print(f"  - Missing values in y_test: {y_test.isnull().sum()}")

# Step 7: Target distribution check
print(f"\n📊 TARGET DISTRIBUTION:")
print(f"Train target stats:")
print(f"  - Mean: ${y_train.mean():,.2f}")
print(f"  - Median: ${y_train.median():,.2f}")
print(f"  - Std: ${y_train.std():,.2f}")

print(f"Valid target stats:")
print(f"  - Mean: ${y_valid.mean():,.2f}")
print(f"  - Median: ${y_valid.median():,.2f}")
print(f"  - Std: ${y_valid.std():,.2f}")

print(f"Test target stats:")
print(f"  - Mean: ${y_test.mean():,.2f}")
print(f"  - Median: ${y_test.median():,.2f}")
print(f"  - Std: ${y_test.std():,.2f}")

print(f"\n🚀 READY FOR MODEL TRAINING!")
print(f"Use: X_train, y_train for training")
print(f"Use: X_valid, y_valid for validation/hyperparameter tuning")
print(f"Use: X_test, y_test for final model evaluation")

# Optional: Save the splits to CSV files
# ============================================================================
print(f"\n--- Saving splits to files (optional) ---")

# Define data path (adjust as needed)
data_path = r'C:\Users\david\OneDrive\Documents\augment-projects\caja-de-ahorros\data\processed'

# Save complete datasets with IDs
train_df.to_csv(data_path + '/train_set.csv', index=False)
valid_df.to_csv(data_path + '/valid_set.csv', index=False)
test_df.to_csv(data_path + '/test_set.csv', index=False)

# Save feature matrices and targets separately
X_train.to_csv(data_path + '/X_train.csv', index=False)
X_valid.to_csv(data_path + '/X_valid.csv', index=False)
X_test.to_csv(data_path + '/X_test.csv', index=False)

y_train.to_csv(data_path + '/y_train.csv', index=False)
y_valid.to_csv(data_path + '/y_valid.csv', index=False)
y_test.to_csv(data_path + '/y_test.csv', index=False)

print(f"✅ All splits saved to {data_path}")

# Validation: Check Random IDs for Overlap
# ============================================================================
print("=" * 60)
print("VALIDATION: CHECKING RANDOM IDs FOR OVERLAP")
print("=" * 60)

import random

# Set random seed for reproducibility
random.seed(42)
np.random.seed(42)

# Step 1: Get all unique customer IDs from each set
train_customer_ids = set(train_df['identificador_unico'].unique())
valid_customer_ids = set(valid_df['identificador_unico'].unique())
test_customer_ids = set(test_df['identificador_unico'].unique())

print(f"Customer ID counts:")
print(f"  - Train set: {len(train_customer_ids):,} unique customers")
print(f"  - Valid set: {len(valid_customer_ids):,} unique customers")
print(f"  - Test set: {len(test_customer_ids):,} unique customers")

# Step 2: Sample random IDs from each set for manual verification
n_samples = 10  # Number of random IDs to check

print(f"\n--- Sampling {n_samples} random IDs from each set ---")

# Sample random IDs from each set
sample_train_ids = random.sample(list(train_customer_ids), min(n_samples, len(train_customer_ids)))
sample_valid_ids = random.sample(list(valid_customer_ids), min(n_samples, len(valid_customer_ids)))
sample_test_ids = random.sample(list(test_customer_ids), min(n_samples, len(test_customer_ids)))

print(f"\nSample Train IDs: {sample_train_ids}")
print(f"Sample Valid IDs: {sample_valid_ids}")
print(f"Sample Test IDs: {sample_test_ids}")

# Step 3: Check if sampled train IDs appear in valid or test
print(f"\n--- Checking Train IDs for overlap ---")
for i, train_id in enumerate(sample_train_ids):
    in_valid = train_id in valid_customer_ids
    in_test = train_id in test_customer_ids
    
    status = "✅ GOOD" if not (in_valid or in_test) else "❌ OVERLAP!"
    print(f"  {i+1:2d}. {train_id}: Valid={in_valid}, Test={in_test} - {status}")

# Step 4: Check if sampled valid IDs appear in train or test
print(f"\n--- Checking Valid IDs for overlap ---")
for i, valid_id in enumerate(sample_valid_ids):
    in_train = valid_id in train_customer_ids
    in_test = valid_id in test_customer_ids
    
    status = "✅ GOOD" if not (in_train or in_test) else "❌ OVERLAP!"
    print(f"  {i+1:2d}. {valid_id}: Train={in_train}, Test={in_test} - {status}")

# Step 5: Check if sampled test IDs appear in train or valid
print(f"\n--- Checking Test IDs for overlap ---")
for i, test_id in enumerate(sample_test_ids):
    in_train = test_id in train_customer_ids
    in_valid = test_id in valid_customer_ids
    
    status = "✅ GOOD" if not (in_train or in_valid) else "❌ OVERLAP!"
    print(f"  {i+1:2d}. {test_id}: Train={in_train}, Valid={in_valid} - {status}")

# Step 6: Comprehensive overlap check
print(f"\n--- COMPREHENSIVE OVERLAP ANALYSIS ---")

# Check all possible overlaps
train_valid_overlap = train_customer_ids.intersection(valid_customer_ids)
train_test_overlap = train_customer_ids.intersection(test_customer_ids)
valid_test_overlap = valid_customer_ids.intersection(test_customer_ids)

print(f"Complete overlap check:")
print(f"  - Train ∩ Valid: {len(train_valid_overlap)} customers")
print(f"  - Train ∩ Test: {len(train_test_overlap)} customers")
print(f"  - Valid ∩ Test: {len(valid_test_overlap)} customers")

# If there are overlaps, show them
if len(train_valid_overlap) > 0:
    print(f"  ❌ Train-Valid overlap IDs: {list(train_valid_overlap)[:10]}...")
    
if len(train_test_overlap) > 0:
    print(f"  ❌ Train-Test overlap IDs: {list(train_test_overlap)[:10]}...")
    
if len(valid_test_overlap) > 0:
    print(f"  ❌ Valid-Test overlap IDs: {list(valid_test_overlap)[:10]}...")

# Step 7: Verify union equals original
print(f"\n--- COMPLETENESS CHECK ---")
all_split_ids = train_customer_ids.union(valid_customer_ids).union(test_customer_ids)
original_ids = set(df_to_split['identificador_unico'].unique())

print(f"ID completeness:")
print(f"  - Original unique IDs: {len(original_ids):,}")
print(f"  - Split unique IDs: {len(all_split_ids):,}")
print(f"  - Missing IDs: {len(original_ids - all_split_ids)}")
print(f"  - Extra IDs: {len(all_split_ids - original_ids)}")

if len(original_ids - all_split_ids) > 0:
    missing_ids = list(original_ids - all_split_ids)[:10]
    print(f"  ❌ Missing IDs (first 10): {missing_ids}")

if len(all_split_ids - original_ids) > 0:
    extra_ids = list(all_split_ids - original_ids)[:10]
    print(f"  ❌ Extra IDs (first 10): {extra_ids}")

# Step 8: Record count verification
print(f"\n--- RECORD COUNT VERIFICATION ---")
print(f"Record distribution:")
print(f"  - Original dataset: {len(df_to_split):,} records")
print(f"  - Train + Valid + Test: {len(train_df) + len(valid_df) + len(test_df):,} records")
print(f"  - Difference: {len(df_to_split) - (len(train_df) + len(valid_df) + len(test_df))}")

# Step 9: Final validation summary
print(f"\n🎯 FINAL VALIDATION SUMMARY:")
no_overlap = (len(train_valid_overlap) == 0 and 
              len(train_test_overlap) == 0 and 
              len(valid_test_overlap) == 0)

complete_split = (len(original_ids) == len(all_split_ids) and 
                  len(df_to_split) == len(train_df) + len(valid_df) + len(test_df))

if no_overlap and complete_split:
    print(f"  ✅ SUCCESS: Perfect split with no overlap!")
    print(f"  ✅ All customer IDs properly distributed")
    print(f"  ✅ All records accounted for")
    print(f"  ✅ Ready for ML training!")
else:
    print(f"  ❌ ISSUES DETECTED:")
    if not no_overlap:
        print(f"    - Customer ID overlap found")
    if not complete_split:
        print(f"    - Missing or extra records/IDs")
    print(f"  ⚠️  Review split logic before proceeding")

# Outlier Analysis and Cleaning for ALL SETS (Train/Valid/Test)
# ============================================================================
print("=" * 60)
print("OUTLIER ANALYSIS AND CLEANING - ALL SETS")
print("=" * 60)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Initial Analysis of Target Variable (TRAINING SET ONLY)
print("--- INITIAL TARGET ANALYSIS (TRAINING SET) ---")
target_train = train_df['ingresos_reportados'].copy()

print(f"Target variable: ingresos_reportados")
print(f"Training set size: {len(target_train):,} records")
print(f"\nBasic statistics:")
print(f"  - Count: {target_train.count():,}")
print(f"  - Missing: {target_train.isnull().sum():,}")
print(f"  - Mean: ${target_train.mean():,.2f}")
print(f"  - Median: ${target_train.median():,.2f}")
print(f"  - Std: ${target_train.std():,.2f}")
print(f"  - Min: ${target_train.min():,.2f}")
print(f"  - Max: ${target_train.max():,.2f}")
print(f"  - Skewness: {target_train.skew():.2f}")

# Step 2: Percentile Analysis (TRAINING SET ONLY)
print(f"\n--- PERCENTILE ANALYSIS (TRAINING SET) ---")
percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99, 99.5, 99.9]
print("Percentile distribution:")
for p in percentiles:
    value = target_train.quantile(p/100)
    print(f"  - {p:4.1f}%: ${value:>12,.2f}")

# Step 3: Define Outlier Detection Parameters (FROM TRAINING SET ONLY)
print(f"\n--- DEFINING OUTLIER PARAMETERS (FROM TRAINING SET) ---")

# Method 1: IQR Method
Q1_train = target_train.quantile(0.25)
Q3_train = target_train.quantile(0.75)
IQR_train = Q3_train - Q1_train
lower_bound_iqr = Q1_train - 1.5 * IQR_train
upper_bound_iqr = Q3_train + 1.5 * IQR_train

# Method 2: Modified Z-Score parameters
median_train = target_train.median()
mad_train = np.median(np.abs(target_train - median_train))

# Method 3: Percentile Method
lower_bound_perc = target_train.quantile(0.01)  # 1st percentile
upper_bound_perc = target_train.quantile(0.99)  # 99th percentile

# Method 4: Domain Knowledge Method
min_reasonable_income = 300
max_reasonable_income = 50000

print(f"Outlier detection parameters (from training set):")
print(f"  - IQR bounds: ${lower_bound_iqr:,.2f} to ${upper_bound_iqr:,.2f}")
print(f"  - Percentile bounds (1%-99%): ${lower_bound_perc:,.2f} to ${upper_bound_perc:,.2f}")
print(f"  - Domain bounds: ${min_reasonable_income:,.2f} to ${max_reasonable_income:,.2f}")
print(f"  - Modified Z-Score: median=${median_train:,.2f}, MAD=${mad_train:,.2f}")

# Step 4: Apply Outlier Detection and Cleaning to ALL SETS
print(f"\n--- APPLYING OUTLIER CLEANING TO ALL SETS ---")

# Define the cleaning function
def clean_target_outliers(target_series, set_name, lower_cap, upper_cap):
    """Apply winsorization using training set parameters"""
    print(f"\n{set_name} Set Cleaning:")
    print(f"  - Original size: {len(target_series):,}")
    print(f"  - Original range: ${target_series.min():,.2f} to ${target_series.max():,.2f}")
    
    # Count outliers before cleaning
    n_lower_capped = (target_series < lower_cap).sum()
    n_upper_capped = (target_series > upper_cap).sum()
    n_total_capped = n_lower_capped + n_upper_capped
    
    print(f"  - Values to cap below: {n_lower_capped:,}")
    print(f"  - Values to cap above: {n_upper_capped:,}")
    print(f"  - Total to cap: {n_total_capped:,} ({n_total_capped/len(target_series)*100:.1f}%)")
    
    # Apply winsorization
    target_cleaned = target_series.clip(lower=lower_cap, upper=upper_cap)
    
    print(f"  - After cleaning range: ${target_cleaned.min():,.2f} to ${target_cleaned.max():,.2f}")
    print(f"  - Mean change: ${target_series.mean():,.2f} → ${target_cleaned.mean():,.2f}")
    
    return target_cleaned

# Use training set percentiles for all sets
lower_cap = lower_bound_perc  # 1st percentile from training
upper_cap = upper_bound_perc  # 99th percentile from training

# Clean all three sets using SAME parameters
target_train_cleaned = clean_target_outliers(target_train, "TRAINING", lower_cap, upper_cap)
target_valid_cleaned = clean_target_outliers(valid_df['ingresos_reportados'], "VALIDATION", lower_cap, upper_cap)
target_test_cleaned = clean_target_outliers(test_df['ingresos_reportados'], "TEST", lower_cap, upper_cap)

# Step 5: Create Cleaned DataFrames
print(f"\n--- CREATING CLEANED DATASETS ---")

# Training set
train_df_cleaned = train_df.copy()
train_df_cleaned['ingresos_reportados'] = target_train_cleaned

# Validation set
valid_df_cleaned = valid_df.copy()
valid_df_cleaned['ingresos_reportados'] = target_valid_cleaned

# Test set
test_df_cleaned = test_df.copy()
test_df_cleaned['ingresos_reportados'] = target_test_cleaned

print(f"✅ All datasets cleaned using training set parameters")

# Step 6: Add Outlier Flags (using training parameters)
print(f"\n--- ADDING OUTLIER FLAGS TO ALL SETS ---")

def add_outlier_flags(df, target_original, target_cleaned, set_name):
    """Add outlier flags to dataset"""
    # IQR outliers
    outliers_iqr = ((target_original < lower_bound_iqr) | (target_original > upper_bound_iqr))
    
    # Domain outliers
    outliers_domain = ((target_original < min_reasonable_income) | (target_original > max_reasonable_income))
    
    # Winsorized flag
    was_winsorized = (target_original != target_cleaned)
    
    # Add flags to dataframe
    df[f'outlier_iqr'] = outliers_iqr.astype(int)
    df[f'outlier_domain'] = outliers_domain.astype(int)
    df[f'was_winsorized'] = was_winsorized.astype(int)
    
    print(f"{set_name} outlier flags:")
    print(f"  - outlier_iqr: {outliers_iqr.sum():,} records")
    print(f"  - outlier_domain: {outliers_domain.sum():,} records")
    print(f"  - was_winsorized: {was_winsorized.sum():,} records")

# Add flags to all sets
add_outlier_flags(train_df_cleaned, target_train, target_train_cleaned, "TRAINING")
add_outlier_flags(valid_df_cleaned, valid_df['ingresos_reportados'], target_valid_cleaned, "VALIDATION")
add_outlier_flags(test_df_cleaned, test_df['ingresos_reportados'], target_test_cleaned, "TEST")

# Step 7: Create Clean Target Variables
print(f"\n--- CREATING CLEAN TARGET VARIABLES ---")

y_train_cleaned = target_train_cleaned.copy()
y_valid_cleaned = target_valid_cleaned.copy()
y_test_cleaned = target_test_cleaned.copy()

print(f"✅ Clean target variables created:")
print(f"  - y_train_cleaned: {len(y_train_cleaned):,} records")
print(f"  - y_valid_cleaned: {len(y_valid_cleaned):,} records")
print(f"  - y_test_cleaned: {len(y_test_cleaned):,} records")

# Step 8: Final Comparison Across Sets
print(f"\n--- FINAL COMPARISON ACROSS ALL SETS ---")

sets_comparison = {
    'TRAINING': {'original': target_train, 'cleaned': target_train_cleaned},
    'VALIDATION': {'original': valid_df['ingresos_reportados'], 'cleaned': target_valid_cleaned},
    'TEST': {'original': test_df['ingresos_reportados'], 'cleaned': target_test_cleaned}
}

for set_name, data in sets_comparison.items():
    print(f"\n{set_name} SET:")
    print(f"  BEFORE: Mean=${data['original'].mean():,.2f}, Std=${data['original'].std():,.2f}, Skew={data['original'].skew():.2f}")
    print(f"  AFTER:  Mean=${data['cleaned'].mean():,.2f}, Std=${data['cleaned'].std():,.2f}, Skew={data['cleaned'].skew():.2f}")

# Step 9: Final Validation
print(f"\n--- FINAL VALIDATION ---")

print(f"All datasets cleaned and ready:")
print(f"  - train_df_cleaned: {train_df_cleaned.shape}")
print(f"  - valid_df_cleaned: {valid_df_cleaned.shape}")
print(f"  - test_df_cleaned: {test_df_cleaned.shape}")

print(f"\nTarget variables ready for modeling:")
print(f"  - y_train_cleaned: {len(y_train_cleaned):,} records")
print(f"  - y_valid_cleaned: {len(y_valid_cleaned):,} records")
print(f"  - y_test_cleaned: {len(y_test_cleaned):,} records")

print(f"\n🎯 CONSISTENT OUTLIER CLEANING COMPLETED!")
print(f"✅ All sets use SAME cleaning parameters from training set")
print(f"✅ No data leakage - parameters learned from training only")
print(f"✅ Ready for consistent model training and evaluation")


# Feature Balance Analysis
# ============================================================================
print("=" * 60)
print("FEATURE BALANCE ANALYSIS")
print("=" * 60)

import pandas as pd
import numpy as np

# Define features to analyze
features = ['is_retired', 'sexo_consolidated_Masculino', 'estado_civil_consolidated_Soltero',
    'age_group_26-35', 'age_group_36-45', 'age_group_46-55', 'age_group_56-65', 'age_group_65+',
    'ocupacion_consolidated_freq',           # Job type (critical!)
    'ciudad_consolidated_freq',              # Location (important!)
    'nombreempleadorcliente_consolidated_freq',  # Employer size/type
    'cargoempleocliente_consolidated_freq']

print(f"Analyzing {len(features)} features from training set")
print(f"Training set size: {len(train_df_cleaned):,} records\n")

# Analyze each feature
for i, feature in enumerate(features):
    print(f"{i+1}. {feature}")
    print("-" * 50)
    
    # Get value counts
    value_counts = train_df_cleaned[feature].value_counts().sort_index()
    total_records = len(train_df_cleaned)
    
    # Calculate percentages
    percentages = (value_counts / total_records * 100)
    
    # Display results
    for value, count in value_counts.items():
        pct = round(percentages[value], 1)
        print(f"  {value}: {count:,} records ({pct}%)")
    
    # Balance assessment
    if len(value_counts) == 2:  # Binary feature
        minority_pct = round(min(percentages), 1)
        majority_pct = round(max(percentages), 1)
        
        if minority_pct < 10:
            balance_status = "❌ SEVERELY IMBALANCED"
        elif minority_pct < 20:
            balance_status = "⚠️  IMBALANCED"
        elif minority_pct < 40:
            balance_status = "⚡ MODERATELY IMBALANCED"
        else:
            balance_status = "✅ BALANCED"
            
        print(f"  Balance: {balance_status} (minority: {minority_pct}%)")
    
    print()

# Summary table
print("=" * 60)
print("BALANCE SUMMARY TABLE")
print("=" * 60)

summary_data = []
for feature in features:
    value_counts = train_df_cleaned[feature].value_counts()
    total = len(train_df_cleaned)
    
    if len(value_counts) == 2:  # Binary feature
        minority_count = min(value_counts)
        majority_count = max(value_counts)
        minority_pct = round((minority_count / total * 100), 1)
        majority_pct = round((majority_count / total * 100), 1)
        
        # Determine which value is minority/majority
        minority_value = value_counts.idxmin()
        majority_value = value_counts.idxmax()
        
        summary_data.append({
            'Feature': feature,
            'Majority_Value': majority_value,
            'Majority_Count': majority_count,
            'Majority_Pct': majority_pct,
            'Minority_Value': minority_value,
            'Minority_Count': minority_count,
            'Minority_Pct': minority_pct
        })

# Create summary DataFrame
summary_df = pd.DataFrame(summary_data)

print("Feature Balance Summary:")
print(summary_df.to_string(index=False))

# Identify most imbalanced features
print(f"\n" + "=" * 60)
print("MOST IMBALANCED FEATURES")
print("=" * 60)

# Sort by minority percentage (ascending = most imbalanced first)
most_imbalanced = summary_df.sort_values('Minority_Pct').head(3)

print("Top 3 most imbalanced features:")
for idx, row in most_imbalanced.iterrows():
    print(f"  {row['Feature']}: {row['Minority_Pct']}% minority class")

# Age group analysis (special case - multiple categories)
print(f"\n" + "=" * 60)
print("AGE GROUP DISTRIBUTION ANALYSIS")
print("=" * 60)

age_features = [f for f in features if f.startswith('age_group_')]
print("Age group features (one-hot encoded):")

# Calculate age group distribution
age_distribution = {}
for age_feature in age_features:
    count = train_df_cleaned[age_feature].sum()
    pct = round((count / len(train_df_cleaned) * 100), 1)
    age_group = age_feature.replace('age_group_', '')
    age_distribution[age_group] = {'count': count, 'pct': pct}

# Add reference category (18-25) - all zeros in one-hot encoding
reference_count = len(train_df_cleaned) - sum(train_df_cleaned[age_features].sum())
reference_pct = round((reference_count / len(train_df_cleaned) * 100), 1)
age_distribution['18-25'] = {'count': reference_count, 'pct': reference_pct}

# Sort by percentage
sorted_age_dist = sorted(age_distribution.items(), key=lambda x: x[1]['pct'], reverse=True)

print("Complete age group distribution:")
for age_group, data in sorted_age_dist:
    print(f"  {age_group}: {data['count']:,} records ({data['pct']}%)")

# Recommendations
print(f"\n" + "=" * 60)
print("RECOMMENDATIONS")
print("=" * 60)

print("Based on the balance analysis:")

# Check for severely imbalanced features
severely_imbalanced = summary_df[summary_df['Minority_Pct'] < 10]
if len(severely_imbalanced) > 0:
    print(f"\n⚠️  SEVERELY IMBALANCED FEATURES:")
    for _, row in severely_imbalanced.iterrows():
        print(f"  - {row['Feature']}: Consider removing or using SMOTE/undersampling")

# Check for moderately imbalanced features
moderately_imbalanced = summary_df[(summary_df['Minority_Pct'] >= 10) & (summary_df['Minority_Pct'] < 30)]
if len(moderately_imbalanced) > 0:
    print(f"\n⚡ MODERATELY IMBALANCED FEATURES:")
    for _, row in moderately_imbalanced.iterrows():
        print(f"  - {row['Feature']}: Monitor model performance, consider class weights")

# Check for balanced features
balanced = summary_df[summary_df['Minority_Pct'] >= 40]
if len(balanced) > 0:
    print(f"\n✅ WELL-BALANCED FEATURES:")
    for _, row in balanced.iterrows():
        print(f"  - {row['Feature']}: Good for modeling")

print(f"\n💡 GENERAL RECOMMENDATIONS:")
print(f"  - Use class_weight='balanced' in models for imbalanced features")
print(f"  - Consider feature engineering to combine rare categories")
print(f"  - Monitor precision/recall for minority classes")
print(f"  - Use stratified sampling for train/validation splits")

#send cleaned sets to csv
train_df_cleaned.to_csv(data_path + '/train_df_cleaned.csv', index=False)
valid_df_cleaned.to_csv(data_path + '/valid_df_cleaned.csv', index=False)
test_df_cleaned.to_csv(data_path + '/test_df_cleaned.csv', index=False)

# =============================================================================
# APPLY TO ALL DATASETS
# =============================================================================

# Apply feature engineering to all three datasets
print("="*80)
print("APPLYING FEATURE ENGINEERING TO ALL DATASETS")
print("="*80)

# Apply to training set
train_df_enhanced = create_interaction_features(train_df_cleaned)

# Apply to validation set  
valid_df_enhanced = create_interaction_features(valid_df_cleaned)

# Apply to test set
test_df_enhanced = create_interaction_features(test_df_cleaned)

print(f"\n🎯 FINAL DATASET SHAPES:")
print(f"   Training: {train_df_enhanced.shape}")
print(f"   Validation: {valid_df_enhanced.shape}")
print(f"   Test: {test_df_enhanced.shape}")

# Group Strategy for Imbalanced Age Features
# ============================================================================
print("=" * 60)
print("CREATING GROUPS FOR IMBALANCED AGE FEATURES")
print("=" * 60)

import pandas as pd
import numpy as np
from sklearn.model_selection import GroupKFold

# Define the most imbalanced age features (minority classes)
imbalanced_age_features = ['age_group_26-35', 'age_group_36-45', 'age_group_65+']

print(f"Most imbalanced age features: {imbalanced_age_features}")

# Function to create groups based on age features
def create_age_groups(df, age_features):
    """
    Create groups based on imbalanced age features
    Returns group labels for GroupKFold
    """
    groups = []
    
    for idx, row in df.iterrows():
        group_assigned = False
        
        # Check each imbalanced age feature
        for i, age_feature in enumerate(age_features):
            if row[age_feature] == 1:  # If person belongs to this age group
                groups.append(i + 1)  # Group 1, 2, 3 for each age feature
                group_assigned = True
                break
        
        # If not in any imbalanced group, assign to default group
        if not group_assigned:
            groups.append(0)  # Group 0 for all other age groups
    
    return np.array(groups)

# Create groups for training set
print(f"\n--- Creating groups for training set ---")
groups_train = create_age_groups(train_df_cleaned, imbalanced_age_features)

# Create groups for validation set
print(f"--- Creating groups for validation set ---")
groups_valid = create_age_groups(valid_df_cleaned, imbalanced_age_features)

# Create groups for test set
print(f"--- Creating groups for test set ---")
groups_test = create_age_groups(test_df_cleaned, imbalanced_age_features)

# Analyze group distribution
print(f"\n--- GROUP DISTRIBUTION ANALYSIS ---")

def analyze_groups(groups, set_name):
    """Analyze the distribution of groups"""
    unique_groups, counts = np.unique(groups, return_counts=True)
    total = len(groups)
    
    print(f"\n{set_name} Set Group Distribution:")
    group_names = {
        0: "Other ages (18-25, 46-55, 56-65)",
        1: "age_group_26-35 (most imbalanced)",
        2: "age_group_36-45 (2nd most imbalanced)", 
        3: "age_group_65+ (3rd most imbalanced)"
    }
    
    for group, count in zip(unique_groups, counts):
        pct = (count / total) * 100
        group_name = group_names.get(group, f"Group {group}")
        print(f"  Group {group}: {count:,} records ({pct:.1f}%) - {group_name}")
    
    return dict(zip(unique_groups, counts))

# Analyze all sets
train_group_dist = analyze_groups(groups_train, "TRAINING")
valid_group_dist = analyze_groups(groups_valid, "VALIDATION")
test_group_dist = analyze_groups(groups_test, "TEST")

# Prepare features for modeling (exclude ID columns and target)
print(f"\n--- PREPARING FEATURES FOR GROUPED MODELING ---")

# Define your final feature columns (adjust as needed)
final_features_columns = [
'is_retired', 'sexo_consolidated_Masculino', 'estado_civil_consolidated_Soltero',
    'age_group_26-35', 'age_group_36-45', 'age_group_46-55', 'age_group_56-65', 'age_group_65+',
    'ocupacion_consolidated_freq',           # Job type (critical!)
    'ciudad_consolidated_freq',              # Location (important!)
    'nombreempleadorcliente_consolidated_freq',  # Employer size/type
    'cargoempleocliente_consolidated_freq',
    'age_group_26-35_x_occupation', 'age_group_36-45_x_occupation', 'age_group_46-55_x_occupation', 'age_group_56-65_x_occupation', 
    'age_group_65+_x_occupation', 'location_x_occupation', 'retired_x_age_group_26-35', 'retired_x_age_group_36-45', 'retired_x_age_group_46-55', 
    'retired_x_age_group_56-65', 'retired_x_age_group_65+', 'employer_x_position', 'gender_x_occupation', 'high_freq_occupation', 'high_freq_city', 
    'high_value_location_job'
]

print(f"Final features for modeling: {len(final_features_columns)}")
print(f"Features: {final_features_columns}")

# Create final feature matrices
X_train_grouped = train_df_enhanced[final_features_columns].copy()
X_valid_grouped = valid_df_enhanced[final_features_columns].copy()
X_test_grouped = test_df_enhanced[final_features_columns].copy()

# Add group information to datasets (for analysis, not modeling)
X_train_grouped['group'] = groups_train
X_valid_grouped['group'] = groups_valid  
X_test_grouped['group'] = groups_test

print(f"\nDataset shapes with groups:")
print(f"  - X_train_grouped: {X_train_grouped.shape}")
print(f"  - X_valid_grouped: {X_valid_grouped.shape}")
print(f"  - X_test_grouped: {X_test_grouped.shape}")

# Prepare final datasets for GroupKFold (without group column in features)
X_train_final = X_train_grouped[final_features_columns].copy()
X_valid_final = X_valid_grouped[final_features_columns].copy()
X_test_final = X_test_grouped[final_features_columns].copy()

print(f"\nFinal feature matrices (for modeling):")
print(f"  - X_train_final: {X_train_final.shape}")
print(f"  - X_valid_final: {X_valid_final.shape}")
print(f"  - X_test_final: {X_test_final.shape}")

# Setup GroupKFold
print(f"\n--- SETTING UP GROUPKFOLD ---")
gkf = GroupKFold(n_splits=4)

print(f"GroupKFold configuration:")
print(f"  - Number of splits: 4")
print(f"  - Groups for training: {len(np.unique(groups_train))} unique groups")
print(f"  - Group distribution ensures imbalanced age groups stay together")

# Validate GroupKFold splits
print(f"\n--- VALIDATING GROUPKFOLD SPLITS ---")
split_info = []
for fold, (train_idx, val_idx) in enumerate(gkf.split(X_train_final, y_train_cleaned, groups_train)):
    train_groups = groups_train[train_idx]
    val_groups = groups_train[val_idx]
    
    train_unique = np.unique(train_groups)
    val_unique = np.unique(val_groups)
    
    split_info.append({
        'fold': fold + 1,
        'train_size': len(train_idx),
        'val_size': len(val_idx),
        'train_groups': train_unique,
        'val_groups': val_unique,
        'overlap': len(np.intersect1d(train_unique, val_unique))
    })
    
    print(f"Fold {fold + 1}:")
    print(f"  - Train: {len(train_idx):,} samples, Groups: {train_unique}")
    print(f"  - Val: {len(val_idx):,} samples, Groups: {val_unique}")
    print(f"  - Group overlap: {len(np.intersect1d(train_unique, val_unique))} groups")

print(f"\n🎯 GROUPED MODELING SETUP COMPLETED!")
print(f"✅ Groups created based on most imbalanced age features")
print(f"✅ GroupKFold will respect age group boundaries")
print(f"✅ This helps model learn better from minority age classes")

# Ready for modeling
print(f"\n💡 USAGE FOR MODELING:")
print(f"Use these variables:")
print(f"  - X_train_final: Features for training")
print(f"  - y_train_cleaned: Target for training") 
print(f"  - groups_train: Groups for GroupKFold")
print(f"  - gkf: GroupKFold object for cross-validation")

# Multi-Model Training and Comparison with GroupKFold
print("=" * 80)
print("TRAINING AND COMPARING 3 MODELS: RF, XGBOOST, LIGHTGBM")
print("=" * 80)

import lightgbm as lgb
import xgboost as xgb
import numpy as np
import pandas as pd
from scipy.stats import randint as sp_randint
from scipy.stats import uniform as sp_uniform
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV, cross_val_score, GroupKFold
from sklearn.metrics import mean_squared_error, r2_score, make_scorer
import time

# Initialize models
models = {
    'Random Forest': RandomForestRegressor(random_state=314, n_jobs=-1),
    'XGBoost': xgb.XGBRegressor(random_state=314, n_jobs=-1),
    'LightGBM': lgb.LGBMRegressor(random_state=314, n_jobs=-1)
}

# Define parameter grids for each model
param_grids = {
    'Random Forest': {
        'n_estimators': sp_randint(100, 500),
        'max_depth': sp_randint(5, 20),
        'min_samples_split': sp_randint(2, 20),
        'min_samples_leaf': sp_randint(1, 10),
        'max_features': ['sqrt', 'log2', None],
        'bootstrap': [True, False]
    },
    
    'XGBoost': {
        'learning_rate': [0.05,0.1,0.15],
        'n_estimators': sp_randint(200,300,500),
        'max_depth': sp_randint(4,6,8),
        'min_child_weight': sp_randint(1, 10),
        'subsample': sp_uniform(loc=0.6, scale=0.4),
        'colsample_bytree': sp_uniform(loc=0.6, scale=0.4),
        'reg_alpha': [0, 0.1, 1],
        'reg_lambda': [0, 0.1, 1]
    },
    
    'LightGBM': {
        'learning_rate': [0.05,0.1,0.15],
        'n_estimators': sp_randint(200,300,500),
        'max_depth': sp_randint(4,6,8),
        'num_leaves': sp_randint(10, 100),
        'min_child_samples': sp_randint(10, 50),
        'subsample': sp_uniform(loc=0.6, scale=0.4),
        'colsample_bytree': sp_uniform(loc=0.6, scale=0.4),
        'reg_alpha': [0, 0.1, 1],
        'reg_lambda': [0, 0.1, 1],
        'boosting_type': ['gbdt'],
        'force_col_wise': [True]
    }
}

# Scoring metrics
scoring = {
    'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),
    'r2': make_scorer(r2_score)
}

# Storage for results
results = {}
best_models = {}

print(f"Starting training for {len(models)} models...")

# Train each model
for model_name, model in models.items():
    print(f"\n" + "=" * 60)
    print(f"TRAINING {model_name.upper()}")
    print("=" * 60)
    
    start_time = time.time()
    
    # Create RandomizedSearchCV
    random_search = RandomizedSearchCV(
        estimator=model,
        param_distributions=param_grids[model_name],
        n_iter=30,
        scoring=scoring,
        cv=gkf,
        refit='neg_mean_squared_error',
        random_state=314,
        verbose=1,
        n_jobs=-1
    )
    
    # Fit the model
    print(f"Fitting {model_name} with RandomizedSearchCV...")
    random_search.fit(X_train_final, y_train_cleaned, groups=groups_train)
    
    # Get best parameters
    best_params = random_search.best_params_
    print(f"\nBest parameters for {model_name}:")
    for param, value in best_params.items():
        print(f"  - {param}: {value}")
    
    # Create best model
    if model_name == 'Random Forest':
        best_model = RandomForestRegressor(**best_params, random_state=314, n_jobs=-1)
    elif model_name == 'XGBoost':
        best_model = xgb.XGBRegressor(**best_params, random_state=314, n_jobs=-1)
    else:  # LightGBM
        best_model = lgb.LGBMRegressor(**best_params, random_state=314, n_jobs=-1)
    
    # Nested CV evaluation
    print(f"\nPerforming nested cross-validation for {model_name}...")
    outer_cv = GroupKFold(n_splits=4)
    nested_scores = cross_val_score(
        best_model, 
        X=X_train_final, 
        y=y_train_cleaned,
        groups=groups_train, 
        cv=outer_cv, 
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )
    
    nested_rmse = np.sqrt(-nested_scores)
    nested_rmse_mean = nested_rmse.mean()
    nested_rmse_std = nested_rmse.std()
    
    print(f"Nested CV RMSE: {nested_rmse_mean:.4f} ± {nested_rmse_std:.4f}")
    
    # Train final model on full training set
    print(f"Training final {model_name} model...")
    if model_name == 'LightGBM':
        best_model.fit(
            X_train_final, 
            y_train_cleaned,
            eval_set=[(X_valid_final, y_valid_cleaned)],
            eval_metric='mse',
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )
    elif model_name == 'XGBoost':
        # FIXED: Removed eval_metric parameter
        best_model.fit(
            X_train_final, 
            y_train_cleaned,
            eval_set=[(X_valid_final, y_valid_cleaned)],
            #early_stopping_rounds=50,
            verbose=False
        )
    else:  # Random Forest
        best_model.fit(X_train_final, y_train_cleaned)
    
    # Make predictions on validation set
    predictions = best_model.predict(X_valid_final)
    
    # Calculate metrics
    mse = mean_squared_error(y_valid_cleaned, predictions)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_valid_cleaned, predictions)
    mae = np.mean(np.abs(y_valid_cleaned - predictions))
    
    # Calculate training time
    end_time = time.time()
    training_time = end_time - start_time
    
    # Store results
    results[model_name] = {
        'best_params': best_params,
        'nested_cv_rmse_mean': nested_rmse_mean,
        'nested_cv_rmse_std': nested_rmse_std,
        'validation_rmse': rmse,
        'validation_r2': r2,
        'validation_mae': mae,
        'training_time': training_time
    }
    
    best_models[model_name] = best_model
    
    print(f"\n{model_name} Results:")
    print(f"  - Validation RMSE: {rmse:.4f}")
    print(f"  - Validation R²: {r2:.4f}")
    print(f"  - Validation MAE: {mae:.4f}")
    print(f"  - Training time: {training_time:.2f} seconds")

# Model Comparison
print(f"\n" + "=" * 80)
print("MODEL COMPARISON RESULTS")
print("=" * 80)

# Create comparison DataFrame
comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Nested_CV_RMSE': [results[model]['nested_cv_rmse_mean'] for model in results.keys()],
    'CV_RMSE_Std': [results[model]['nested_cv_rmse_std'] for model in results.keys()],
    'Validation_RMSE': [results[model]['validation_rmse'] for model in results.keys()],
    'Validation_R2': [results[model]['validation_r2'] for model in results.keys()],
    'Validation_MAE': [results[model]['validation_mae'] for model in results.keys()],
    'Training_Time': [results[model]['training_time'] for model in results.keys()]
})

# Sort by validation RMSE (best first)
comparison_df = comparison_df.sort_values('Validation_RMSE')

print("Performance Comparison (sorted by Validation RMSE):")
print(comparison_df.round(4).to_string(index=False))

# Identify best model
best_model_name = comparison_df.iloc[0]['Model']
print(f"\nBest Model: {best_model_name}")
print(f"Validation RMSE: {comparison_df.iloc[0]['Validation_RMSE']:.4f}")
print(f"Validation R²: {comparison_df.iloc[0]['Validation_R2']:.4f}")

print(f"\nTraining completed successfully!")